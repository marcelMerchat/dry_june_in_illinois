---
title: "Predicting a Dry June in Illinois"
author: "Marcel Merchat"
date: "11/21/2020"
output:
  html_document:
    base_level: 1
    code_folding: hide
    css: style.css
    df_print: paged
    highlight: tango
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 3
    toc_float: no
  word_document:
    toc: yes
    toc_depth: '3'
subtitle: Weather Station USW00094846 at O'Hare Airport in Chicago
editor_options:
  chunk_output_type: inline
---

install Rtools 4.0 from https://cran.r-project.org/bin/windows/Rtools/, remove the incompatible version from your PATH.
<br><br>
<script>
sectionnum = 1;
</script>

<script type="text/x-mathjax-config">
var sectionnum = sectionnum + 1;
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: { 
            autoNumber: "all",
            formatNumber: function (n) {return  n}
      } 
  }
});

</script>
<style> 

custsty {
  /*background-image:url(C:/path/mypng.png);*/ 
  background-repeat: no-repeat;
  background-position: center center;
  background-size: cover;
}

.boxText {
     font-size:0.9em;
}

caption {
    /*padding-left: 200px;*/
    padding-bottom: 15px;
    margin: auto;
    font-family: arial, verdana, sans-serif;
    font-size: 44px;
    color: #008899;
    height: 50px;
    font-size: 1.5em;
    margin: auto;
    margin-bottom: 26px;
    text-align: center;
}

    
table {
    border-collapse: collapse;
    /*width: 980px;*/
    /*height: 400px;*/
    /*background-color: #9999FF;*/
    background-image: none;
    cell-spacing: 0px;
    padding-right: 0px;
    padding-top: 0px;
    padding-bottom: 0px;
    border: 3px solid #AABBDD;
    background-color: #CCDDEE;
    margin: auto;
    margin-left: 2px;
}

th {
    color:#008899;
    padding-left: 4px;
    padding-right: 4px;
    padding-top: 8px;
    padding-bottom: 4px;
    background-color: #CCDDEE;
    cell-spacing: 0px;
    border: 1px solid #505050;
    margin: auto;
    height: 18px;
    font-size: 1.05em;
    font-weight: normal;
}

.th {
    color:#008899;
    padding-left: 4px;
    padding-right: 4px;
    padding-top: 8px;
    padding-bottom: 4px;
    
    margin: auto;
    font-size: 1.3em;
}

td {
    padding-left: 4px;
    padding-right: 4px;
    padding-top: 6px;
    padding-bottom: 3px;
    cell-spacing: 0px;
    border: 1px solid #505050;
    margin: auto;
    font-family: arial, verdana, sans-serif;
    background-color: #CCEEFF;
    height: 38px;
    font-size: 1em;
}

.dataTables_wrapper .dataTables_filter {
    float: left;
    text-align: left;
}    

label {
    width: 50%;
    /*height: 400px;*/
    /*background-color: #9999FF;*/
    font: 18px;
}

tr {
    margin: auto;
}

dt {
  margin-top: 20px;
}

ul li {
  list-style: none;
}

.underline {
  text-decoration-line: underline;
}

.ulnone {
   list-style: none; /* Remove default bullets */
}

.square-bullet li::before {
   content: "\25AA";
   color: #004400;
   font-size: 130%;
   font-weight: bold;
   width: 0.8em;
   margin-left: -1em;
   display: inline-block; 
}


.fig-title {
    margin-top: 12px;
    margin-bottom: 10px;
}

body {
  background-color: #DDDDEE;
}

slide:not(.segue)  h2{
 font-family: "Open Sans Condensed";
 font-weight: 700;
 color: #449944;
 /*background-color: #00FFFF; background of S3 */
}

h1 {
    font-family: arial;
    font-weight: normal;
    font-size:40px;
    line-height:48px;
    color: #449944;
    padding-top: 20px;
    padding-bottom: 10px;

}
h2 {
    font-family: arial;
    font-weight: normal;
    font-size:32px;
    LINE-HEIGHT:40px;
    color: #449944;
    padding-top: 10px;
    padding-bottom: 8px;
}
h3 {
    font-family: arial;
    font-weight: normal;
    font-size:28px;
    LINE-HEIGHT:32px;
    color: #449944;
    padding-top: 8px;
    padding-bottom: 6px;

}
h4 {
    font-family: arial;
    font-weight: normal;
    font-size:24px;
    LINE-HEIGHT:28px;
    color: #449944;
    padding-top: 6px;
    padding-bottom: 3px;
}
h5 {
    font-family: arial;
    font-weight: normal;
    font-size:28px;
    line-height:28px;
    color: #208020;
    padding-top: 1px;
    padding-bottom: 1px;
    margin-left: -60px;
    
}
h5 span.header-section-number {
    font-size:28px;
    margin: 0px;
    margin-block-start: 0px;
    line-height:0px;
    visibility: hidden;
}
h6 {
    font-family: arial;
    font-weight: 200;
    font-size: 3px;
    line-height: 6px;
    color: #202020;
    padding-top: 0px;
    padding-bottom: 0px;
    margin-block-start: 0x;
    margin-block-end: 0px;
}
h6 span.header-section-number {
    margin: 0px;
    margin-block-start: 0px;
    line-height:0px;
    visibility: hidden;
}

.MathJax_Preview {
   line-height: 0px;
   font-size:0px;

}
.MathJax_Display {
   margin: 0px;
   font-size: 18px;
}

span.math.display {
   margin: 0px;
   font-size: 18px;
}

a:link {
  text-decoration: none;
}
a:visited {
  text-decoration: none;
}
a:hover {
  text-decoration: underline;
}


.list-line-1 {
    LINE-HEIGHT:20px;
    padding-top: 15px;
    padding-bottom: 10px;
    color: #101010;
}

.list-line-middle {
    LINE-HEIGHT:20px;
    padding-top: 10px;
    padding-bottom: 10px;
    color: #101010;
}

.list-line-last {
    LINE-HEIGHT:20px;
    padding-top: 10px;
    padding-bottom: 10px;
    color: #101010;
}

.booktitle {
    LINE-HEIGHT:20px;
    padding-top: 15px;
    padding-bottom: 10px;
    color: #101010;
    font-style: italic;
}

.columntitle {
    font-size:16px;
    LINE-HEIGHT:20px;
    padding-top: 5px;
    padding-bottom: 2px;
    text-align: center;
    color: #101010;
    }

p {
    font-family: arial, verdana, sans-serif;
    font-size:18px;
    color: #112211;
    /*color: #1122FF; */
    padding-left: 55px;
    font-weight: normal;
    text-align: justify;
}

.pdiv {
    color: #1A301A;
    padding: 1px;
    font-size: 1em;
    padding-left: 60px;
}

.pdivsmall {
    color: #1A301A;
    padding: 1px;
    font-size: 1em;
    padding-left: 8px;
}

.p1div {
    color: #1A301A;
    padding: 1px;
    font-size: 1em;
    padding-left: 78px;
}

.p-double-digits {
    padding-left: 74px;
}

.p1 {
    padding-left: 74px;
}

.p1-double-digits {
    padding-left: 90px;
}

.pminus {
    margin-left: -54px;
}

.p2 {
    padding-left: 88px;
}
.p2-double-digits {
    padding-left: 104px;
}

.p2app {
    padding-left: 64px;
}

.p3 {
    padding-left: 98px;
}

.p3-double-digits {
    padding-left: 112px;
}

.center {
   margin: auto;
}
.bluegreen {
    color: #008899;
}
.centeredh3 {
    font-family: arial;
    font-weight: normal;
    font-size:36px;
    LINE-HEIGHT:54px;
    color: #449944;
    padding-top: 10px;
    padding-bottom: 4px;
}

.tablenote{
    padding-top: 15px;
    font-size:20px;
}

.psmall {
    padding-left:10px;
    font-family: arial, verdana, sans-serif;
    font-size:14px;
    color: #11FFFF;
}

img {
    display: block;
    /*padding-left: 0px;*/
    /*padding-right: 0px;*/
    padding-top: 0px;
    padding-bottom: 0px;
    margin-left: auto;
    margin-right: auto;
    margin-top: 15px;
    margin-bottom: 15px;
}

.img {
    /*padding-left: 0px;*/
    /*padding-right: 0px;*/
    padding-top: 0px;
    padding-bottom: 0px;
    border: 0px solid #00FFFF;
    margin: auto;
    margin-top: 15px;
    margin-bottom: 15px;
}

sub, sup {
 font-size: 75%;
 line-height: 0;
 position: relative;
 vertical-align: baseline;
}

.logga img {
    width: 70%;
    height: auto;
}

.header-section-number {
    margin-right: 16px;
}

.toc-section-number {
    margin-right: 12px;
}

.title-slide hgroup > h1 {
  color: #449944; /*  #111111;  #537E43 #EF5150*/  /*#535E43*/
}

.title-slide hgroup > h2 {
  color: #110000 ;  /* #537E43 #EF5150*/  /*#535E43*/
  font-size:44px;
  LINE-HEIGHT:54px;
  color: #449944;
}

.title-slide hgroup > p {
  font-family: 'Open Sans','Helvetica', 'Crimson Text', 'Garamond',  'Palatino', sans-serif;
  text-align: justify;
  font-size:22px;
  line-height: 1.5em;
  padding-left:10px;
  color: #1111FF;
  
  font-family: arial;
    font-weight: normal;
    font-size:44px;
    LINE-HEIGHT:54px;
    color: #449944;
    padding-top: 10px;
    padding-bottom: 4px;
}

.title-slide {
  background-color: #CBE7B5 /*#CBE7A5; #EDE0CF; ; #CA9F9D*/
  /* background-image:url(http://goo.gl/EpXln); */
  
  /* Reduce Space between Title and Body */
  slides > slide  {
    margin-top: 15px;
    background-color: #00FFFF;
    color: #00FFFF;
  }
}

.MathJax {
    font-size: 1em;
}

</style>

```{r setup2, results='hide', echo = FALSE, message=F, warning=F, include=F}

library(R6)
library(tidyverse)  # data manipulation
library(lattice)
library(ggplot2)
library(caret)
library(glmnet)
#library(randomForest)
library(AppliedPredictiveModeling)
library(cluster)    # clustering algorithms
library(factoextra)

library(psych)
library(xtable)
library(pander) 

library(grid)
library(gridExtra)
library(stats)
library(RColorBrewer)

library(matlib)
library(gtools)
library(boot)
library(DT)
library(reshape2)

library("plotrix")
library(PlaneGeometry)


oldw <- getOption("warn")
options(error = browser())
options(stringsAsFactors=FALSE)
options(warn = -1)
options(xtable.comment = FALSE)

knitr::opts_chunk$set(echo = TRUE)

tablenum <- 0
figurenum <- 0

```

```{r functionsection1, echo=FALSE, results='hide'}

findmatchgreg <- function(text,pattern){
 ## gregexpr returns a list of giving the starting position of the matches
 ## or -1 if there is none, with attribute "match.length", an integer vector
 ## giving the length of the matched text (or -1 for no match)
    
 ## Same as regexpr except provides multiple matches.    
    re <- gregexpr(pattern,text)
    rm <- regmatches(text, re)
    unlist(Filter(function(x) !identical(character(0),x), rm))
}

rounded <- function(vect,places){
  sapply(vect,function(u){
    as.numeric(formatC(u, format = "f", digits = places))
  })
}
  
convert_degrees_c_to_f <- function(vect){
  sapply(vect,function(u){
    32 + 9*u/5
  })
}
  
convert_degrees_mm_to_inch <- function(vect){
  sapply(vect,function(u){
    u/25.4
  })
}

months <- c("jan","feb","mar","apr","may","jun",
            "jul","aug","sep","oct","nov","dec")

get_month_days <- function(m){
    monthdays31 <- c(1:31)
    monthdays30 <- c(1:30)
    if(m == "jan" | m == "mar" | m == "may" | m == "jul" |
       m == "aug" | m == "oct" | m == "dec" )
    {
        dd <- monthdays31
    } else if(m == "apr" | m == "jun" | m == "sep" | m == "nov" )
    {
        dd <- monthdays30
    } else if(m == "feb")
    {
        dd <- c(1:28)
    } else {
        dd <- rep(NA,31)
    }
    dd
}

get_month_num <- function(m){
    if(m == "jan")
    {
        num <- 1
    } else if(m == "mar")
    {
        num <- 3
    } else if(m == "may" )
    {
        num <- 5
    } else if(m == "jul" )
    {
        num <- 7
    } else if(m == "aug")
    {
        num <- 8
    } else if(m == "oct")
    {
        num <- 10
    } else if(m == "dec" )
    {
        num <- 12
    } else {
        num <- NA
    }
    num
}

get_month_abbreviation <- function(num){
    if(num == 1) {
        m <- "jan" 
    } else if(num == 2) {
        m <- "feb"
    } else if(num == 3) {
        m <- "mar"
    } else if(num == 4) {
        m <- "apr"
    } else if(num == 5) {
        m <- "may"
    } else if(num == 6) {
        m <- "jun"
    } else if(num == 7) {
        m <- "jul"
    } else if(num == 8) {
        m <- "aug"
    } else if(num == 9) {
        m <- "sep"
    } else if(num == 10) {
        m <- "oct"
    } else if(num == 11) {
        m <- "nov"
    } else if(num == 12) {
        m <- "dec"
    } else {
        m <- NA
    } 
    m
}

get_real_data <- function(text){
   as.numeric(as.character(text))
}

get_num_data3 <- function(text){
    found <- gregexpr("-?\\d{1,3}", text)
    len <- as.numeric(as.character(attr(found[[1]],'match.length')))
    start <- found[[1]][1]
    as.numeric(as.character(substr(text, start, start+len-1)))
}

get_bool_data <- function(text){
    found <- gregexpr("\\d", text)
    start <- found[[1]][1]
    substr(text, start, start)
}

get_quote_trimmed <- function(text){
    strlen <- nchar(text)
    found <- grep('^"', text)
    len <- length(found)

    if (length(found) > 0){
        text <- substr(text, 2, strlen)
        strlen <- nchar(text)
    } 
    found <- grep('"$', text)
    len <- length(found)
    if (length(found) > 0){
        text <- substr(text, 1, strlen-1)
    } 
    text
}

get_zero_trimmed <- function(text){
    found <- grep("0\\d", text)
    len <- length(found)
    if (length(found) > 0){
        found <- gregexpr("0\\d", text)
        start <- found[[1]][1]
        text <- substr(text, start+1, start+1)
    } else {
        found <- gregexpr("\\d{2}", text)
        start <- found[[1]][1]
        text <- substr(text, start, start+1)
    }
    text
}

get_zero_mean_vector <- function(x) {
   x-mean(x)
}

get_scaled_vector <- function(x) {
  (x-mean(x))/sd(x)
}

get_scaled_vector_inverse <- function(scaled, x) {
   sd(x) * scaled + mean(x)
}

get_zero_mean_frame <- function(dat) {
   numcols <- dim(dat)[2]      
   iterate <- 1:numcols
   for(i in seq_along(iterate)) {
       dat[,i] <- get_zero_mean_vector(dat[,i])
   }
   dat
}

get_zero_mean_matrix <- function(m) {
   apply(m,2, function(x) {
      get_zero_mean_vector(x) 
   })
}

get_scaled_frame <- function(dat) {
   numcols <- dim(dat)[2]      
   iterate <- 1:numcols
   for(i in seq_along(iterate)) {
       dat[,i] <- get_scaled_vector(dat[,i])
   }
   dat
}

get_scaled_matrix <- function(m) {
  apply(m,2, function(x) {
    get_scaled_vector(x) 
  })
}

mag <- function(x){sqrt(sum(x^2))}

get_fog_fit <- function(df) {
        fit <- lm(df$JunRain ~ df$MayFog)
        fit$coefficients
}

get_fit <- function(df, JunRain) {
         fit <- lm(df$JunRain ~ df$NovSnow)
         fit$coefficients
}
get_line_parameters <- function(df,diagnosis){
    slope <- get_adjusted_concave_points_fit(df,diagnosis)[2]
    intercept <- get_adjusted_concave_points_fit(df,diagnosis)[1]
    c(slope, intercept)
}
get_perimeter_line <- function(df,diagnosis){
    fit <- get_perimeter_fit(df,diagnosis)
    slope <- fit[2]
    intercept <- fit[1]
    c(slope, intercept)
}
get_kmeans <- function(x,y){
    df <- data.frame(x,y)
    kmeans(df, centers = 2)
}
one_variable_mod <- function(data_vector, diagnosis){
    dat <- data.frame(data_vector, diagnosis, stringsAsFactors = FALSE)
    colnames(dat) <- c("test_results", "diagnosis")
    head(dat)
    train(diagnosis ~ test_results, method="rf", data=dat)
}
get_sensitivity <- function(predictions, diagnosis){
    detection <- predictions == diagnosis
    df <- data.frame(predictions,diagnosis,detection,stringsAsFactors = FALSE) 
    colnames(df) <- c("prediction","diagnosed","detect")

    detected <- df[df$diagnosed=="M",]
    sumdetect <- sum(detected$detect==TRUE)
    lendetect <- dim(detected)[1]
    sumdetect # trf[2,2]
    sumdetect / lendetect
}
get_selectivity <- function(predictions, diagnosis){
    detection <- predictions == diagnosis
    df <- data.frame(predictions, diagnosis, detection, stringsAsFactors = FALSE) 
    colnames(df) <- c("prediction", "diagnosed", "detect")

    cleared <- df[df$diagnosed=="B",]
    sumcleared <- sum(cleared$detect==TRUE)
    lencleared <- dim(cleared)[1]
    sumcleared # trf[1,1]
    sumcleared / lencleared
}

#########################################

## ANOVA

#########################################

## Two-Tailed t-test
## For example, Null Hypothesis Test for
## the probability that the null hypothesis is still true
## given the t-value corresponding to the outcome of an experiment
null_hypothesis_t_two_tail <- function(t,degf) {
  if (t < 0){
    2 * pt(t, degf, lower.tail = TRUE)
  } else {
    2 * pt(t, degf, lower.tail = FALSE)
  }
  #2 * pt(abs(t), degf, lower.tail = FALSE)
}

get_data_matrix <- function(input_data){
  cls <- class(input_data)
  cls_len <- length(cls)
  if(cls_len > 1 && cls[2] == "array") {
    input_data <- data.frame(input_data)  
  }
  
  row_count <- dim(input_data)[1]
  input_column_count <- dim(input_data)[2]
  dim_soln <- input_column_count + 1
  input_size_vector <- 1:input_column_count
  ## data_v is a serial list of all values in
  ## the columns
  data_v <- c() 
  
  for (i in input_size_vector){
   
    data_v <- c(data_v, input_data[,i])
  }
  matrix(c(rep(1,row_count),data_v),ncol=dim_soln)
}

get_inv_sqr <- function(input_data){
  a  <- get_data_matrix(input_data)
  at <- t(a)
  #yt <- t(y)
  xsqr <- at %*% a
  inv(xsqr)
}

get_meansquare <- function(x){
  mean(x^2)
}

get_rms <- function(x){
  sq <- mean(x^2)
  sqrt(sq)
}

get_sample_variance <- function(x){
  n <- length(x)
  xbar <- rep(mean(x),n)
  sum_sqrs <- sum((x - xbar)^2)
  sum_sqrs / (n-1)
}

get_mse <- function(prediction,outcome){
  n <- length(outcome)
  sum_sqrs <- sum((prediction-outcome)^2)
  sum_sqrs / n
}

get_sse <- function(input_data,y){
  if(class(input_data) != "data.frame"){
    input_data <- matrix(input_data,ncol=1)  
  }
  input_data_dim  <- dim(input_data)
  n <- input_data_dim[1]
  a  <- get_data_matrix(input_data)
  at <- t(a)
 
  inv_sqr <- get_inv_sqr(input_data)
  yt <- t(y)
  i <- diag(n)
  (yt %*% (i-a %*% inv_sqr %*% at) %*% y)[1,1]
}

get_regression_analysis <- function(x,y){
  len <- length(x)
  n <- length(x)
  xbar <- mean(x)
  #if(min(x) > 0) {
     fit_min <- floor(min(x)/2)
     fit_max <- 3*ceiling(max(x)/2)
  #}
  fit <- lm(formula = y ~ x)
  mslope <- fit$coefficients[2]
  intrcpt <- fit$coefficients[1]

## SSE:  Sum of squares of Residual Errors (from estimated regression Line)
  y_est <- mslope*x +intrcpt

## Walpole Example 11.2
## Confidence interval for slope
  Sxx <- sum((x - mean(x))^2)
  Syy <- sum((y - mean(y))^2)
  Sxy <- sum((x - mean(x))*(y - mean(y)))

  degf <- length(x)-2
## Estimated Variance
  ##MSE <- (Syy-mslope*Sxy)/degf
  sse <- sum((y - y_est)^2)
## SST:  Total error from overall mean
  s_sqr <- sse/(n-2)
  s <- s_sqr^0.5
  sst <- sum((y - mean(y))^2)
  
## Slope confidence interval 
## Walpole 11P4
## 95% confidence, t-dist
  tscores <- qt(c(.025, .975), df=degf)   # 5 degrees of freedom
  slope_low_limit <- mslope + tscores[1]*s/Sxx^0.5
  slope_high_limit <- mslope + tscores[2]*s/Sxx^0.5

## Hypothesis Test for Slope
## Null Hypothesis:        slope = 0
## Alternate Hypothesis:   slope > 0
## t-statistic: the Student-t distribution parameter  
## p-value: the probability that the Null hypothesis true  

  t0 <- (0 - mslope)/(s/Sxx^0.5)
  p0 <- 2 * pt(t0, degf, lower.tail = TRUE)

## The Coefficient of Determination expresses what fraction of the variance
## of the dependent variable is explained by the independent variables.

## R-Squared: Coefficient of Determination
  rsquared <- (sst - sse)/sst

## Pearson's Correlation Coefficient
  r_corr <- cov(x,y)/(sd(x)*sd(y))
## The square of Pearson's Correlation Coefficient is identical 
## to the Coefficient of Determination; i.e., R-Squared, for
## the case only one independent variable.

  # fit_xmu  <- 20
  # fit_ymu <- mslope*fit_xmu +intrcpt
  # s_term <- ((fit_xmu - xbar)^2)/Sxx
  # tscores <- qt(c(.025, .975), df=degf)   
  # low_ymean_limit <- fit_ymu + tscores[1]*s*(1/n + s_term)^0.5
  # high_ymean_limit <- fit_ymu + tscores[2]*s*(1/n + s_term)^0.5

  # low_pop_limit <- fit_ymu + tscores[1]*s*(1+1/n + s_term)^0.5
  # high_pop_limit <- fit_ymu + tscores[2]*s*(1+1/n + s_term)^0.5

# plot
  fit_xmu  <- seq(fit_min, fit_max,by=25)  # one shorter than data
  fit_ymu <- mslope*fit_xmu +intrcpt
  
  s_term <- ((fit_xmu - xbar)^2)/Sxx
  tscores <- qt(c(.025, .975), df=degf)   
  low_ymean_limit <- fit_ymu + tscores[1]*s*(1/n + s_term)^0.5
  high_ymean_limit <- fit_ymu + tscores[2]*s*(1/n + s_term)^0.5

## Population, Walpole 11P7 
  low_pop_limit <- fit_ymu + tscores[1]*s*(1+1/n + s_term)^0.5
  high_pop_limit <- fit_ymu + tscores[2]*s*(1+1/n + s_term)^0.5

  fit_x  <- seq(fit_min, 300,by=50)  # one shorter than data
  fit_y <- mslope*fit_x +intrcpt
  list(fit, c(mslope, intrcpt),y_est,c(Sxx, Syy, Sxy),degf,c(s_sqr,s),
            c(sse, sst), c(rsquared, r_corr),
            list(low_ymean_limit,high_ymean_limit),
            list(low_pop_limit, high_pop_limit),
            list(fit_xmu, fit_ymu),list(fit_x, fit_y),
         ## Null interecept hypothesis : To be included later
            c(slope_low_limit, slope_high_limit), c(t0, p0),
         ## Null slope hypothesis 
            c(slope_low_limit, slope_high_limit), c(t0, p0)) 
}

get_purged_mse <- function(purged_row,input_data,y) {
  purged <- input_data[-purged_row,]
  purged_y <- y[-purged_row]
  n <- length(purged_y)
  input_data_dim  <- dim(input_data)
  ## Degrees of freedom
  def_total <- n - 1
  model_column_count <- input_data_dim[2] + 1
  dim_soln <- model_column_count
  ## Degrees freedom for Model:
  def_subtractor <- dim_soln - 1
  def_model <- def_subtractor # k
  def_error <- def_total - def_model
  sse <- get_sse(purged,purged_y)
  (sse / def_error)^0.5 
}

get_hii <- function(input_data,y) {
  dat <- input_data
  n <- length(y)
  a <- get_data_matrix(dat)
  at <- t(a)
  inv_sqr <- get_inv_sqr(dat)
  hat <- a %*% inv_sqr %*% at 
  diag(hat)
}


get_purged_hii <- function(purged_row,input_data,y) {
  purged <- input_data
  purged_y <- y
  n <- length(purged_y)
  a <- get_data_matrix(purged)
  at <- t(a)
  inv_sqr <- get_inv_sqr(purged)
  hat <- a %*% inv_sqr %*% at 
  v <- diag(hat)
  v
}

get_purged_sst <- function(purged_row,input_data,y) {
  purged_y <- y[-purged_row]
  n <- length(purged_y)
  i <- diag(n)
  one <- matrix(rep(1,n),ncol=1)
  one_t <- t(one)
  yt <- t(purged_y)
  
  one_sqr <-  one_t %*% one
  inv_one <- 1/one_sqr
## Total sst
  yt %*% (i-one %*% inv_one %*% one_t) %*% purged_y
}

anova_model <- function(input_data,y,x0){
  ## Total Discrete values of the independent variable
  n <- length(y)
  x0 <- matrix( c(1,x0),ncol = 1)
  if(class(input_data) != "data.frame"){
     input_data <- data.frame(input_data)  
  }
  
  input_data_dim  <- dim(input_data)
  #n <- input_data_dim[1]
  
  input_column_count <- input_data_dim[2]
  model_column_count <- input_data_dim[2] + 1
  dim_soln <- model_column_count
  obs <- 1:n
  input_size_vector <- 1:input_column_count
  model_size_vector <- 1:model_column_count
  
  mu_list <- lapply(input_size_vector, function(u){
    mean(input_data[,u])
  })
  mu_vec <- c() 
  mu_vec[1] <- 1 
  for (i in input_size_vector){
    mu_vec[i+1] <- mean(input_data[,i])
  }
  mu_0 <- matrix( mu_vec,ncol = 1)
  mu_0_t <- t(mu_0)
  y_mean <- mean(y)

################################################################## Model parameter 1
  syy <- sum((y - mean(y))^2)
  sy <- sum(y)
  s_x1_y <- sum(x1   *y)
  
  x <- input_data[,1]
  Sxx <- sum((x - mean(x))^2)
  if (input_column_count ==1){
    x <- input_data[,1]
    Sxy <- sum((x - mean(x))*(y - mean(y)))
  }
  
################################################################## Model parameter 2
  
  ## g <- xt * y 
  g <- c() 
  g[1] <- sum(y) 
  for (i in input_size_vector){
    j <- i+1
    g[j] <- sum(input_data[,i]*y)
  }
  
  # List x0 input vectors
  data_vector_list <- list()
  for (i in seq_along(y)){
    row_v <- c(1)
    for (j in input_size_vector){
      row_v <- c(row_v, input_data[i,j])
    }
    data_vector_list[[i]] <-  row_v  
  }
  fit_list <- data_vector_list
  
  ###################################
  
  var_names <- c("y-intercept")
  for (i in input_size_vector){
    curr_var <- paste("x",i,sep="")
    var_names <- c(var_names,curr_var )
  }
  
  ###################################
  
  ## SST: Corrected Total Variability 
  def_total <- n - 1
  
  ## Degrees freedom for Model:
  def_subtractor <- dim_soln - 1
  def_model <- def_subtractor # k
  
  ## This is one less than the length the b vector  
  ## and the dimension the matrix.
  ## For one variable, dim_soln = 2, SSR = 1
  ## We always subtracted 2 from n for SSE freedom 
  
  ## Error
  def_error <- def_total - def_model # n - k - 1
  ## This case is similar to the one variable case
  ## which was n - 2
  
  deg_freedom_v <- c(def_model,def_error,def_total)
  source <- c("model","error","total")
  
  ## We needed to square or obtain the dot product the rows of A
  ## this way. Let At be the transpose of A
  ## SSE <- At %*% A
  
  ########################################
  
  ## Matrix solution
  
  ## Parameter Estimate Vector
  
  ########################################################## Model parameter 3
  a  <- get_data_matrix(input_data)
  at <- t(a)
  yt <- t(y)
  y <- t(yt)
  
  i <- diag(n)
  one <- matrix(rep(1,n),ncol=1)
  one_t <- t(one)
  one <- matrix(rep(1,n),ncol=1)
  one_t <- t(one)
  one_sqr <-  one_t %*% one
  inv_one <- 1/one_sqr

  inv_sqr <- get_inv_sqr(input_data)
  unknown <- inv_sqr %*% at %*% y
  b <- unknown
  #b <- matrix(c(3.829633,0.903643),ncol=1)
  #b <- matrix(c(3.8296,0.9036),ncol=1)
  est_coef <- b
# Walpole p405 est_coef <- c(3.829633,0.903643)
# my soln est_coef      <- c(3.82965, 0.90376)
  
  ################################################# Model parameter 5 (Text only)
  est_coef_txt <- formatC(b[,1],format = "f",digits=4)
  allcolumns = lapply(model_size_vector,function(u){
    est_coef[u]* a[,u]
  })
  
  prediction_matrix <- as.matrix(data.frame(allcolumns))
  # b <-  (inv_sqr %*% at) %*% y
  # predictions <- a %*% b
  # predictions2 <-  (a %*% inv_sqr %*% at) %*% y 
  
  ## The matrix solution for sse affected by the accuracy of inverse matrix
  ## calculation.
  ## sse <- abs(yt %*% (i-a %*% inv_sqr %*% at) %*% y)[1,1]
  
  predictions <-  apply(prediction_matrix, 1, sum)

  residual <- y - predictions
# residual2 <-  y - (a %*% inv_sqr %*% at) %*% y
  
## Error: sse
  sse <- sum(residual^2)     ## walpole 405 sse 38.6632
## Total Variance sst; the numerator when you calculate variance
  sst <- yt %*% (i-one %*% inv_one %*% one_t) %*% y
  sst # 438.13077
  
  ## The Model or Regression Sum of Squares
  ## ssr <- yt %*% (a %*% inv_sqr %*% at - one %*% inv_one %*% one_t) %*% y
  ## Inverting the matrix for inv_sqr appears to introduce some accuracy issues.
  ssr <-  sum((predictions - mean(y))^2)
  ssr # 399.4676
  
## Matrix solution

## SAS Definition:
## coeff_var: Coefficient of variation
## ratio of the root MSE to the mean of the dependent variable
  # MSE matrix solution: (s^2) * (x0_t %*% inv_sqr %*% x0)
  s <-  (sse / (deg_freedom_v[2]))^0.5 
  coeff_var <-   coeff_var <- 100*s / y_mean
  
  ## s is the estimated variance. For Anova analysis, mean-squared-error (MSE)
  ## has a special definition as the sum of the squared errors (SSE) divided
  ## by the degrees of freedom.
  
  ## For ANOVA analysis, s is sometimes called Root MSE or even RMSE anyway, 
  ## although this follow the special definition for MSE. When referring to 
  ## MSE, RMSE, or Root MSE with the ANOVA definition, the case should always
  ## be clear because the degrees of freedom are involved. Even for a single
  ## independent variable, there are 2 degrees of freedom.
  ## s = 2; s <- (sse / (n-2))^0.5  # Walpole page 402
  
  ############################################################### Model parameter 4
  x0_t <- t(x0)
  
  ## The first component of ss is the Model or Regression Sum of Squares (ssr)
  ss <- c(ssr,sse,sst)
  ## The second component of ms, ms[2], is s^2, the variance estimate.
  ## The second component is obtained by dividing sse by the error
  ## of freedom which is def_total - def_model or n - k - 1.
  ms <- c(ss/deg_freedom_v)
  ##s_sqr <- ms[2]
  MSE <- sum(residual^2)/def_error
  
  MSE <- ms[2]
 
  #SSE_mean <- mean(residual^2)
  ms[3] <- NA
  
  ## Probability values
  f_value <- rep(NA,3)
  f_value[1] <- ms[1]/ms[2]
  
  p_value <- rep(NA,3)
  prob_model <- pf(f_value[1], 1, deg_freedom_v[3],lower.tail = FALSE)
  p_value[1] <- prob_model 

  df1 <- data.frame(source,deg_freedom_v,ss,ms,f_value,p_value)
  colnames(df1) <- c("Variation","Degrees of Freedom","Sum of Squares","Mean Square", "F-value", "p-value")

  ###############################################################
  
  dependent_mean <- y_mean
  
  ## Standard Error of Prediction

  #std_err_prediction <- s * (x0_t %*% inv_sqr %*% x0) / deg_freedom_v[2]
  r_square <- 1-ss[2]/(ss[3])
  adjusted_r_sqr <- 1-(ss[2]/(deg_freedom_v[2]))/(ss[3]/(deg_freedom_v[3]))
  
  ## Hat Matrix
  hat <- a %*% inv_sqr %*% at 
  hii <- diag(hat)
  
  sd_est <- s * (one - hii)^0.5
  
  x0_vec <- x0[,1] [2:dim_soln]
  x0_rounded <- formatC(x0_vec,format = "f",digits=4)
  x0_text <- paste(x0_rounded,collapse = ", ")
  
  b_vec <- formatC(b[,1] [2:dim_soln],format = "f",digits=4)
  b_text <- paste(b_vec,collapse = ", ")
  intrcpt <- b[1,1]
  b0_vec <- formatC(b[1,1] ,format = "f",digits=4)
  b0_text <- paste(b0_vec,collapse = ", ")
  #b0_var_coef <- formatC(inv_sqr[1,1] ,format = "f",digits=4)
  #b0_var_text <- paste(b0_var_coef ,collapse = ", ")
  
  trace_vec <- formatC(diag(inv_sqr)[2:dim_soln] ,format = "f",digits=4)
  trace_text <- paste(trace_vec,collapse = ", ")
  
  b0_stderr <- formatC(s*(inv_sqr[1,1])^0.5 ,format = "f",digits=4)
  b0_error_text <- paste(b0_stderr ,collapse = ", ")
  b_stderr <- formatC(s*(diag(inv_sqr)[2:dim_soln])^0.5 ,format = "f",digits=4)
  b_error_text <- paste(b_stderr ,collapse = ", ")
  b_std_error <- s * (diag(inv_sqr))^0.5
  
## Item-2 
  fit_parameter <- c("s","Mean Outcome", "Percent Variance","$R^{2}$","Adjusted $R^{2}$")
  fit_value <- c(s,dependent_mean, coeff_var, r_square, adjusted_r_sqr)
  
  regression_fit <- data.frame(fit_parameter,fit_value)
  colnames(regression_fit) <- c("fit_parameter","value")

## Item-2B
  independent_input <- x0_text
  
## Item-2C
  ##intercept_coef <- b0_var_text
  
  df_regression_model <- data.frame(inv_sqr[1,1], independent_input)
  value <- b_text
  #coefficients <- data.frame(independent_input,b[,1])
  coefficients <- b[,1]
  
  ## Item-2D
  ## Walpole 11P5## Null Hypothesis test that b[1,1] = 0, the intercept 
  ## Include both tails
  if(model_size == 1){
     intr_factor <- (sum_xsquared/(n*Sxx))^0.5
     b_std_error[1] <- s* intr_factor
  }
  
  b_std_error_txt <- formatC(b_std_error,format = "f",digits=4)
  t_value <- formatC(b/b_std_error,format = "f",digits=4)
  
  ## number_of_vars <- 3 ## x1, x2, x3
  number_of_vars <- dim_soln - 1
  degf <- n - number_of_vars - 1 # n is 13
  
  se_vec <- unlist(lapply(model_size_vector, function(u){
      (inv_sqr[u,u])
  }))
  se_coef <- sum(se_vec)
  if(model_size == 1){
     slope_factor <- 1/(Sxx^0.5)
     se_coef <-  s * slope_factor 
  }

## Slope confidence interval 
## Walpole 11P4, 12P7, 95% confidence, t-dist
## For example: Probability of b[3] = 0, diagonal variance elemnt xsqr_inv[3,3]
   tscores <- qt(c(.025, .975), df=degf)   # 9 degrees of freedom 
   sum_xsquared <- sum(x^2)
  
   t_vec <- unlist(lapply(model_size_vector, function(u){
     if(model_size == 1){
         slope_factor <- 1/(Sxx^0.5)
     } else {
         slope_factor <- (inv_sqr[u,u])^0.5
     }  
     se_coef <- s*slope_factor   
     (b[u,1]- 0) / se_coef
   }))
  
   p_vec <- unlist(lapply(model_size_vector, function(u){
     #se_coef <- s*(inv_sqr[u,u])^0.5
     #t <- (b[u,1]- 0) /se_coef
     t <- t_vec[u]
     null_hypothesis_t_two_tail(t,degf)
   }))

## Intercept 

  #tscores <- qt(c(.025, .975), df=degf)   
  #intr_low_limit <- intrcpt + tscores[1]*s*intr_factor
  #intr_high_limit <- intrcpt + tscores[1]*s*intr_factor
  #intrcpt + tscores[2]*s*sum_xsquared^0.5/(n*Sxx)^0.5

## 95% confidence, t-dist
  #ti0 <- (intrcpt-0)/(s*intr_factor)
  #pi0 <- 2 * pt(ti0, degf, lower.tail = TRUE)

## number_of_vars <- 3 ## x1, x2, x3
## number_of_vars <- length(x_rec) - 1
## degf <- n - number_of_vars - 1 # n is 13

## variable <- c("y-intercept","x1","x2","x3")
   variable <- unlist(lapply(input_size_vector, function(u){
     paste0("$c_",u,"$")
   }))
   variable <- c("$c_0$",variable)
  
  df3 <- data.frame(variable,est_coef,b_std_error_txt,t_value,p_vec)
  colnames(df3) <- c("Coefficient","Estimate","Std_Error","student_t_value","p_value")
  
  ################################################ Model parameters 7, 8, (ncol)
  
  fit_xmatrix <- matrix(unlist(fit_list),ncol=dim_soln,byrow = TRUE)
  
  ########################################################## Model parameters 6 (Null Hypothesis above)

  hat <- unlist(lapply(model_size_vector, function(x){
    t(fit_xmatrix[x,]) %*% inv_sqr %*% fit_xmatrix[x,] 
  }))
  
  std_error <- unlist(lapply(obs, function(x){
   s * ( t(fit_xmatrix[x,]) %*% inv_sqr %*% fit_xmatrix[x,])^0.5 
  }))
  
  ## t-dist for 95% confidence interval
  ## var_y0 <- t(fit_xmatrix[x,]) %*% inv_sqr %*% fit_xmatrix[x,]
  std_var_mean_y0 <- unlist(lapply(obs, function(x){
    t(fit_xmatrix[x,]) %*% inv_sqr %*% fit_xmatrix[x,] 
  }))
  
  ## 95% confidence interval mean
  mean_low_limit  <- predictions + tscores[1]*s*std_var_mean_y0^0.5
  mean_high_limit <- predictions + tscores[2]*s*std_var_mean_y0^0.5
  
  var_y0_pop <- unlist(lapply(obs, function(x){
    1 + t(fit_xmatrix[x,]) %*% inv_sqr %*% fit_xmatrix[x,] 
  }))
  
  ## 95% confidence interval for population
  pop_low_limit  <- predictions + tscores[1]*s*var_y0_pop^0.5
  pop_high_limit <- predictions + tscores[2]*s*var_y0_pop^0.5


 
  df4 <- data.frame(y,predictions,std_error,
                     mean_low_limit,mean_high_limit,
                     pop_low_limit,pop_high_limit,residual)
  colnames(df4) <- c("y_model","prediction","std_error",
                     "std_low_limit","std_high_limit",
                     "pop_low_limit","pop_high_limit","residual_error")
  row.names(df4) <- row.names(input_data) 
  
  ri <- (residual/sd_est) [,1]
  # sd_est
  sd_ti <- sapply(obs, function(u){
     sp <- get_purged_mse(u,input_data,y) 
     sp*(1 - hii[u])^0.5  
  }) 
  
  mse_purged <- sapply(obs, function(u){
    get_purged_mse(u,input_data,y) 
  })
  mean_purged_mse <- mean(mse_purged^2)
  
  sst_purged <- sapply(obs, function(u){
    get_purged_sst(u,input_data,y) 
  })
  ti <- residual/sd_ti
  
  residuals <- data.frame(y,predictions,residual,hii,sd_est,
                            ri,ti)
  deltai <- residual/ (1 - hii) 
  delta_abs <- sum(abs(deltai))
  pressi <- deltai^2 
  press <- sum(deltai^2)
  r_sqr_predict <- 1 - press/syy
 
  ## Table 12.13
  ## In the first table column, S^2 is based the deleted variance.
  ## The sum of the deleted variances is divided by def_error which 
  ## is the error degrees of freedom or deg_freedom_v[2]
  r_squared <- r_square
  compare_press <- data.frame(model_column_count,MSE,mean_purged_mse,
                              delta_abs,press,
                              r_squared,
                              r_sqr_predict, 
                              adjusted_r_sqr)
  compare_press[,"id"] <- seq_len(nrow(compare_press))
  list(df1,
       list(regression_fit, df_regression_model, coefficients, t_vec, p_vec,
                coeff_var, inv_sqr), 
       df3,df4,residuals,compare_press)
}

get_model_data <- function(input_data,y,model_result,input_column_count,num_vars,list_s2){
  mp <- combinations(n=input_column_count,r=num_vars,v=c(1:input_column_count),repeats.allowed=F)
  row_size <- dim(mp)[1]
  model_list <- lapply(1:row_size, function(u){
    mp[u,]
  })
  for(i in seq_along(model_list)){
    cols <-  model_list[[i]]
    model_data <- input_data[,cols]
    model_names <- colnames(model_data)
    ## Compare press, Item-6
    calc <- anova_model(model_data,y,cols)[[6]]
    row <- as.data.frame(calc)
    row[1,"model"] <- paste0(cols, collapse = ", ")
    row[1,"model_vars"] <- paste0(model_names, collapse = ", ")
##  Append model to list    
    model_result <- rbind(row,model_result)
    list_s2 <-  c(list_s2,calc[1,"MSE"])
  }
  list(model_result,list_s2)
}

```

```{r modelcomparison, echo=FALSE}

get_model_comparison <- function(input_data,y,model_names,sigma_hat){
  input_data_dim  <- dim(input_data)
  ## The data frame 'input_data' does not include a result or outcome column.
  input_column_count <- input_data_dim[2]
  x0 <- c(1)
  input_data1 <- input_data[,1]
  modelanova <- anova_model(input_data1,y,x0)[[6]]
  model_result <- as.data.frame(modelanova)
  model_result[1,"model"] <- 1
  model_result[1,"model_vars"] <-  model_names[1]  ## "x1","x2","x3"

# Get data for singlet models
  
  list_s2<- c()
  model_list <- lapply(1:input_column_count, function(u){
    c(u)
  })
  for(i in seq_along(model_list)){
    cols <-  model_list[[i]]
    x0 <- cols
 ## The data frame 'model_data' is the list item or current model
 ## being evaluated. It never includes a result or outcome column.    
    model_data <- input_data[,cols]
    calc <- anova_model(model_data,y,x0)[[6]]
    row <- as.data.frame(calc)
    row[1,"model"] <- paste0(cols, collapse = ", ")
    row[1,"model_vars"] <- model_names[i]
    model_result <- rbind(row, model_result)
    list_s2 <-  c(list_s2,calc[1,"MSE"])
  }
  
## Get data for multiple variable models  
## input_data is original raw data;
## model_result is output from anova_model
  for(i in seq_along(2:input_column_count)){
    j <- i + 1  
    rowlist <- get_model_data(input_data,y,model_result,input_column_count,num_vars=j, list_s2)
    model_result <- rowlist[[1]]
    list_s2 <-  rowlist[[2]]
  }
  last_row <- dim(model_result)[1] - 1
  ## "model_column_count" "MSE"  "mean_deleted_mse" delta_abs" "press"             
  ## [5] "r_squared"          "r_sqr_predict"      "adjusted_r_sqr" 
  ## [8]"id"                 "model"            
  ## [10] "model_vars"
  
  answer <- model_result[ ,c(9:11,1:8)][1:last_row,]
  
  row_count <- dim(answer)[1]
  sigma_num <- answer[1,"MSE"]
  sigma <- rep(sigma_num, row_count)
  
  #p2 <-  xfold_data[,"model_column_count"]
  #p <- p2 - 1
  #numvars <- max(p2) -  1
  #maxvars <- max(p2)
  #sigma_hat <-  xfold_data[xfold_data$model_column_count==maxvars,][1,"MSE"]
  ## Variance for most complete model at Cp = n
  ##sigma_hat <- 26.2075
  
  # cp1 <- (xfold_data[,"MSE"] - sigma_hat) * (rep(input_data_dim[1], row_count) - p)
  # cp2 <- p + (cp1 / sigma_hat)
  # xfold_data[,"cp"] <- cp2
  # 
  # colnames(xfold_data) <- c("id","Model Numbers", "Model", "Model_Size",
  #                           "MSE",  "MSE_mean",  "Delta","PRESS",
  #                           "R^2", "R^2_Pred","adjusted_r_sqr",
  #                           "Cp")
  
  p <-  answer[,"model_column_count"]
  ##p <- rep(( answer[,"model_column_count"]),length(list_s2))
  cp1 <- (answer[,"MSE"] - sigma) * (rep(input_data_dim[1], row_count) - p)
  #cp1 <- (answer[,"s_sqr_deleted"] - sigma) * (rep((  input_data_dim[1] - (input_data_dim[2]+1) ),length(list_s2)))
  cp2 <- p + (cp1 / sigma)
  answer[,"cp"] <- cp2

  ordered <- answer[order(answer[,"cp"],decreasing = FALSE),]

  ordered[,c(4:12)] <- sapply(c(4:12),function(u)
    {as.numeric(formatC(ordered[,u], format = "f", digits = 4))}
  )
  row.names(ordered) <- NULL
  ordered[,"id"] <- 1:row_count
  colnames(ordered) <- c("Row","Model","Model_Size",
                         "MSE","Avg_Deleted_MSE",
                         "Delta_Sum","PRESS","R^2","R_Predict","adjusted_r_sqr",
                         "Cp")
  ordered
}
get_press_comparison <- function(dat){
  #dat <- xfold_data[,2:13]
  ##print(colnames(ord))
  ## "param_list"(row), "Model","Model_Size",
  ## "MSE","Avg_Deleted_S^2","S^2_Variance_Est" "Delta_Sum","PRESS"           
  ##  "R^2","R_Predict","Avg_Deleted_R^2","adjusted_r_sqr","Cp"
  ##ord[,-c(10)] # Remove "Avg_Deleted_R^2"
  headed <- dat[1:4,]
  print(headed, quote = FALSE, row.names = TRUE)
  dat[1:30,c(11,6,7,8,2,1)]
}
get_cp_comparison <- function(dat){
  ## "Model","Model_Size",
  ## "MSE","Avg_Deleted_S^2","S^2_Variance_Est" "Delta_Sum","PRESS"           
  ##  "R^2","R_Predict","Avg_Deleted_R^2","adjusted_r_sqr","Cp"
  ##ord[,-c(10)] # Remove "Avg_Deleted_R^2"
  dat[1:30,c(12,7,8,9,3,1)]
}

################################################################

get_simple_data <- function(df,threshold=0.25){
   #df_years <- df[,"Year"]
   june_rain <- as.numeric(as.character(df$JunRain))
   year <- df[,"Year"]
   #training_year <- df[,"Year"]
   ## Minimum correlation with June rain
   column_names <- colnames(df)
   cols <- dim(df)[2]
   colvec <- c(1:cols)
   corrs <- unlist(lapply(colvec, function(x){cor(june_rain, df[, x])[[1]][1]}))
   #threshold <- 0.25
   helpfuldata <- unlist(lapply(corrs, function(x){!is.na(x) &
       ((x < -threshold  | x > threshold ))[[1]][1]}))
   helpfulcolnames <- column_names[helpfuldata==TRUE & column_names!="FebAverageIrrad"]
   helpfulcols <- train[,helpfulcolnames]
## Eliminate noisy redundant correlated columns 
   corrmatrixjuneA <- cor(helpfulcols)
   redundant <- findCorrelation(corrmatrixjuneA, cutoff = .70)
   cols <- dim(helpfulcols)[2]
   colvec <- c(1:cols)
   remaining <- colvec[-redundant]
   reduced <- helpfulcols[,remaining]
   list(year,reduced,june_rain)
}

get_validation_data <- function(df,selectedcolnames){
   june_rain <- as.numeric(as.character(df$JunRain))
   year <- df[,"Year"]
   reduced <- df[,selectedcolnames]
   list(year,reduced,june_rain)
}

formatcolheads<-function(x) {
   sanitize<-get("sanitize", parent.frame())
   x<-sanitize(x) #replace special features from xtable output
   x<-gsub("lambda","$&lambda;$",x) #latex mathematical replacement for chisq
   #<-gsub("p","{\\\\it p}",x) #italicize p
   x
}

get_heat_map <- function(corrmatrix){
   corr <- as.data.frame(corrmatrix)
   cols <- brewer.pal(dim(corr)[2], "RdBu")
   rowcount <- dim(corr)[1]
   meltcount <-  rowcount + 1
   rowid <- c(1:rowcount)
   idcol <- rowcount+1
   rgbpalette <- colorRampPalette(c(rgb(0,0,0.99),
                                 rgb(0.99,0.99,0.99),
                                 rgb(0.8,0.2,0)), space = "Lab")
   par(bg = rgb(145,245,250, max=255))
   melted <- melt(corr)
   melted[,"var"] <- rep(row.names(corrmatrix), rowcount)
   melted[,"id"] <- as.factor(rep(rowid, rowcount))
   melted <- melted[order(melted$id),] 
   ggplot(melted, aes(id, variable)) +
     ggtitle("Correlation Heat Map\nFigure-1") +
     geom_tile(aes(fill = value), colour = "white") +
     scale_fill_gradientn(
        colors=c('#00EEBB','#EEBB00','#FF3300'),
        values=c(-0.5,0.5,1)) +
     coord_cartesian(xlim = c(1, 22)) +
     scale_x_discrete(labels= c(row.names(corrmatrix))) +
     xlab(label = "") + ylab(label = "") + 
     theme(plot.title = element_text(size=14,hjust = 0.5, color="#008800"),
       legend.title = element_text(size=16),
       legend.text = element_text(size=14),
       legend.background = element_rect(fill = "#DDDDEE"),
       legend.key = element_rect(colour = "transparent", fill = "#DDDDEE"),
       text = element_text(size=10),
       axis.text = element_text(size=10,angle=0, hjust=0),
       axis.text.x = element_text(size=12,angle=88, hjust=-0.05, vjust=-0.1),
       axis.ticks.length = unit(0.15, "cm"),
       panel.grid.minor =   element_line(color = "white",size=0.6),
       panel.grid.major =   element_line(color = "light blue",size=0.8),
       panel.background = element_rect(fill = "#EFE5E5", color = "#008800",
            size = 0.5, linetype = "solid"),
       plot.background = element_rect(fill = "#DDDDEE",color = "#DDDDEE"))
}

#############################################################################

## Function-: Adding solar data 

## Make solar irradiation year records using provided raw data

processing <- function(planet="earth",sample_rate=8000,
                       decimator,fname){
  #setwd("C:/Users/merch/OneDrive/Documents/physics/earthScience/jupiter")
  pie <- 3.1415926535897932385
  two_pie <-  2 * 3.1415926535897932385
  #solar_system <- SolarSystem$new(planet=planet)
  dfresult <- read.csv(fname,stringsAsFactors=FALSE, row.names = NULL)
  ## First record is July 5, 1958 at earth aphelion
  totalrecords <- (length(dfresult$planet_x_coord))/decimator
  fast <- (10 * 1.1 * sample_rate)/decimator + 1
  # [1] 11001  (before decimation [1] 88000)
  orbit1 <- fast + 11.62*sample_rate/decimator + 1
  
  timelt <- as.POSIXlt(x=dfresult[,1],tz="GMT",origin="1958-07-05 01:36:39 GMT")
  ## dfresult$time[1] = 0 = -362,701,401 seconds
  year <- timelt$year + 1900
  month <- timelt$mon + 1
  monthday <- timelt$mday
  
  dfresult[,"Year"] <- year
  dfresult[,"Month"] <- month
  dfresult[,"Day"] <- monthday
  ## Reduce possible multiple records to day records
  agg1 <- aggregate(dfresult, by=list(year=dfresult$Year,month=dfresult$Month,
                                      day=dfresult$Day), FUN=mean)
  dates <- as.character(as.Date(floor(agg1$time/86400),origin="1958-07-05 01:36:39 GMT"))
  agg1$Date <- as.character(dates)
  agg1 <- agg1[agg1$year!=1958,]
  # dim(agg1)
  # head(agg1)
  yearly_solar_data <- agg1[order(agg1$year,agg1$month,agg1$day),]
  yearly_solar_data <- yearly_solar_data[,c(-34,-35,-36)]
  d_planet_sun <- yearly_solar_data[,"d_planet_sun"]
  
  #watt_m2 <- 1368
  watt_m2 <- 1361
  
  ys1 <- d_planet_sun
  msed <- (1/mean(ys1^(-2)))^0.5
  ## [1] 149642243647
  # mean inverse square is [1] 4.465721e-23
  # Reciprocal is [1] 2.23928e+22
  
  r <- msed
  area <- 2 * two_pie * r^2
  
  # effective radius of the earth's orbit is 
  msed / 10^9  # [1] 149.6422 million kilometers
  
  total <- area * watt_m2  
  # [1] 3.829803e+26 384.6 yotta watts (3.846×1026 watts)
  # https://en.wikipedia.org/wiki/Solar_irradiance
  # 3.829803e+26 based msed mean inverse square root distance
  
  yw <- 3.829803e+26 
  
  #irrad <- 1361*(msed/ys1)^2
  irrad_mean <- yw/(2*two_pie*msed^2)
  irrad <- yw/(2*two_pie*d_planet_sun^2)
  ##irrad <- watt_m2*(msed/ys1)^2
  ## [1] 1361 Watts/ square meter
  
  yearly_solar_data[,"irrad"] <- irrad
  yearly_solar_data <- yearly_solar_data[,c(34,1:33,35)] 
  # df <- solar_data
  # planet="earth"
  # analysis <- OrbitAnalysis$new(planet, df=df, theta0, optimized=FALSE)
  
  ## Making solar irradiation summary data frame
  ## and processing results from run_orbit_analyzer function
  ## list(analysis,totalrecords,fast,orbit1,irrad)
  yearly_solar_data
}

#############################################################################

## Function-: Make merged meteorological data file
## combining weather and lake-level file with solar irradiation 

make_merged_meteorological_file  <- function(weather_fname,solar_df){
  year_weather_records <- read.csv(weather_fname,
                         stringsAsFactors=FALSE, row.names = NULL)
  yearlymeteorological <- year_weather_records
  dim(yearlymeteorological)
  yrvec <- yearlymeteorological$Year
  
  YearAverageIrrad <- sapply(yrvec,function(x){
    mean(solar_df[solar_df$year==x,"irrad"])
  })
  JanAverageIrrad <- sapply(yrvec,function(x){
    mean(solar_df[solar_df$year==x & solar_df$month==1,"irrad"])
  })
  FebAverageIrrad <- sapply(yrvec,function(x){
    mean(solar_df[solar_df$year==x & solar_df$month==2,"irrad"])
  })
  MarAverageIrrad <- sapply(yrvec,function(x){
    mean(solar_df[solar_df$year==x & solar_df$month==3,"irrad"])
  })
  AprAverageIrrad <- sapply(yrvec,function(x){
    mean(solar_df[solar_df$year==x & solar_df$month==4,"irrad"])
  })
  MayAverageIrrad <- sapply(yrvec,function(x){
    mean(solar_df[solar_df$year==x & solar_df$month==5,"irrad"])
  })
  JunAverageIrrad <- sapply(yrvec,function(x){
    mean(solar_df[solar_df$year==x & solar_df$month==6,"irrad"])
  })
  JulAverageIrrad <- sapply(yrvec,function(x){
    mean(solar_df[solar_df$year==x & solar_df$month==7,"irrad"])
  })
  AugAverageIrrad <- sapply(yrvec,function(x){
    mean(solar_df[solar_df$year==x & solar_df$month==8,"irrad"])
  })
  SepAverageIrrad <- sapply(yrvec,function(x){
    mean(solar_df[solar_df$year==x & solar_df$month==9,"irrad"])
  })
  OctAverageIrrad <- sapply(yrvec,function(x){
    mean(solar_df[solar_df$year==x & solar_df$month==10,"irrad"])
  })
  NovAverageIrrad <- sapply(yrvec,function(x){
    mean(solar_df[solar_df$year==x & solar_df$month==11,"irrad"])
  })
  DecAverageIrrad <- sapply(yrvec,function(x){
    mean(solar_df[solar_df$year==x & solar_df$month==12,"irrad"])
  })
  
  yearlymeteorological[,"YearAverageIrrad"] <- YearAverageIrrad
  
  yearlymeteorological[,"JulAverageIrrad"] <- JulAverageIrrad
  yearlymeteorological[,"AugAverageIrrad"] <- AugAverageIrrad
  yearlymeteorological[,"SepAverageIrrad"] <- SepAverageIrrad
  yearlymeteorological[,"OctAverageIrrad"] <- OctAverageIrrad
  yearlymeteorological[,"NovAverageIrrad"] <- NovAverageIrrad
  yearlymeteorological[,"DecAverageIrrad"] <- DecAverageIrrad
  yearlymeteorological[,"JanAverageIrrad"] <- JanAverageIrrad
  yearlymeteorological[,"FebAverageIrrad"] <- FebAverageIrrad
  yearlymeteorological[,"MarAverageIrrad"] <- MarAverageIrrad
  yearlymeteorological[,"AprAverageIrrad"] <- AprAverageIrrad
  yearlymeteorological[,"MayAverageIrrad"] <- MayAverageIrrad
  yearlymeteorological[,"JunAverageIrrad"] <- JunAverageIrrad
  
  irrad_df <- data.frame(yrvec,YearAverageIrrad,
             JulAverageIrrad, AugAverageIrrad, SepAverageIrrad,
             OctAverageIrrad, NovAverageIrrad, DecAverageIrrad,
             JanAverageIrrad, FebAverageIrrad, MarAverageIrrad,
             AprAverageIrrad, MayAverageIrrad, JunAverageIrrad)
  
  write.csv(yearlymeteorological, "junkyearlyChicagoMeteorologicalWeather.csv",
            row.names = FALSE)
  yearlymeteorological
}

## Function-: process_meteorological_file

## Make irradiation data file 

process_meteorological_file <- function(
     weather_file_name="weather_year_records.csv",
           solar_fname="raw_solar_data.csv")
{
  yearly_solar_data_summary <- processing(decimator=8, fname=solar_fname)
  # analysis <- analysis_out[[1]]
  # solar_data <- analysis$orbit_data
  ## Function merges irradiation with weather and lake-level data
  make_merged_meteorological_file(weather_file_name, yearly_solar_data_summary)
}

```

#  Overview

The Farmer\'s Almanac advises that ["A cold and wet June spoils the rest of the year"](https://www.farmersalmanac.com/june-weather-lore-and-more-2945). It might seem foolhardy to improve upon a maxim like this one or try to predict anything about weather, but this investigation nevertheless considers whether anything might be gained if we apply machine-learning or artificial intelligence to weather data records from O'Hare Airport at Chicago, Illinois covering the years from 1960 to 2018. This analysis certainly shows the difficulties in weather prediction, but perhaps it shows that the likelihood or probability of a wet June appears be weakly correlated with snow in February and negative weak correlation with March temperature and other variables over the previous eleven months of a yearly weather record. 

   After exploring the relationship between June rainfall and weather data by looking for clusters in Section-5, a preliminary least-squares analysis in Section-7 and Section-8, and principal component analysis in Section-10, we develop four algorithms that predict the amount of rain for the month of June in Section-12. We compare the mean-squared error for four models using cross-validation in in Section-13, and finally perform final tests for which the bias, variance, and mean-square-error for each method is presented in Section-14.
   
   In trying to predict the amount of June rain, this report can only point to a relatively weak correlation with weather in the preceding months. The data indicates that the probability of a wet June is slightly correlated with heavy snow in February and cold weather in March and April. This lowers the probability of a dry June a little, but the difference between a dry June that affects crop yields and a less damaging one can be affected by one storm in the middle of the month. Another way of saying this is that the relatively large variance in June rain level is more crucial when the predicted total rainfall is small, particularly below 100-mm, compared to years with more rain. None of the dry years in the final test have predictions above 100-mm for June rain while the three wettest year in the training data have predictions above 100-mm for June rain as shown in the exploratory plot of Figure-6. 

<script>
sectionnum = sectionnum + 1;
</script>

# Raw Data

## Weather

<p class="p1">The raw data for O'Hare Airport was automatically downloaded following the application programming interface (API) at the National Centers for Environmental Information (NCEI) for the United States Government.</p>

<p class="p1">https://www.ncei.noaa.gov/access/services/data/v1?dataset=daily-summaries</p>
<p class="p1">The following query parameters were appended to the internet address to fetch the data:</p>
<p class="p1">stations=USW00094846</p>
<p class="p1">startDate=1958-01-01, endDate=2019-07-01</p>
<p class="p1">format=csv</p>


```{r downloadweatherdata, echo=FALSE}

## weather Data

## The field definitions for the weather data are given here.
## https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf
## https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt

## List of other weather stations:
## ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt

## The end date query parameter of the internet address should be updated for new downloads. The last download was for data up to July 1, 2019. 

## fileUrl <- "https://www.ncei.noaa.gov/access/services/data/v1?dataset=daily-summaries&stations=USW00094846&startDate=1958-01-01&endDate=2019-07-01&format=csv"

## download.file(fileUrl, dest="chicagoData2018.csv")

```

<p class="p1"> The raw lake-level data covers the period beginning with the 1958-1959 snow year and ends with the 2018-2019 snow year where a snow year begins on July 1 and ends the following calendar year on June 30. The download was saved as a file named <i>ChicagoData2018.csv.</i> </p>   

### Numerical Data Fields

<p class="p2">These fields were selected for analysis.</p>

<p class="p2">PRCP, Precipitation (tenths of mm)</p>
<p class="p2">SNOW, Snowfall (mm)</p>
<p class="p2">SNWD, Snow depth (mm)</p>
<p class="p2">TMAX, Maximum temperature in tenths of degrees (°C)</p>
<p class="p2">TMIN, Minimum temperature in tenths of degrees (°C)</p>
<p class="p2">AWND, Average daily wind speed (tenths of meters per second)</p>
<p class="p2">WDF2, Direction of fastest 2-minute wind (degrees)</p>
<p class="p2">WSF2, Fastest 2-minute wind speed (meters per second)</p>
<p class="p2">WSF5, Gust intensity as fastest 5-second wind speed (meters per second)</p>

<p class="p2">The raw data was processed to make a more uniform data table or frame where units as tenths of millimeters or tenths of degrees are simply expressed as millimeters or degrees respectively. </p>

### Categorical or Incident Report Fields

<p class="p2">These fields indicate whether not something occurred on a given day. A field with two possible values such as a medical diagnosis where a disease is present or not present is an example of a categorical variables. Another example is the WT03 field which indicates if thunder was reported on a given day or not.</p> 

<p class="p2">WT01, Fog, ice fog, or freezing fog (may include heavy fog)</p>
<p class="p2">WT03, Thunder</p>
<p class="p2">WT05, Hail</p>
<p class="p2">WT08, Smoke or haze</p>
<p class="p2">WT09, Blowing or drifting snow</p>
<p class="p2">WT11, High or damaging winds</p>
<p class="p2">WT13, Mist</p>
<p class="p2">WT16, Rain (may include freezing rain, drizzle, and freezing drizzle)</p>
<p class="p2">WT17, Freezing rain</p>
<p class="p2">WT18, Snow, snow pellets, snow grains, or ice crystals</p>

## Other Meterological Data

<p class="p1">Although we only use the above weather data from Station USW00094846 at O'Hare Airport for our final predictions, we also explored if the level of Lakes Huron-Michigan or solar irradiation levels were correlated with June rain, but this other meteorological data was eliminated from model-building and prediction as the airport weather data provides most of the predictive power. </p>

### Lake Huron-Michigan Water level

<p class="p2">The Army Corp of Engineering considers Lakes Huron and Michigan as a single body of water with the same average water level. The monthly mean water level for Station 9075014 at Harbor Beach, MI and Station 9087044 at Calumet Harbor in Illinois was automatically downloaded from the NOAA government website with query parameters to select the years from 1959 until 2019 as well as the IGLD datum which specifies a water-level correction for the slow elevation change caused by the ice loading during the ice age thousands of years ago.</p>

<p class="p2">The lake levels for the two stations were averaged together for the analysis to solve the problem of a few missing day reports. The raw downloaded data was saved using these file names:</p>

```{r lakestations1, echo=FALSE, results='asis'}
cat('<p class="p2">9075014 Harbor Beach, MI:  “huron_michigan_harbor_beach19592018.csv”</p>')
cat('<p class="p2">9087044 Calumet Harbor, IL: “huron_michigan_calumet_harbor19592018.csv.”</p>')

## NOAA "https://www.ncei.noaa.gov/access/services/data/v1?dataset=daily-summaries"

```

```{r lakestations2, echo=FALSE, results='asis'}

## Lake Huron-Michigan water level

## Harbor Beach Station, Michigan
## huronUrl <- "https://tidesandcurrents.noaa.gov/api/datagetter?begin_date=19590101&end_date=20190701&station=9075014&product=monthly_mean&datum=IGLD&units=metric&time_zone=gmt&application=web_services&format=csv"

## Calumet Harbor Station, Illinois
## michUrl <- "https://tidesandcurrents.noaa.gov/api/datagetter?begin_date=19590101&end_date=20190701&station=9087044&product=monthly_mean&datum=IGLD&units=metric&time_zone=gmt&application=web_services&format=csv"

# <p>product=monthly_mean</p>
# <p>datum=IGLD</p>

## The IGLD parameter specifies a standard correction the height above sea level that accounts for rebounding elevations due to the removal of the ice age glaciers about 10,000 years ago.

# download.file(huronUrl, dest="huron_michigan_harbor_beach19592018.csv")
# download.file(fileUrl, dest="huron_michigan_calumet_harbor19592018.csv")

```


<p class="p2">As only airport weather data was selected for the final models, the dimension of the solution was reduced by elimination of lake level variables from the model. However, the lake data is still part of the data frame of all variables.</p>

### Simulated Solar Irradiation Data

<p class="p2">Solar data was not selected for the final models despite having some low correlation with June rain as the weather variables in the airport data have higher correlation. As shown in Table-1, the highest absolute correlation for solar irradiation for a simplified solar system model is about 0.25 whereas weather variables such as February snow depth and March temperature have correlation of at least 0.3. Since it might be unwarranted to discard the perturbation of the earth's orbit and the solar irradiation levels for the earth caused by the gravitational mass of Jupiter without understanding how insignificant this might be, the solar data requires further study. The description of the simulated solar irradiation data is included in the Appendix at paragraph 15.6. The average yearly irradiation and month averages for January and July is shown at Section 15.6.2 by the plot in Figure-11 and the data in the table at the end of the report. </p>

<p class="p2">The code for generating the raw solar data is contained in the file named <i>SunJupiterEarthSimulation.R</i>. The raw data was processed to produce monthly and year averages as described below under preparing year records.</p>

<script>
sectionnum = sectionnum + 1;
</script>

#  Processing Raw Data

## Handling Missing Data

<p class="p1">The data quality for the airport and lake stations are very high. However, as fields were left blank for data entry where zero is obvious such as reports of blowing snow for July at Chicago and other binary categorical fields, there was some need convert the blanks to zero for the analysis. Where data is missing for binary categorical fields, blanks were replaced with 0. The binary categorical data fields are listed in paragraph 2.1.2 above.</p>

<p class="p1">Another type of obvious missing data is wind data for years before wind data was recorded. In this case, missing data is indicated as <i>NA</i> which helps data processing to identify missing data versus many other kinds of blank spaces in files.</p>

<p class="p1">Here is a list of changes for the modified data file:</p>

<div class="pdiv"><ol type="1">
<li>There are separate columns for the year, month, and date as integers.</li>
<li>Temperatures are now indicated in degrees Centigrade. In the original data, the temperature is multiplied by 10 to eliminate any decimals. </li>
<li>Rain precipitation is converted to millimeters where the raw PRCP field was expressed in tenths of a millimeter without decimals. For example, 1.2-mm of precipitation was originally recorded as 12 which may have been helpful for data entry. In the new data file, 1.2-mm of rain is expressed as 1.2.</li>
<li>The data begins in 1959 when maximum and minimium temperature are first reported over a full year.</li>
<li>Before average wind speed data was reported beginning in 1984, this data is indicated as NA in the new file. Since peak daily two-minute wind speed and direction as well as peak 5-second wind gusts are first reported for a full year in 1997, the missing data before 1997 for these fields are also indicated by <i>NA</i>.</li></ol></div>

<p class="p1">The processed version of the daily raw weather data was saved as this file:</p>
<p class="p2"><i>chicagoWeatherMetricUnits.csv</i></p>
<p class="p1 do-not-justify">The code that accomplishes this is in this file: </p>
<p class="p2"><i><i>prepareWeatherAndLakeLevelYearRecords.R</i></i></p>
<p class="p1 do-not-justify">This file also contains other data-processing code for making the file named <i>yearlyChicagoWeatherAndLakeLevel.csv</i> that is described directly below for preparing year records.</p>

## Preparing Year Records

<p class="p1">Year records consist of snow years beginning in July and ending in June. The raw daily record data was processed to produce snow year records beginning with the year designated as 1959 from July 1959 to June 1960 as the first year. For each raw daily weather variable, a year record is constructed from twenty-two variables including nineteen daily weather variables, and three lake-level variables for high, mean, and low average monthly water levels. For each of the twenty-two variables, the computed year record includes twelve monthly variables plus a thirteenth variable as the average for the entire year. A year record consists of 286 columns (13 X 22) plus one variable for the year. As a result, a data frame with 287 columns was prepared and saved as a file named <i>yearlyChicagoWeatherAndLakeLevel.csv</i>. </p>
   
<p class="p1">After generating the raw simulated irradiation data, the raw irradiation data was processed to produce a year record data frame and file that corresponds to the year records for weather and lake-levels. A year record for irradiation includes twelve monthly averages plus a thirteenth variable for the year average plus one variable for the year. The data frame of irradiation year records was saved as a file named <i>yearlyIrradiationData.csv</i>. The code for generating the raw irradiation data and year records is in the file named <i>SunJupiterEarthSimulation.R</i> </p>

<p class="p1">The thirteen solar irradiation columns were added to the 287 columns of weather and lake-levels and the combined file of 300 columns was saved as a file named yearlyChicagoMeteorologicalWeather.csv. The code for this final merging of data is at the end of the file named <i>SunJupiterEarthSimulation.R</i>.</p>


```{r merge_solar_data, echo=FALSE}

# solar_fname <- "earth_sample_rate8000e29301jup12409rev72.csv"
# solar_data <- read.csv(solar_fname, stringsAsFactors=FALSE, row.names = NULL)
# merged_data <- process_meteorological_file(
#       weather_file_name="yearlyChicagoWeatherSummary.csv",
#       solar_fname="earth_sample_rate8000e29301jup12409rev72.csv")
# merged_data
  
```

```{r read_yearlyChicagoMeteorologicWeather, echo=FALSE}
pie <- 3.1415926535897932385
two_pie <- 2 * 3.1415926535897932385

dfr <- read.csv("yearlyChicagoMeteorologicalWeather.csv",
                stringsAsFactors = FALSE)
dfr[is.na(dfr)] <- 0

all_june_rain <- dfr$JunRain  ## column 40

## Eliminate year total and June data for every weather category
smaller <- dfr[, c( 
    
## Maximum daily temperature
    #  -2, -3,-4,-5,-6,-7,-8,-9,-10, -11 , -12, -13, -14,
   -2,-14,
   
## Minimum daily temperature
    # -15,-16,-17,-18,-19,-20,-21,-22,-23,-24,-25,-26,-27,
    -15,-27,
   
    # Rain
    # -28,-29,-30,-31,-32,-33,-34,-35,-36,-37,-38,-39,-40,
    -28,-40,
   
    # Snow; Also eliminate months without snow 
    # -41,-42,-43,-44,-45,-46,-47,-48,-49,-50,-51,-52,-53,
     -41,-42,-43,-44,-45,-52,-53,
   
    # Snow depth; Also eliminate months without snow 
    # -54,-55,-56,-57,-58,-59,-60,-61,-62,-63,-64,-65,-66,
     -54,-55,-56,-57,-58,-65,-66,
   
    # Average wind speed
    # -67,-68,-69,-70,-71,-72,-73,-74,-75,-76,-77,-78,-79,
    -78,-79,
   
    # Peak 2-min wind
    # -80,-81,-82,-83,-84,-85,-86,-87,-88,-89,-90,-91,-92,
    -80,-92,
   
    # Peak 2-min wind direction
    # -93,-94,-95,-96,-97,-98,-99,-100,-101,-102,-103,-104,-105,
    -93,-105,
   
    # Peak gust wind speed
    # -106,-107,-108,-109,-110,-111,-112,-113,-114,-115,-116,-117,-118,
    -106,-118,
   
    # Fog
    # -119,-120,-121,-122,-123,-124,-125,-126,-127,-128,-129,-130,-131,
     -119,-131,
   
    # Thunder
    # -132,-133,-134,-135,-136,-137,-138,-139,-140,-141,-142,-143,-144,
    -132,-144,
   
    # Hail
    # -145,-146,-147,-148,-149,-150,-151,-152,-153,-154,-155,-156,-157,
    -145,-157,
   
    # Haze
    # -158,-159,-160,-161,-162,-163,-164,-165,-166,-167,-168,-169,-170,
    -158,-170,
   
    # Blowing snow
    # -171,-172,-173,-174,-175,-176,-177,-178,-179,-180,-181,-182,-183,
    -171,-172,-173,-174,-175,-176,-182,-183,
   
    # Funnel cloud; nearly all zero
     -184,-185,-186,-187,-188,-189,-190,-191,-192,-193,-194,-195,-196,
   
    # Damaging wind
    -197,-198,-199,-200,-201,-202,-203,-204,-205,-206,-207,-208,-209,
    
    # Rain (Discrete Data: Rain or no Rain)
      # -210,-211,-212,-213,-214,-215,-216,-217,-218,-219,-220,-221,-222,
    -210,-222,
   
    # Freezing rain
      -223,-224,-225,-226,-227,-228,-229,-230,-231,-232,-233,-234,-235,
    #-223,-224,-225,-226,-227,-234,-235,
   
    # Snow_wt18 (Discrete Data: Snow or no snow)
      # -236,-237,-238,-239,-240,-241,-242,-243,-244,-245,-246,-247,-248,
    -236,-237,-238,-239,-240,-247,-248,
   
    # Low Lake Michigan water level
    -249,-250,-251,-252,-253,-254,-255,-256,-257,-258,-259,-260, -261,
   
    # Mean Lake Michigan water level
      # -262,-263,-264,-265,-266,-267,-268,-269,-270,-271,-272,-273,-274,
    -262,-274,
   
    # High Lake Michigan water level
       #-275,-276,-277,-278,-279,-280,-281,-282,-283,-284,-285,-286,-287
    -275,-287,
   
     # Solar irradiation; Also eliminate July-Nov since these months
     # mirror solar radiation six months later, retain the year total,
     # and January-June.
       #-288,-289,-290,-291,-292,-293,-294,-295,-296,-297,298,-299,300
     -288,-289,-290,-291,-292,-293,-300
)]
## remove irradiation Jun-Nov
smaller[,"JanAverageIrrad"] <- get_zero_mean_vector(smaller[,"JanAverageIrrad"])
smaller[,"FebAverageIrrad"] <- get_zero_mean_vector(smaller[,"FebAverageIrrad"])
smaller[,"MarAverageIrrad"] <- get_zero_mean_vector(smaller[,"MarAverageIrrad"])
smaller[,"AprAverageIrrad"] <- get_zero_mean_vector(smaller[,"AprAverageIrrad"])
smaller[,"MayAverageIrrad"] <- get_zero_mean_vector(smaller[,"MayAverageIrrad"])
#smaller[,"JunAverageIrrad"] <- get_zero_mean_vector(smaller[,"JunAverageIrrad"])
#smaller[,"JulAverageIrrad"] <- get_zero_mean_vector(smaller[,"JulAverageIrrad"])
#smaller[,"AugAverageIrrad"] <- get_zero_mean_vector(smaller[,"AugAverageIrrad"])
#smaller[,"SepAverageIrrad"] <- get_zero_mean_vector(smaller[,"SepAverageIrrad"])
#smaller[,"OctAverageIrrad"] <- get_zero_mean_vector(smaller[,"OctAverageIrrad"])
#smaller[,"NovAverageIrrad"] <- get_zero_mean_vector(smaller[,"NovAverageIrrad"])
smaller[,"DecAverageIrrad"] <- get_zero_mean_vector(smaller[,"DecAverageIrrad"])

smaller <- subset(smaller, select = c(-JulAverageMinTemp,-MarAverageMinTemp,
                                      -AprAverageMinTemp,
                                      -DecAverageIrrad, -FebAverageIrrad,
                                      -AprAverageIrrad
                                      ))
smaller <- na.omit(smaller, fill=0)

yr <- smaller[,"Year"]
june_rain <- as.numeric(as.character(all_june_rain))
smaller[,"JunRain"] <- june_rain
# JunRain is column 185

```

# Training and Final Test Sets

<p>The data science exploration and machine-learning begins by importing the data from the final merged file named <i>yearlyChicagoMeteorologicalWeather.csv.</i> The data was divided into training and final test sets. Approximately 70% of the year records, forty-four records, were randomly selected for training models and the remaining sixteen records were reserved for a formal final test. A cross-validation comparison of different models was performed using only the training set as this is intended as a scientific study. </p>

```{r trainingsets, echo=FALSE}

set.seed(3433)
#data.frame(smaller[,"Year"],smaller$JunRain)
inBuild <- createDataPartition(y=smaller$JunRain,p=0.7,list=FALSE)
build <- smaller[inBuild,]
validation0 <- smaller[-inBuild,]

#folds <- createFolds(y=build$JunRain,k=3,list=TRUE,returnTrain=FALSE)
training  <- build
year_train <- training[,"Year"]
train0 <- subset(training, select= -c(Year))
train0_datax <- subset(training, select= -c(Year,JunRain))

```


```{r reduced_dataset, echo=FALSE}

y_model <- train0[,"JunRain"]
x1 <- (train0[,"FebSnowDepth"])
x2 <- train0[,"FebBlowingSnow"]
x3 <- train0[,"MarThunder"]
x4 <- train0[,"MarAverageMaxTemp"]
x5 <- train0[,"NovAverageMinTemp"]
x6 <- train0[,"MarRain_wt16"]

test_x1 <- (validation0[,"FebSnowDepth"])
test_x2 <- validation0[,"FebBlowingSnow"]
test_x3 <- validation0[,"MarThunder"]
test_x4 <- validation0[,"MarAverageMaxTemp"]
test_x5 <- validation0[,"NovAverageMinTemp"]
test_x6 <- validation0[,"MarRain_wt16"]

y_validation <- validation0[,"JunRain"]

anova_analys_data <- data.frame(x1,x2,x3,x4,x5,x6,y_model)
anova_datax <- subset(anova_analys_data, select=-c(y_model)) 
reduced_data <- anova_analys_data
reduced_datax <- anova_datax

#dfdata <- anova_analys_data
selected <- data.frame(x1,x3,x6,y_model)

cluster_data <- subset(anova_analys_data)
selected_datax <- subset(selected, select=-c(y_model))

rounded3 <- lapply(c(1:6), function(u)
 {as.numeric(formatC(reduced_data[,u], format = "f", digits = 3))}
)

reduced_data[,1] <- rounded3[[1]]
reduced_data[,2] <- rounded3[[2]]
reduced_data[,3] <- rounded3[[3]]
reduced_data[,4] <- rounded3[[4]]
reduced_data[,5] <- rounded3[[5]]
reduced_data[,6] <- rounded3[[6]]

tbl_df <- reduced_data
tbl_df[,"Year"] <- year_train
#tbl_df[,"Prediction"] <- as.numeric(formatC(ox, format = "f", digits = 1))
colnames(tbl_df) <- c("FebSnowDepth","FebBlowingSnow","MarThunder",
                      "MarAvgMaxTemp","NovAvgMinTemp","MarRain_wt16","JunRain","Year")

```

```{r process_reduced_dataset, echo=FALSE}

train0_y <- y_model
model_size <- dim(anova_datax)[2]
model_build_size <- model_size + 1
selected_model_size <- dim(selected_datax)[2]
selected_build_size <- selected_model_size + 1
outcome_col <- model_build_size
datarows <- length(y_model)
testrows <- datarows-1
model_vars <- colnames(anova_datax)

x1_mu <- mean(x1, na.rm = TRUE)
x2_mu <- mean(x2, na.rm = TRUE)
x3_mu <- mean(x3, na.rm = TRUE)
x4_mu <- mean(x4, na.rm = TRUE)
x5_mu <- mean(x5, na.rm = TRUE)
x6_mu <- mean(x6, na.rm = TRUE)

#x00 <- c(x1_mu, x2_mu, x3_mu, x4_mu, x5_mu, x6_mu, x7_mu)
x00 <- c(x1_mu, x3_mu,x6_mu)

```


```{r validationdata, echo=FALSE}

xadder1 <- rnorm(datarows)/6
xadder2 <- rnorm(datarows)/3
xadder3 <- rnorm(datarows)/2
xadder4 <- rnorm(datarows)/2
xadder5 <- rnorm(datarows)/2
xx1 <- (x1 + xadder1)[1:testrows]
xx2 <- (x2 + xadder2)[1:testrows]
xx3 <- (x3 + xadder3)[1:testrows]
xx4 <- (x4 + xadder3)[1:testrows]
xx5 <- (x5 + xadder3)[1:testrows]
yadder <- rnorm(datarows)/2

validationreduced <- data.frame(test_x1,test_x2,test_x3,test_x4,test_x5,test_x6,y_validation)
col_names <- c("x1","x2","x3","x4","x5","x6","y")

validationselect <- data.frame(test_x1,test_x3,test_x6,y_validation)
col_names_ls <- c("x1","x3","x6","y")

colnames(validationreduced)  <- col_names
colnames(validationselect)  <- col_names_ls

outcome_ls_col <- dim(validationselect)[2]
## validation0 includes entire dataset.
## validation0_datax <- subset(validation0, select = -c(Year,JunRain))
validation_datax <- validationreduced[,-outcome_col]
validationselect_datax <- validationselect[,-outcome_ls_col]
validation_year <- validation0[,"Year"]

```


```{r heatmapcalc1, echo=FALSE, message=F}

one_var <- FALSE
if(model_size > 1 && !one_var) {
   column_names <- names(train0)
   cols <- dim(train0)[2]
   colvec <- c(1:cols)
   June_Rain <- train0$JunRain
   junecorrs2 <- unlist(lapply(colvec, function(x){cor(June_Rain, train0[, x])[[1]][1]}))

   threshold <- 0.20
## helpfuldata2 is a true-false vector of length equal to the number of columns
   helpfuldata2 <- unlist(lapply(junecorrs2, function(x){!is.na(x) &
     (x < -threshold  | x > threshold )[[1]][1]}))
   helpfulcols2 <- column_names[helpfuldata2==TRUE]

   if(sum(helpfuldata2==TRUE)>1){
      helpful2 <- train0[,helpfulcols2]
   } else {
      #helpful2 <- train0
   }

# Eliminate noisy redundant correlated columns
   corrmatrixjuneB <- cor(helpful2)
   redundant <- findCorrelation(corrmatrixjuneB, cutoff = 0.8)
   num_remove <- length(redundant)
   if(num_remove > 0){
     cols <- dim(helpful2)[2]
     colvec <- c(1:cols)
     helpful3 <- helpful2[-redundant]
   } else {
     helpful3 <- helpful2
   }
   corrmatrix3 <- cor(helpful3)
} else {
  corrmatrix3 <- cor(anova_analys_data)
}
var_names <- dimnames(corrmatrix3)[[1]]
matrix_size <- dim(corrmatrix3)[2]
corr_june <- corrmatrix3[,matrix_size]
corrdf <- data.frame(var_names,corr_june)
corrdf <- corrdf[order(abs(as.numeric(as.character(corrdf$corr_june))),
                       decreasing=T),]
colnames(corrdf) <- c("Variable","Correlation")

```

<script>
sectionnum = sectionnum + 1;
</script>

# Preliminary Selection of Variables

<br>
<p class="sub-heading">Variables with Correlation of 0.2 or Greater with June Rain</p>
<p>we begin by selecting variables with at some significant non-zero correlation with June rain. Since there are nearly three hundred variables in the year records file described by paragraph 3.2, it is desirable to eliminate some of these even to begin exploration. Therefore, we select approximately twenty variables or columns of data with absolute correlation of at least 0.2 with June rain.</p>

<div class="inline-block align-top">
 <div class="inline-fortypercent more-right-margin">
   <br><br><br>
   <p> As negatively correlated variables are just as predictive as positively-correlated ones, the variables are listed by the order of the magnitude of the absolute value of the correlation. Many of these variables are included in the heat map below in Figure-1.</p>
   <p>Certain unlikely events such as funnel clouds and damaging winds were removed because there are not enough  incidents to include them in a statistical model. Freezing rain is removed because it depends on a particular unlikely temperature to occur. To make the report easier to follow, minimum high temperatures are removed where maximum high temperature can replace it as these are mutually correlated variables. </p>
 </div><div class="inline-half inline-sixtypercent align-top">

```{r heattable, echo=FALSE, results='asis', message=F}

if(model_size > 1 && !one_var) {  
    tablenum <- tablenum + 1
    fig <- paste0("Figure-",figurenum)
    colcount1 <- dim(corrdf)[2] + 1
    alignment <- rep("ccc", colcount1)
    tablecaption <- paste0('<p class = "center bluegreen">Absolute June Correlation</p><p class = "center bluegreen fig-title">Table-',tablenum,'</p>') 
    print(xtable(corrdf, caption = tablecaption, align = alignment, size="8pt"),
    caption.placement ='top', include.rownames=FALSE, type = "html")
}

figurenum <- figurenum + 1
fig <- paste0("Figure-",figurenum)

```

</div></div>
<br>

## Heat Map

<p class="p1">In the graph below, light-brown and red  hues indicates positive correlation and green indicates negative correlation. The squares across the top row indicate the correlation of the variables with June rain. As the map is symmetrical about the red diagonal line, the squares along the right edge reflect the same data. Since the map is symmetrical about the diagonal line of red squares, we only need half of the map and could erase the other part. The red diagonal line of squares from the lower left to the upper right of the map contains zero information as it indicates the trivial self-correlation of the variables with themselves for which r = 1. If we remove the top row and the right-most column, the remainder is a heat map of the self-correlation of only the predictive variables. </p>

```{r heatmap2, echo=FALSE, message=F, fig.cap = "", fig.width=8, fig.height=6}

if(model_size > 1) { 
   get_heat_map(corrmatrix3)
}

```

<br>
<p class="p1">Positive correlation with June rain is indicated by the light-brown in the right-most column. These correspond to weather in February and March including the amount of snow, average snow depth, blowing snow, rain. April rain and snow is also positively correlated. Negative-correlation in green corresponds to solar radiation from January to March, rain in August, November, and March; thunder in August, March, and May; above average temperatures in late fall in November and late winter and early spring from February to April. </p>

<p class="p1">Negatively correlated variables have the same predictive power as positively correlated ones. Later, we will use principal components analysis to generate an equivalent set of independent variables for which the mutual correlation is zero.</p>

<p class="p1">The correlation here is the Pearson Correlation Coefficient.</p>

```{r glmdata1b, echo=FALSE}

xglm <- train0_datax
yglm <- y_model
testyglm <- y_validation

xnorm <- as.matrix(xglm)
y_model_matrix <- as.matrix(yglm)

## all columns
## testxnorm <- as.matrix(validation0_datax)
x_helpful <- subset(helpful3, select= -c(JunRain))
fullglmnames <- colnames(helpful3)
cv_names <- colnames(x_helpful)
validation0_helpful <- subset(validation0, select=fullglmnames)
validation0_helpful_datax <- subset(validation0_helpful, select = -c(JunRain))

x_helpful_matrix <- as.matrix(x_helpful)
testglm <- as.matrix(validation0_helpful_datax)

```

## Reduced Variable Set for Model Building

<p class="p1">A reduced set of six variables was chosen for an ANOVA comparison of models having all possible variable combinations as well as principal components analysis. The six selected variables are listed in Table-2 below. The selection is based on the highest correlation with June rain in the range from 0.3 to 0.5. Some variables that were correlated with these were eliminated. Simply to eliminate confusion where warm weather is a factor, average daily maximum temperature for a given month is selected even if the average minimum temperature has slightly better correlation as daily high and low temperatures are well-correlated.</p>

<div class="p1">

```{r selectedvars, echo = F, results='asis'}

## Variable definition table

Variable <- c("x1","x2","x3","x4","x5","x6") 
Name <- c("FebSnowDepth","FebBlowingSnow","MarchThunder",
          "MarchAvgMaxTemp","NovAvgLowTemp","MarchRain")
Units <- c("mm","","","°C","°C","")
Description <- c("Average snow depth for February","February blowing snow days",
                 "March thunderstorm days","Average daily high temperature for March",
                 "Average daily low temperature for November","March Rain")
Field <- c("SNWD","WT09","WT03","TMAX","TMIN","WT16")
Alias <- c("February snow depth","February blowing snow","March thunder",
           "March temperature","November low temperature","March rain")
variable_df <- data.frame(Variable,Description,Alias,Name, Units, Field)

dfhypoth <- data
if(model_size > 1 ) {  
  tablenum <- tablenum + 1
  tablecaption <- paste0('<p class = "center bluegreen fig-title">Variable Names and Definitions</p><p class = "center bluegreen">Table-',tablenum,'</p>')  
  colcount1 <- dim(variable_df)[2] + 1
  alignment <- rep("c", colcount1)
  print(xtable(variable_df, caption = tablecaption, align = alignment, size="8pt"),
        caption.placement ='top', include.rownames=FALSE, type = "html")
}

```

</div>
<br>

<p class="p1">The field names in the right-most column are defined by the GHCND documentation for Station USW00094846. The fields WT03, WT09, and WT16 were processed to become ratios without units as x2, x3, and x6. In the raw data, these names indicated whether not there was thunder, blowing snow, or rain on a particular day, which was processed to compute a fraction of month days. The same field name may appear more than once as the raw data was processed to compute the average of each month. </p>

<br>
<div class="p1">

```{r selectedrawdata1, echo=FALSE, results='asis'}
rawtable <- tbl_df[,c(8,7,1:6)]
colnames(rawtable) <- c("Year", "June Rain","x1 (mm)","x2","x3","x4 (°C)","x5 (°C)","x6")
tablenum <- tablenum + 1
tablecaption <- paste0('<p class = "center bluegreen">Selected Data </p><p class = "center bluegreen">Table-',tablenum,'</p>') 
cat(tablecaption)

```

```{r selectedrawdata, echo=FALSE}
datatable(rawtable, rownames=F,
          options = list(
            columnDefs = list(list(className = 'dt-center', targets = 0:7))
            ))

```

</div>


```{r selectedvars2, echo = F, results='asis'}
anovanum <- tablenum + 3
textvars <- paste0('<p>The reduced set of six variables was subjected to ANOVA analysis based on an ordinary-least-squares model described in Section-7. Based on the ANOVA model comparison in Table-',anovanum,' of paragraph 7.2, we will select one of the promising combination for the OLS and PLS models.</p>')

cat(textvars)

```

<div class="p1">

```{r trainingolspred, echo = FALSE, results='asis'}

colnames(tbl_df) <- c("FebSnowDepth","FebBlowingSnow","MarThunder",
                      "MarAvgMaxTemp","NovAvgMinTemp","MarRain_wt16","JunRain","Year")
```

</div>

<script>
sectionnum = sectionnum + 1;
</script>

# Unsupervised Learning and Exploration

## Cluster Analysis

<p class="p1">Searching for clusters is a form of unsupervised learning that can help discover patterns and relationships among variables. If the data were dot patterns, we might use K-means cluster analysis to determine if we can separate the dot patterns into clusters. As an example, assume we discover ten clusters of dots. We have not assigned names or labeled the patterns with respect to anything outside of the data that requires prior knowledge or assumptions yet. Any such labeling would be beyond cluster analysis and unsupervised learning. We cannot build a predictive model from only cluster analysis.</p>

```{r clustercalcs, echo=FALSE}

row.names(cluster_data) <- year_train

if(model_size > 1) { 

# convert to normal variable distribution
  df <- scale(cluster_data)
  #validation_scaled <- scale(validationreduced)

  k2 <- kmeans(df, centers = 2, nstart = 25)
# cluster column generated by k-means function 
  k2cluster <- k2$cluster
  k_2 <- unname(k2cluster)
  k_2c <- as.character(k_2)
  k_2c[k_2c=="2"] <- "wet"
  k_2c[k_2c=="1"] <- "dry"

  cluster_data[,"cluster_2_level"] <- k_2c

  num_cols <- dim(cluster_data)
  msp1 <- model_size + 1
  msp2 <- model_size + 2
  #cluster_data <- cluster_data[,c(msp2,1:msp1)]
  cluster_data <- cluster_data[,c(msp2,msp1,1:model_size)]
}

```

```{r aggregateclusterdata, echo = FALSE}

aggkmeans2 <- aggregate(. ~ cluster_2_level, data = cluster_data, mean)

## Snow
fswet90 <- cluster_data[cluster_data$x1>90,"x1"]
fswet90txt <- formatC(mean(fswet90), format = "f", digits = 1)

rainmorethan90 <- cluster_data[cluster_data$x1>90,"y_model"]
rainmorethan90txt <- formatC(mean(rainmorethan90), format = "f", digits = 1)
fslessthan90 <- cluster_data[cluster_data$x1<90,"y_model"]
fslessthan90txt <- formatC(mean(fslessthan90), format = "f", digits = 1)

aggclustermean2 <- aggkmeans2
aggclusterenglish2 <- aggclustermean2
aggclustermetric2 <- aggclustermean2
 
aggclustermetric2[,2] <- rounded(aggclustermean2[,2],1)
aggclustermetric2[,3] <- rounded(aggclustermean2[,3],1)
aggclustermetric2[,4] <- rounded(aggclustermean2[,4],3)
aggclustermetric2[,5] <- rounded(aggclustermean2[,5],3)
aggclustermetric2[,6] <- rounded(aggclustermean2[,6],2)
aggclustermetric2[,7] <- rounded(aggclustermean2[,7],2)
aggclustermetric2[,8] <- rounded(aggclustermean2[,8],3)
 
aggclusterenglish2[,2] <- convert_degrees_mm_to_inch(aggclustermean2[,2])
aggclusterenglish2[,3] <- convert_degrees_mm_to_inch(aggclustermean2[,3])
aggclusterenglish2[,6] <- convert_degrees_c_to_f(aggclustermean2[,6])
aggclusterenglish2[,7] <- convert_degrees_c_to_f(aggclustermean2[,7])
 
aggclusterenglish2[,2] <- rounded(aggclusterenglish2[,2],2)
aggclusterenglish2[,3] <- rounded(aggclusterenglish2[,3],2)
aggclusterenglish2[,4] <- rounded(aggclusterenglish2[,4],3)
aggclusterenglish2[,5] <- rounded(aggclusterenglish2[,5],3)
aggclusterenglish2[,6] <- rounded(aggclusterenglish2[,6],2)
aggclusterenglish2[,7] <- rounded(aggclusterenglish2[,7],2)
aggclusterenglish2[,8] <- rounded(aggclusterenglish2[,8],3)

```

<div class="p1">

```{r clustertablemetric, echo=FALSE, results='asis', fig.width=4, fig.height=2}

tabledatametric2  <- aggclustermetric2
cols <- c("Cluster","JunRain", "FebSnowDepth","FebBlowingSnow","MarThunder",
                        "MarAverageMaxTemp","NovAverageMinTemp","MarRain_WT16")
colcount <- dim(tabledatametric2)[2] + 1
alignment <- rep("c", colcount)

tablecols_c <- c("Cluster","June Rain (mm)",
                 "February Snow Depth \u00A0 (mm)","February Blowing Snow",
                 "March Thunder",  "March High (\u00B0C)",
                 "November Low (\u00B0C)","March Rain WT16")

colnames(tabledatametric2) <- tablecols_c
tablenum <- tablenum + 1
metrictablenum <- tablenum

dryjunrain <- tabledatametric2[1,2]
wetjunrain <- tabledatametric2[2,2]
dryfebdepth <- tabledatametric2[1,3]
wetfebdepth <- tabledatametric2[2,3]


tablecaption <- paste0('<p class = "center bluegreen image">Metric Cluster Data</p><p class="center bluegreen">Table-',tablenum,'</p>')
print(xtable(tabledatametric2, caption = tablecaption, 
        align = alignment, digits=c(2,0,1,1,3,3,1,1,3)),
     caption.placement ='top', include.rownames=FALSE, type = "html")

```

<br>

```{r clustertableenglish, echo=FALSE, results='asis', fig.width=4, fig.height=2}
## English Units: Precipitation in inches, temperature Degrees Fahrenheit (&deg;F)

tabledataenglish2 <- aggclusterenglish2
colcount <- dim(tabledataenglish2)[2] + 1
alignment <- rep("c", colcount)
tablecols_f <- c("Cluster","June Rain (inches)",
                 "February Snow Depth \u00A0 (inches)","February Blowing Snow",
                 "March Thunder",  "March High (\u00B0F)",
                 "November Low (\u00B0F)","March Rain WT16")
colnames(tabledataenglish2) <- tablecols_f
tablenum <- tablenum + 1
tablecaption <- paste0('<p class = "center bluegreen image">English Units</p><p class="center bluegreen">Table-',tablenum,'</p>')
 print(xtable(tabledataenglish2, caption = tablecaption, 
         align = alignment, digits=c(2,0,1,1,3,3,1,1,3)),
     caption.placement ='top', include.rownames=FALSE, type = "html")

```

</div>

```{r clusterstatement, echo = FALSE, results='asis'}

clusterstatemnt1 <- paste('<br><p class="p1">A two-cluster analysis of the reduced dataset separates the data into wet and dry years. As shown in Table-',metrictablenum,' above, the dry cluster has average June rain of ',dryjunrain,'-mm and the wet year cluster has average June rain of ', wetjunrain,'-mm. The wet years have average February snow depth of ',wetfebdepth,'-mm and dry years have average February snow depth of ',dryfebdepth,'-mm.</p>',sep='')

cat('<p class="p1" style=color:#002200;font-size:20px>',clusterstatemnt1, '</p>')

```

## February Snow Depth

<p class="p1"> The plot below shows dry and wet year clusters on a plot of June rain vs. February snow depth. As shown earlier in Table-1, the correlation between average February snow depth (FebSnowDepth) and June rain is 0.36. Yellow-brown text color indicates dry cluster years in the plot below; blue-green corresponds to wet years.</p>

```{r kmeansfebsnow, echo = FALSE}
  
colors2r <- c("#BB9900","#22AAAA")
colors2 <- c("#22BBBB", "#CCAA00")
colors3 <- c("#CCAA00","#66AAAA","#888888")

figurenum <- figurenum + 1
fig <- paste0("Figure-",figurenum)
heading <- paste('June Rain vs. Average February Snow Depth\n',fig)

dtib <- as_tibble(cluster_data)
mutated <- mutate(dtib,  cluster = cluster_data$cluster_2_level,
         year = row.names(cluster_data))

ggplot(mutated,aes(x1,y_model)) +
    coord_cartesian(xlim = c(0, 450), ylim = c(0, 300)) +
  geom_text(data=mutated, aes(x1,y_model,label = year, color = factor(cluster)), size=3.5)  +
  scale_color_manual(values=colors2r) +
  xlab("Average February Snow Depth (mm)") + ylab("June Rain (mm)") +
  labs(color = "Cluster") +
  scale_x_continuous(minor_breaks = seq(50 , 450, 100), breaks = seq(0, 500, 100)) +
  scale_y_continuous(minor_breaks = seq(50 , 450, 100), breaks = seq(0, 500, 100)) +
  ggtitle(heading) +
  guides(shape = guide_legend(override.aes = list(size = 14),alpha = FALSE)) +
  theme(plot.title = element_text(size=14,hjust = 0.5, color="#008800"),
     legend.title = element_text(size=16),
     legend.text = element_text(size=14),
     legend.background = element_rect(fill = "#DDDDEE"),
     legend.key = element_rect(colour = "transparent", fill = "#DDDDEE"),
     text = element_text(size=12),
     axis.text = element_text(size=14,angle=0, hjust=0.5),
     axis.title = element_text(size = 16),
     axis.title.x = element_text(margin = margin(t = 7)),
     axis.title.y = element_text(margin = margin(l = 13)),
     panel.grid.minor =   element_line(color = "white",size=0.6),
     panel.grid.major =   element_line(color = "light blue",size=0.8),
     panel.background = element_rect(fill = "#EFE5E5", color = "#008800",
            size = 0.5, linetype = "solid"),
     plot.background = element_rect(fill = "#DDDDEE",color = "#DDDDEE"))

```

```{r kmeansfebsnowtext, echo = FALSE, results ='asis'}

pfebsnow1 <- paste('<br><p class="p1">Cluster analysis divided the years into two groups where the wet custer includes the eight cases of June rain over 150-mm such as 1992, 2017, and 1966. The dry year cluster of less than 50-mm of June rain in the lower left corner of the plot consists of the five years 1981, 1987, 1990, 1994, and 2004. Higher June rain levels appear to be related to snow depth in February in these cases. </p>  ')

cat(pfebsnow1)

listfebsnow <- paste('<div class="p1div"><ul class="square-bullet more-left-margin">
   <li>February snow depth over 90-mm coincides with wet cluster years.</li>
   <li>For dry cluster years, February snow depth is less than 90-mm.</li>
   <li>For February snow depth exceeding 90-mm, average June rain is ',rainmorethan90txt,'-mm.</li> 
   <li>For February snow depth less than 90-mm, average June rain is ',fslessthan90txt,'-mm.</li>
   <li>There is some overlap of the wet and dry clusters such as years 1979, 2000 and 2018 appearing in the boundary area between the two clusters.</li>
   <li>Wet Junes occurred without any February snow in five years including 1969, 1970, 1995, 1998, and 2000. The grouping of these years in two bunches around 1970 and 1995-2000 might be investigated further.</li>
</ul></div>',sep="")

cat(listfebsnow)
  
```

## March Thunderstorms

<p class="p1">As shown earlier in Table-1, the correlation between March thunderstorms (MarThunder) and June rain is negative with a value of -0.39.</p>

```{r kmeansmarchthunder, echo = FALSE}
  figurenum <- figurenum + 1
  fig <- paste0("Figure-",figurenum)
  heading <- paste("June Rain vs. March Thunderstorm Days\n",fig)

  dtib <- as_tibble(cluster_data)
  mutated <- mutate(dtib,  cluster = cluster_data$cluster_2_level,
         year = row.names(cluster_data))

  ggplot(mutated,aes(31*x3,y_model)) +
    coord_cartesian(xlim = c(0, 10), ylim = c(0, 300)) +
  geom_text(data=mutated, aes(31*x3,y_model,label = year, color = factor(cluster)), size=3.5)  +
  scale_color_manual(values=colors2r) +
  xlab("Days with March Thunder") + ylab("June Rain (mm)") +
  labs(color = "Cluster") +
  scale_x_continuous(breaks = seq(0 , 40, 2)) +
  scale_y_continuous(minor_breaks = seq(50 , 450, 100), breaks = seq(0, 500, 100)) +
  ggtitle(heading) +
  guides(shape = guide_legend(override.aes = list(size = 15),alpha = FALSE)) + 
  theme(plot.title = element_text(size=14,hjust = 0.5, color="#008800"),
     legend.title = element_text(size=15),
     legend.text = element_text(size=15),
     legend.background = element_rect(fill = "#DDDDEE"),
     legend.key = element_rect(colour = "transparent", fill = "#DDDDEE"),
     text = element_text(size=12),
     axis.text = element_text(size=14,angle=0, hjust=0.5),
     axis.title = element_text(size = 16),
     axis.title.x = element_text(margin = margin(t = 7)),
     axis.title.y = element_text(margin = margin(l = 13)),
     panel.grid.minor =   element_line(color = "white",size=0.6),
     panel.grid.major =   element_line(color = "light blue",size=0.8),
     panel.background = element_rect(fill = "#EFE5E5", colour = "#008800",
        size = 0.5, linetype = "solid"),
     plot.background = element_rect(fill = "#DDDDEE",color = "#DDDDEE"))

thunder1 <- cluster_data[31*cluster_data$x3 < 1.5,"y_model"]
#print("Avg rain for 0 or 1 thunder days.")
thunder1txt  <- formatC(mean(thunder1), format = "f", digits = 1)
thunder2 <- cluster_data[31*cluster_data$x3 > 1.5,"y_model"]
#print("Avg rain for two or more thunder days.")
thunder2txt  <- formatC(mean(thunder2), format = "f", digits = 1)
thunder4 <- cluster_data[31*cluster_data$x3 > 3.5,"y_model"]
#print("Avg rain for four or more thunder days.")
thunder4txt  <- formatC(mean(thunder4), format = "f", digits = 1)

```

```{r kmeansthundertext, echo = FALSE, results ='asis'}

listthunder <- paste('<div class="p1div more-left-margin"><ul class="square-bullet">
   <li>For one or less thunderstorm days, average June rain is ',thunder1txt,'-mm.</li> 
   <li>For two or more thunderstorm days, average June rain is ',thunder2txt,'-mm.</li> 
   <li>For four or more thunderstorm days, average June rain is ',thunder4txt,'-mm.</li> 
   <li>The eight years with over 150-mm of June rain correspond to years with no more than two thunderstorm days. </li>
   <li>That thunderstorms in March have a relationship with dry weather in June is likely a result of the association of thunderstorms with warm weather in March as is explored in the next plot. Thunderstorms are obviously more unlikely in cold winter weather in early March.</li>
   
</ul></div>',sep="")

cat(listthunder)
```

## March Daily High Temperature

<p class="p1">As shown earlier in Table-1, there is negative correlation between March daily high temperature (MarAverageMaxTemp) and June rain of -0.33.</p>

```{r kmeansmarchtemp, echo = FALSE}
  figurenum <- figurenum + 1
  fig <- paste0("Figure-",figurenum)
  heading <- paste("June Rain vs. Average March Daily High Temperature\n",fig)

  dtib <- as_tibble(cluster_data)
  mutated <- mutate(dtib,  cluster = cluster_data$cluster_2_level,
         year = row.names(cluster_data))

  ggplot(mutated,aes(x4,y_model)) +
    coord_cartesian(xlim = c(0, 20), ylim = c(0, 300)) +
  geom_text(data=mutated, aes(x4,y_model,label = year, color = factor(cluster)), size=3.5)  +
  scale_color_manual(values=colors2r) +
  xlab("Temperature (°C)") + ylab("June Rain (mm)") +
  labs(color = "Cluster") +
  scale_x_continuous(minor_breaks = seq(1 , 29, 1), breaks = seq(0, 40, 5)) +
  scale_y_continuous(minor_breaks = seq(50 , 450, 100), breaks = seq(0, 500, 100)) +
  ggtitle(heading) +
  guides(shape = guide_legend(override.aes = list(size = 15),alpha = FALSE)) +
  theme(plot.title = element_text(size=14,hjust = 0.5, color="#008800"),
     legend.title = element_text(size=15),
     legend.text = element_text(size=15),
     legend.background = element_rect(fill = "#DDDDEE"),
     legend.key = element_rect(colour = "transparent", fill = "#DDDDEE"),
     text = element_text(size=12),
     axis.text = element_text(size=14,angle=0, hjust=0.5),
     axis.title = element_text(size = 16),
     axis.title.x = element_text(margin = margin(t = 7)),
     axis.title.y = element_text(margin = margin(l = 13)),
     panel.grid.minor =   element_line(colour = "white",size=0.6),
     panel.grid.major =   element_line(colour = "light blue",size=0.8),
     panel.background = element_rect(fill = "#EFE5E5", colour = "#008800",
        size = 0.5, linetype = "solid"),
     plot.background = element_rect(fill = "#DDDDEE",color = "#DDDDEE"))
  
martempdry <- cluster_data[cluster_data$cluster_2_level=="dry","x4"]
#print("Dry Mar Temp")
martempdrytxt <- formatC(mean(martempdry), format = "f", digits = 1)
martempdryf <- convert_degrees_c_to_f(martempdry)
martempdryftxt <- formatC(mean(martempdryf), format = "f", digits = 1)

martempwet <- cluster_data[cluster_data$cluster_2_level=="wet","x4"]
martempwettxt <- formatC(mean(martempwet), format = "f", digits = 1)
#print("Wet Mar Temp")

martempwetf <- convert_degrees_c_to_f(martempwet)
martempwetftxt <- formatC(mean(martempwetf), format = "f", digits = 1)

mtover10 <- cluster_data[cluster_data$x4>10,"y_model"]
#print("Avg March temperature over 10C")
mtover10txt <- formatC(mean(mtover10), format = "f", digits = 1)

mtover5 <- cluster_data[cluster_data$x4>5,"y_model"]
#print("Avg March temperature over 5C")
mtover5txt <- formatC(mean(mtover5), format = "f", digits = 1)

mtbelow5 <- cluster_data[cluster_data$x4<5,"y_model"]
mtbelow5txt <- formatC(mean(mtbelow5), format = "f", digits = 1)
```

```{r kmeansmarchtemperaturetext, echo = FALSE, results ='asis'}
pmartemp1 <- paste('<br><p class="p1" style="color:#002200;font-size:20px">The seven dry years with June rain below 50-mm are easily spotted at the bottom of the above plot.</p>  ')

cat(pmartemp1)

listmartemp <- paste('<div class="p1div"><ul class="square-bullet more-left-margin">
   <li class="square-bullet">Wet cluster years have average March daily high temperature of ',martempwettxt,'&deg;C (',martempwetftxt,'&deg;F) compared to ',martempdrytxt,'&deg;C (',martempdryftxt,'°F) for dry cluster years. </li>
   <li>Years with average March temperature greater than 10&deg;C belong to the dry cluster and have average June rain of ',mtover10txt,'-mm.</li>
   <li>Years with average March temperature of less than 5&deg;C belong to the wet cluster and have average June rain of  ',mtbelow5txt,'-mm. For average temperature greater than 5°C, average June rain is ',mtover5txt,'-mm.</li>
   
</ul></div>',sep="")

cat(listmartemp)
  
```

## March Rain Days

<p class="p1">As shown earlier in Table-1, the correlation between March rain days (MarRain_wt16) and June rain is negative with a value of -0.28.</p>

```{r kmeansmarchrain, echo = FALSE}
  figurenum <- figurenum + 1
  fig <- paste0("Figure-",figurenum)
  heading <- paste("June Rain vs. March Rain Days\n",fig)

  dtib <- as_tibble(cluster_data)
  mutated <- mutate(dtib,  cluster = cluster_data$cluster_2_level,
         year = row.names(cluster_data))

  ggplot(mutated,aes(31*x6,y_model)) +
    coord_cartesian(xlim = c(0, 20), ylim = c(0, 300)) +
  geom_text(data=mutated, aes(31*x6,y_model,label = year, color = factor(cluster)), size=3.5)  +
  scale_color_manual(values=colors2r) +
  xlab("March Rain Days") + ylab("June Rain (mm)") +
  labs(color = "Cluster") +
  scale_x_continuous(minor_breaks = seq(5 , 35, 10), breaks = seq(0, 40, 10)) +
  scale_y_continuous(minor_breaks = seq(50 , 450, 100), breaks = seq(0, 500, 100)) +
  ggtitle(heading) +
  guides(shape = guide_legend(override.aes = list(size = 15),alpha = FALSE)) +
  theme(plot.title = element_text(size=14,hjust = 0.5, color="#008800"),
     legend.title = element_text(size=15),
     legend.text = element_text(size=15),
     legend.background = element_rect(fill = "#DDDDEE"),
     legend.key = element_rect(colour = "transparent", fill = "#DDDDEE"),
     text = element_text(size=12),
     axis.text = element_text(size=14,angle=0, hjust=0.5),
     axis.title = element_text(size = 16),
     axis.title.x = element_text(margin = margin(t = 7)),
     axis.title.y = element_text(margin = margin(l = 13)),
     panel.grid.minor =   element_line(colour = "white",size=0.6),
     panel.grid.major =   element_line(colour = "light blue",size=0.8),
     panel.background = element_rect(fill = "#EFE5E5", colour = "#008800",
        size = 0.5, linetype = "solid"),
     plot.background = element_rect(fill = "#DDDDEE",color = "#DDDDEE"))
  
rain9 <- cluster_data[31*cluster_data$x6 < 9.5,"y_model"]
rain9txt  <- formatC(mean(rain9), format = "f", digits = 1)

rain10 <- cluster_data[31*cluster_data$x6 > 9.5,"y_model"]
rain10txt  <- formatC(mean(rain10), format = "f", digits = 1)
  
```


```{r kmeansmarchraintext, echo = FALSE, results ='asis'}

pmarrain <- paste('<p class="p1" style="color:#002200;font-size:20px">The dry years at the bottom with nine or more days of March rain reports and June rain below 50-mm are easily spotted at the bottom of the above plot.</p>  ')

cat(pmarrain)

listmarrain <- paste(
'<div class="p1div"><ul class="square-bullet more-left-margin">
   <li>For ten or more March days with a report of rain, average June rain is ',rain10txt,'-mm.</li> 
   <li>For nine or less March days with a report of rain, average June rain is ',rain9txt,'-mm.</li>
   <li> The wet cluster has a uniform distribution over the full range of the x-axis. Eight days or less of March rain coincides with at least moderate June rain except in 2015. </li>
   <li> For the dry group, there are at least nine March days with rain reports except in 2015.</li>
</ul></div>',sep="")

cat(listmarrain)
  
```

<script>
sectionnum = sectionnum + 1;
</script>

<br>

# Preliminary Ordinary Least Squares Analysis

## Regression as Supervised Learning

<p class="p1">After concluding cluster analysis and unsupervised exploration of the records in the training dataset, we might have prior knowledge about relationships among the variables or find correlations between the columns of numbers that represent an instance of the variables in the dataset. If this is the case, we might proceed to build a regression model in an attempt to predict an unknown variable in a future data record. Building or training a regression model is an example of supervised learning that can improve our knowledge by providing a more accurate prediction. The prediction model goes beyond the data when it includes additional knowledge about the world to help construct an algorithm or choose and eliminate variables for a model.</p> 

## General Model Equation

<p class="p1">A prediction or estimate is calculated as a linear combination of the six variables selected for the Reduced Group described in paragraph 5.2 where some of the variables are often missing with zero coefficients. A standard statistics program easily finds the coefficients for each of the model components. To calculate the estimate, the contributions of component variables are summed over as follows:</p>

###### $$\hat {y} = c_0 \; +  \; c_1 x_1  \; + \; c_2 x_2 \; + \; c_3 x_3 \; +  \; c_4 x_4  \; + \; c_5 x_5 \; + \; c_6 x_6$$

<br>

<p class="p1"></p>

<p class="p1">In this Section, we will explore all possible ordinary-least-square models that are combinations of six selected variables of the Reduced Group. Later, we will employ machine-learning to find the best models in Sections 12-14.</p>

<br>

```{r press_stat, echo=F}
sigma_hat <- 26.2075
sigma_hat <- 0.0369

## The data frame anova_datax does not include the result or outcome column
compare <- get_model_comparison(anova_datax,y_model,model_vars,0.1)
compare[,"id"] <- seq_len(nrow(compare))
colnames(compare) <- c("ID","Model Numbers", "Model Components", "Model Size",
                        "MSE","Mean_Deleted_MSE",
                        "Delta","PRESS",
                        "R-Square", "R-Square-Predict","Adjust-R-Square",
                        "Cp") # "C~p~"

#df <- data.frame(A=c(1,2),B=c(4,2),C=c(3,4),D=c(8,7))
#require(knitr)
#knitr::kable(df, col.names = c("Space in name", "(Special Characters)","$\\delta{m}_1$","Space in name"))

rounded0 <- lapply(c(1,5,8),function(u) {
   formatC(compare[,u], format = "f", digits = 0)}
)
rounded2 <- lapply(c(7,9:12),function(u) {
  formatC(compare[,u], format = "f", digits = 2)}
)
rounded2 <- lapply(c(7,9:12),function(u) {
  as.numeric(formatC(compare[,u], format = "f", digits = 2))}
)
rounded3 <- lapply(c(6,8),function(u)
  formatC(compare[,u], format = "f", digits = 3)
)
compare[,1] <- rounded0[[1]] # id
#compare[,2] <- rounded2[[5]] # cp
compare[,5] <- rounded0[[2]] # MSE
#press_stats[,4] <- rounded0[[3]] # PRESS 8
compare[,9] <- rounded2[[2]] # R sqr
compare[,10] <- rounded2[[3]] # R sqr pred
compare[,11] <- rounded2[[4]] # adj

press_stats <- compare[,c(1,3,5,8:12)]
# c("ID", "Model Components", "MSE","PRESS",
#   "R-Squared", "$R^{2}$ Predict","Adjusted R-Squared",
#                        "C~p~")
ordered <- press_stats[order(press_stats[,"PRESS"],decreasing = F),]
ordered[,4] <- rounded0[[3]] # PRESS 8
ordered[,8] <- rounded2[[5]] # cp

```

## Model Comparisons

<p class="p1">Here are rankings of the sixty-three possible models for combinations of the six variables. For each possible combination, a Leave-One-Out-Cross-Validation comparison (LOOCV) was carried out which is reflected in the PRESS statistic in Column-5. </p> 

<div class="p1div"><ul class="square-bullet more-left-margin">

<li>The $C_p$ statistic in the fourth column is an underfitting and overfitting statistic that also penalizes the size or the number of variables included in the model. While it has a penalty term for each variable included in the model it also has a penalty for the estimated variance which is equal to the sample standard deviation. The $C_p$ statistic is considered reasonable if it is roughly equal to the number of variables in the model.</li>

<li>The PRESS statistic in the Column-5 is an acronym meaning <strong><i>Prediction Sum-Of-Squares</i></strong>. The PRESS statistic is a corrected error sum of squares (SSE) called a ‘deleted’ <i>delta</i> figure that is calculated on a Leave-One-Out-Cross-Validation basis where the figure reflects the error with the records deleted one at a time. The <i>delta</i> figure also includes a term that reflects the statistical confidence level. There is more information about the PRESS statistic in the Appendix at paragraph 15.4.</li> </ul></div>

```{r cp_statistic, echo=F}

cp_stats <- compare[,c(1,12,8,5,9,11,3)]

cp_ordered <- cp_stats[order(cp_stats[,"Cp"],decreasing = F),]
cp_ordered[,2] <- rounded2[[5]] # cp
cp_ordered[,4] <- rounded0[[3]] # PRESS 8

```

<br>
<div class="p1">

```{r cp_comparison, echo = FALSE,  results='asis', fig.width=5, fig.height=2}

colcount1 <- dim(cp_ordered)[2] + 1
alignment <- rep("c", colcount1)
tablenum <- tablenum + 1
cptablenum <- tablenum 
tablecaption <- paste0('<p class = "center bluegreen">Model Comparision</p>',
                    '<p class = "center bluegreen">Table-',tablenum,'</p>') 
cat(tablecaption)               
datatable(cp_ordered[,c(1,7,2:6)], rownames=F,
          options = list(
            columnDefs = list(list(className = 'dt-center', targets = 0:6))
            ))

```

</div>

<p class="p1">The first model in the list composed of x1 and x2 has the best $C_p$ figure. Three other models have lower MSE including the fifth model (x1,x3,x5,x6) with a size of four which has the lowest MSE of 2117. The small model size of two variables (x1,x2) is the main reason why the first model has the lowest $C_p$ figure. </p>

<p class="p1">The first two models have the best $C_p$ and PRESS figures.</p>

<p class="p1">The predictions for this preliminary ANOVA analysis are inherently overfitted because the actual outcome for the record being predicted is included in the model calculations. For final testing, which begins at Section-11, the model will be tested using independent data records. </p>


# Best Preliminary OLS Model

<p>The model in the second row of Table-6 that is composed of the x1, x3, and x6 variables was selected as the best preliminary model. This model has the lowest PRESS statistic, the second best $C_p$ statistic, and is tied for best adjusted $R^2$ figure in the Adjust-R-Square column. Recall the original names for these variables: </p>

<div class="pdiv">
<ul list-style: none;">
  <li style="list-style: none;">x1: February snow-depth</li>   
  <li style="list-style: none;">x3: March thunderstorm days</li> 
  <li>x6: March rain days</li> 
 </ul> 
</div>

<p>For the purpose of preliminary exploration, the ordinary-least-squares model was constructed using the matrix solution in the Appendix at paragraph 15.3 for the training dataset using these three variables. </p>

<p>Another practical and simple choice for the best model would have the lowest mean-square-error (MSE) in the fifth column of Table-6. Our choice considers the $C_p$ statistic in the third column which includes a complexity penalty for each additional variable added to the model.</p> 


```{r ols_matrix_solution, echo=F}
## Matrix Solution
n <- length(y_model)
x000 <- rep(1,n)

# xxx <- matrix(c(x000,x1,x2,x3),ncol=4)
xxx <- matrix(c(x000,x1,x3,x6),ncol=selected_build_size)
xsqr <- t(xxx) %*% xxx

M <- as.matrix(anova_datax) #xxx[,2:selected_build_size]
M_mean <- matrix(data=1, nrow=n) %*% cbind(mean(M[,1]),mean(M[,2]),mean(M[,3]),
                mean(M[,4]),mean(M[,5]),mean(M[,6]))
                #mean(M[,7]),mean(M[,8]),mean(M[,9]),mean(M[,10]) )

# make difference matrix
D <- M - M_mean
cov_matrix <- (1/(n-1))*t(D) %*% D

unknown <- inv(xsqr) %*% t(xxx) %*% y_model
model_coeff <- unknown[,1]

#coeff <- as.matrix(model_coeff[2:3]) # 2 variables
coeff <- as.matrix(model_coeff[2:selected_build_size])

intr_fullv <- rep(model_coeff[1],dim(validationselect_datax)[1])
intr_full <- as.matrix(intr_fullv)

full <- as.matrix(validationselect_datax)
y_regression_estimate <- (full %*% coeff) + intr_full

```


```{r bestmodelanova, echo=F}
## The data frame 'anova_datax' does not include the result or outcome column.
## selected_ols_results <- anova_model(anova_datax,y_model,x00)
selected_ols_results <- anova_model(selected_datax,y_model,x00)
anova_for_model <- selected_ols_results[[1]]
model_fit_summary <- selected_ols_results[[2]][[1]]
df_regression_model <- selected_ols_results[[2]][[2]]
model_coeff <- selected_ols_results[[2]][[3]]
t0 <- selected_ols_results[[2]][[4]]
p0 <- selected_ols_results[[2]][[5]]
secoef <- selected_ols_results[[2]][[6]]
xx_inv <- selected_ols_results[[2]][[7]]
dfff <- selected_ols_results[[3]]
observations <- selected_ols_results[[4]]
colnames(observations) <- c("Outcome","Fit", "SE Fit",
  "STD Low-Limit", "STD High-Limit","Low-Limit","High-Limit",
  "Residual")
```

```{r olsoutcome3, echo = FALSE,  results='asis'}
 
dfobs <- observations
rounded2 <- lapply(c(1:8), function(u)
 {as.numeric(formatC(dfobs[,u], format = "f", digits = 2))}
)
rounded3 <- lapply(c(1:8), function(u)
 {as.numeric(formatC(dfobs[,u], format = "f", digits = 3))}
)
rounded4 <- lapply(c(1:8), function(u)
 {as.numeric(formatC(dfobs[,u], format = "f", digits = 4))}
)
dfobs[,1] <- rounded3[[1]]
dfobs[,2] <- rounded2[[2]]
dfobs[,3] <- rounded3[[3]]
dfobs[,4] <- rounded3[[4]]
dfobs[,5] <- rounded3[[5]]
dfobs[,6] <- rounded3[[6]]
dfobs[,7] <- rounded3[[7]]
dfobs[,8] <- rounded2[[8]]
#dfobs %>% rmarkdown::paged_table()

colcount1 <- dim(dfobs)[2] + 1
alignment <- rep("c", colcount1)
tablenum <- tablenum + 1
tablecaption <- paste0('<p class = "center bluegreen">Residual Prediction Errors</p>',
                    '<p class = "center bluegreen">Table-',tablenum,'</p>') 
residual_tbl <- dfobs[,c(1,2,8)]
colnames(residual_tbl) <- c("June Rain","Fitted\nPrediction","Residual\nError")

```

## Predictions for Best Preliminary Model

<div class="inline-block align-top align-left-margin more-top-margin">
<div class="inline-fortypercent">

<p>The outcome for June Rain in the first column corresponds to the y-axis in Figure-6 above. Similarly, the 'Fitted Prediction' column corresponds to the x-axis in Figure-6.</p>
<p>Residual errors refer to errors generated during model building&mdash;not final testing errors with independent data records from outside of the training dataset. A residual is the difference between an outcome taken from a data record in the training dataset and the prediction for the same record generated by a model built from the same training dataset. Residual errors will typically be less on average than final testing errors. It is even possible to build an overfitted model with near zero residual errors by finding the curve that goes through all of the data points, but this will actually make the errors for new data records worse due to overfitting. Although a straight regression line typically doesn't pass through any of the points when there are three or more points, predictions for the data records in the training set are still overfitted because their outcomes were used to build the model which inherently reduces the expected error.
</div>

<div class="inline-fivepercent"></div>
<div class="inline-sixtypercent">

```{r olsoutcome2, echo = FALSE,  results='asis'}
cat(tablecaption)
               
datatable(residual_tbl, rownames=F,
          options = list(
            columnDefs = list(list(className = 'dt-center', targets = 0:2))
            ))

```

</div></div>

## Plot of June Rain versus Fitted Predictions

<p class="p1">Considering the outcomes and the fitted predictions for the training data, we construct a best fitted regression line in the plot below. The residual error is indicated by the vertical displacement of the points above or below the solid best-fitted line. The dot color indicates the cluster that is assigned from k-means analysis.</p>

```{r outputvsinputsreducedvars, echo=F}

oy <- y_model
ox <- observations[,"Fit"] 
dfresid <- data.frame(oy,ox)
regress_analysis <- get_regression_analysis(ox,oy)
n <- length(ox)
xbar <- mean(ox)

fit <- regress_analysis[[1]] 
mslope <- regress_analysis[[2]][1]
intrcpt <- regress_analysis[[2]][2]
 
y_est <- regress_analysis[[3]] 
Sxx <- regress_analysis[[4]] [1]
Syy <- regress_analysis[[4]] [2]
Sxy <- regress_analysis[[4]] [3]

degf <- regress_analysis[[5]]
sse <- regress_analysis[[7]][1]
## SST:  Total error from overall mean
sst <- regress_analysis[[7]][2]

## R-Squared: Coefficient of Determination
rsquared <- regress_analysis[[8]][1]
s_sqr <- regress_analysis[[6]][1]
s <- regress_analysis[[6]][1]

## Pearson's Correlation Coefficient
r_corr <- regress_analysis[[8]][2]
## The square of Pearson's Correlation Coefficient is identical 
## to the Coefficient of Determination; i.e., R-Squared, for
## the case only one independent variable.

```

```{r residual_plot_calcs, echo=FALSE}

fit_xmu  <- regress_analysis[[11]][[1]]
fit_ymu <- regress_analysis[[11]][[2]]
s_term <- ((fit_xmu - xbar)^2)/Sxx
low_ymean_limit <- #fit_ymu + tscores[1]*s*(1/n + s_term)^0.5
regress_analysis[[9]][[1]]
high_ymean_limit <-  regress_analysis[[9]][[2]]

## population Walpole 11P7 
low_pop_limit <- regress_analysis[[10]][[1]]
high_pop_limit <- regress_analysis[[10]][[2]]
# one shorter than data
fit_x <- regress_analysis[[12]][[1]]
fit_y <- regress_analysis[[12]][[2]]

```

```{r residtable2, echo = FALSE, results='asis', fig.width=5, fig.height=2}

vecnamelist <- seq(0,model_size,by=1)
commonlist <- rep("c",model_size + 1)
namelist <- paste0(commonlist,vecnamelist)

```


```{r xy_residualerrorplot, echo=FALSE}

par(mar=c(5.1,6,4.1,2.1), bg = rgb(0.87,0.87,0.94, alpha = 0.03)) 
colors2r <- c("#CCAA00","#22BBBB")
colors2 <- c("#22BBBB", "#CCAA00")
colors3 <- c("#CCAA00","#66AAAA","#888888")

xlims <- c(0, 2*ceiling(1.3*max(ox)/2))

if(max(ox) > 100) {
   byx <- 20*floor(max(ox)/100)
   xlabs <- seq(0, 2*max(ox), by=byx)
   xminor <- seq(0, 2*max(ox), by=byx/2)
} else if (max(ox) > 10) {
   byx <- 2*floor(max(ox)/10)
   xlabs <- seq(0, 2*max(ox), by=byx)
   xminor <- seq(0, 2*max(ox), by=byx/5)
} else {
   xlabs <- seq(0, 2*max(ox), by=2)
   xminor <- seq(0, 2*max(ox), by=1)
}
if(max(oy) > 10) {
   byy <- 2*floor(max(oy)/10)
   xlims <- c(0, 2*ceiling(1.3*max(ox)/2))
ylims <- c(0, 300)
} else {
   ylabs <- seq(2, 2*max(oy), by=2)
   yminor <- seq(0, 2*max(oy), by=1)
}
ylabs <- seq(0, 300, by=50)
yabs <- seq(50, 300, by=50)
yminor <- seq(0, 300, by=10)
xlims <- c(20, 180)
ylims <- c(0, 300)

figurenum <- figurenum + 1
fig <- paste0("Figure-",figurenum)
heading <- paste("June Rain vs. Prediction\n",fig)
mycols <- factor(cluster_data$cluster_2_level,levels=c("dry","wet"))

if(model_size > 1) {  
   pltraw <- plot(ox,oy,lwd=2,pch=19,cex=1.5, col=c("#FF9900","#00CCFF")[mycols],
                xlim=xlims, ylim=ylims, xlab="", ylab="",
                main=heading,
                xaxt="n", yaxt="n",yaxs="i",
                cex.main=1.2, cex.sub=1.5,
                col.main="#008800",
                col.lab="#008800",col.sub="red",
                col.axis="#008800")
} else{
    pltraw <- plot(ox,oy,lwd=2,pch=19,cex=1.5, col="#008800",
                xlim=xlims, ylim=ylims, xlab="", ylab="",
                main=heading,
                xaxt="n", yaxt="n",
                cex.main=1.2, cex.sub=1.5,
                col.main="#008800",
                col.lab="#008800",col.sub="red",
                col.axis="#008800")
}
rect(-50,-50,500,500, col= rgb(0.98,0.95,0.95, alpha=0.05))
axis(1, at=xlabs,labels=xlabs, col.axis="black", cex.lab=1.5)
axis(2, at=ylabs,labels=ylabs, col.axis="black", las=2, cex.axis=1.2)
mtext("Prediction (mm)", side=1, line=2.5, cex=1.3,col="#008800")
mtext("June Rain", side=2, line=2.8, cex=1.3,col="#008800")
mtext("", side=3, line=0.5, cex=1.1,col="#008800")
abline(h=yminor, v=xminor, col = "#CCCCFF")
abline(h=yabs, v=xlabs, col = "#AAAAFF")
box('plot', col='#008800')
legend("topleft",
       bg =  rgb(0.97,0.97,0.75, alpha=1),
       legend = c("Dry Year", "Wet Year"),
       pch=19, 
       lty = c(1, 1),
       col=c("#FF9900","#00CCFF"),
       cex = 0.9, # Change legend size
       lwd = 2,
       border = "#008800")

par(new=TRUE)

right_limit <- length(fit_x)
left_limit <- right_limit - 1
leftIndici <- 1:left_limit # one shorter than data
rightIndici <- 2:right_limit # one shorter than data
segments(fit_x[leftIndici],fit_y[leftIndici],
         fit_x[rightIndici], fit_y[rightIndici], col= 'darkgreen')
right_limit <- length(fit_xmu)
left_limit <- right_limit - 1
leftIndici <- 1:left_limit # one shorter than data
rightIndici <- 2:right_limit # one shorter than data

segments(fit_xmu[leftIndici],low_ymean_limit[leftIndici],
         fit_xmu[rightIndici], low_ymean_limit[rightIndici],
         lty=2, col= 'darkgreen')
segments(fit_xmu[leftIndici],high_ymean_limit[leftIndici],
         fit_xmu[rightIndici], high_ymean_limit[rightIndici],
         lty=2, col= 'darkgreen')
segments(fit_xmu[leftIndici],high_pop_limit[leftIndici],
         fit_xmu[rightIndici], high_pop_limit[rightIndici],
         lty=4, lwd=2, col= '#88DD88')
segments(fit_xmu[leftIndici],low_pop_limit[leftIndici],
         fit_xmu[rightIndici], low_pop_limit[rightIndici],
         lty=4, lwd=2, col= '#88DD88')

```

<p class="p1">Predictions appear possibly more accurate for large rain levels.</p>

<p class="p1">The dark set of dashed lines indicate the standard confidence limits for the estimated mean over all years. The wider limits shown by the lighter green dotted-and-dashed lines apply to the population of individual years. The 95% confidence levels are used.</p>

<p class="p1">Predictions generated for the records in the training data are called fitted values because the records were also used to help build the model. Predictions for new independent data records will be inherently less accurate. At the end of the report at paragraph 14.1.5, the results for the independent testing set are plotted in Figure-11.</p><br>

<div class="p1">

```{r olslimits, echo = FALSE,  results='asis', fig.width=5, fig.height=2}

tablenum <- tablenum + 1
tablecaption <- paste0('<p class = "center bluegreen">Confidence Limits</p>',
                    '<p class = "center bluegreen">Table-',tablenum,'</p>') 
cat(tablecaption)               
datatable(dfobs[,c(2:7)], rownames=F,
          options = list(
            columnDefs = list(list(className = 'dt-center', targets = 0:5))
            ))

```

<p class="pminus">'Fit' refers to a prediction made using a data record from the training dataset for building the model.</p>

<p class="pminus">'SE fit' refers to the standard error of the mean which corresponds to the uncertainty of the expected average outcome that falls on the regression line. </p>

<p class="pminus">In columns 3 and 4, STD-Low-Limit and STD-High-limit are the 95% confidence limits for the standard error of the mean. The two right-most columns named Low-Limit amd High-limit are the wider 95% confidence limits for individual outcomes in the population. The computations for these limits were performed using the matrix formulas in Section 12.5 of the Walpole textbook (Walpole, Myers, Myers, Ye, 2007, chap. 12, pp. 458, 460)</p>

</div>

## Null Hypothesis Test for Coefficients

<p class="p1 sub-heading">Does the Model Explain Anything? </p>

<p class="p1">The effect of the six variables is represented by the component for each of them. The expected value for each coefficient is used to calculate a contribution to the prediction. As the regression model is multivariate, the effect of each component that makes up the model can be isolated and analyzed separately. The degree of contribution for each variable is determined and whether or not it is statistically different from zero is summarized in the table below. </p>

<p class="p1">The student-t value (t) and probability (p-value) indicate that the predicted rain level helps explain the actual rain level. June rain is affected by the weather data for the previous eleven months, but there is uncertainty as expressed by the p-value which is a probability. In this ANOVA analysis, mean-squared error (MSE) is calculated using the degrees of freedom in order to estimate the t and F statistics and a p-value. This is different from the common calculation of mean-squared error or root-mean-squared error (RMSE) that uses the sample size in the formula in the rest of the report including cross-validation and final model results.</p>

```{r statistics1temp1, echo = FALSE}

colcount1 <- dim(dfff)[2] + 1
alignment <- rep("c", colcount1)
tablenum <- tablenum + 1
tablecaption <- paste0('<p class = "center bluegreen">Coefficients of Components</p>',
                    '<p class = "center bluegreen">Table-',tablenum,'</p>') 

```

<div class="p1">

```{r statistics1, echo = FALSE, results='asis', fig.width=5, fig.height=2}
colnames(dfff) <- c("Coefficient","Estimate", "Standard Error","t","p-value")
print(xtable(dfff, caption = tablecaption,
            align = alignment, digits=c(0,0,5,4,4,5)), 
            caption.placement ='top', include.rownames=FALSE, type = "html")
```

</div>

```{r pvalue1, echo = FALSE, results='asis' }

p0_txt <- formatC(p0[2] ,format = "f",digits=5)
fitted_coeff <- c("intercept","slope")
p0_txt <- formatC(p0[2] ,format = "f",digits=5)

p0output <- paste('<br>Does the model explain anything meaningful? What is the probability that the slope is actually zero? The non-zero slope of the regression line provides some predictive power that improves our estimate and reduces prediction error, but the slope is a random variable that varies from sample group to another sample group for a different set of records. The probability that the true slope is actually zero instead of the one shown in Figure-1 is expressed as a probability or p-value. The p-value of the slope is actually zero is ',
                  p0_txt,' to five significant decimal places over a 95% confidence interval.')

cat('<p class="p1" style=color:#002200;font-size:20px>',p0output, '</p>')

```

<p  class="p1">As there are several dozen columns in the data that have non-zero correlation, we consider algorithms or models with multiple variables or covariates. Beyond merely non-zero correlation, at a minimum we would perhaps hope that the correlation is statistically different from zero for at least one column and indeed there is at least one as the p-value of 0.004 for February snow depth correlation is significant. If we compare the estimates with actual amount of rain in Figure-2, we might also consider the probability that the true slope of the best-fitted line would be zero.</p>

<p class="p1">The expected value for each coefficient is used to calculate a contribution to the prediction, but there is always a probability that the component is actually 0 and has no effect at all; that is, the coefficient merely fluctuates around zero for a small sample of years but would average to zero if enough years were included in the sample.  The probability each coefficient is 0 is provided in the table below as the p-value in the far right column.</p>

<p class="p1">The standard NULL Hypothesis assumes that the coefficient is non-zero if the p-value is less than 0.05. A natural question might be to question why any statistically insignificant variables are included the model and this is a factor. But any such definition of insignificance such as 0.05 is arbitrary and the goal is to make the best prediction. It certainly is a consideration for the model if the p-value for some components is greater than 0.05, but here we are trying to make the best possible prediction for the amount of rain in June.</p>


##  ANOVA: Analysis of Variance

<p class="p1">The degree of lack-of-fit for the OLS regression model is summarized in the table below. </p> 

<br><div class="p1">

```{r statistics3, echo = FALSE,  results='asis', fig.width=5, fig.height=2}

colcount1 <- dim(anova_for_model)[2] + 1
alignment <- rep("c", colcount1)
tablenum <- tablenum + 1
anovanum <- tablenum
tablecaption <- paste0('<p class = "center bluegreen">ANOVA Summary</p>',
                    '<p class = "center bluegreen">Table-',tablenum,'</p>') 
print(xtable(anova_for_model, caption = tablecaption, 
             align = alignment, digits=c(0,0,0,0,0,2,5)),
       caption.placement ='top', include.rownames=FALSE, type = "html")

mse_anova <- formatC(anova_for_model[2,4] ,format = "f",digits=0)

```

</div><br><p class="p1">The variation is listed in the 'Sum of Squares' (SS) column. The third column named 'Mean Square' is obtained by dividing the 'Sum-of-Squares' column by the 'Degrees of Freedom' column. The F-Value is the ratio of the entry in the first row of the 'Means Square' column to the second entry in that column. This ratio of two variances is a 'F' statistic that describes the lack of fit for the model and corresponds to a probability or p-value that the data is represented by the model.</p>

<p class="p1">The first row presents the sum-of-squares variation explained the model curve or regression line (SSR). The second row is the error (SSE). In the case of model calculations, the error called residual error. This can also be considered variation that not explained by the model. The third row is the total variation (SST).</p>
   
<p class="p1">The mean square column is obtained from the Sum-of-Squares column by dividing by the degrees of freedom. For the error row, the Sum-of-Squares is the SSE and the corresponding number in the Mean Square column is the MSE. The square root of this MSE is not the same as the commonly known RMSE calculated for a population because it is adjusted for the degrees of freedom.</p><div class="p1">

```{r statistics4, echo = FALSE,  results='asis', fig.width=5, fig.height=2}

colcount1 <- dim(model_fit_summary)[2] + 1
alignment <- rep("c", colcount1)
tablenum <- anovanum + 1
tablecaption <- paste0('<p class = "center bluegreen">Quality of Fit</p>',
                    '<p class = "center bluegreen">Table-',tablenum,'</p>') 
print(xtable(model_fit_summary, caption = tablecaption, 
             align = alignment,digits=c(2,2,2)),
       caption.placement ='top', include.rownames=FALSE, type = "html")

rsqr_txt <- formatC(100*model_fit_summary[4,2] ,format = "f",digits=0)
adjust_txt <- formatC(100*model_fit_summary[5,2] ,format = "f",digits=0)
remaining_txt <- formatC(100-100*model_fit_summary[4,2] ,format = "f",digits=0)

rsqrout <- paste('<p class="pminus"> <strong><i>s</i></strong> is a particular estimated variance for ANOVA, by definition the square root of ',mse_anova,' which appears in the error row in the Mean-Square column in ',paste0("Table-",anovanum),'.</p>

<p class="pminus">After converting to percent, <strong><i>R<sup>2</i></sup></strong> is approximately equal to  ', rsqr_txt,'% and adjusted <strong><i>R<sup>2</sup></i></strong> is approximately ', adjust_txt,'%. $R^2$ indicates how much of the variation is explained by the model. If all of the plotted data is on the regression line, the explanation would be perfect and $R^2$ would have a fractional value of 1 or 100%. The remaining ', remaining_txt,'% portion of <strong><i>R<sup>2</i></sup></strong> is not explained by the model. The error level is particularly problematic in dry years as there is a big difference between a small amount of rain and no rain at all. In this case, a constant error level over the range of the prediction levels may not be desirable as it has less impact for very wet years when the fields might be flooded and a much critical impact for rain levels.</p>

<p class="pminus">Percent Variance is similar to <b>coeff var</b> in the <a href="https://support.sas.com/resources/papers/proceedings10/270-2010.pdf">SAS</a> output in Figure 12.1 of Walpole\'s textbook (Walpole, Myers, Myers, Ye, 2007, chap. 12, 462). It is the ratio of <strong>s</strong> to the <strong><i>mean outcome</i></strong> expressed as a percentage.</p>',sep="")

cat("<br>",rsqrout)

```

<script>
sectionnum = sectionnum + 1;
</script>

# Simplified Variable Sets 

<p>Model building often involves selecting correlated variables and eliminating others where a variable corresponds to a column of test data or database. While ridge regression and PCA are suitable for eliminating variables and reducing dimensions to simplify a model, other methods such as ordinary-least-squares (OLS) require manual variable selection. Since there are nearly three hundred possible variables for each year record, a preliminary group of twenty-one variables with the best correlation to June rain were initially selected.</p> 

<div class="pdiv">
<dl>
 <dt>Preliminary Group</dt>
 <dd>Twenty-one variables with the best correlation to June rain (See list in Table-1.)</dd>
 <dt>Reduced Group</dt>
 <dd>The Preliminary group of 20 variables was further reduced based on the highest correlation with June rain and some practical considerations to the following reduced group of six variables. Total snow and average February snow depth (FebSnowDepth) replaced March blowing-snow reports (MarBlowingSnow) because they are more reliable and common.</dd>
 <div class="pdivsmall"><Ul class="square-bullet more-left-margin">
  <li>February snow depth</li>
  <li>FebBlowingSnow</li>
  <li>MarThunder</li>
  <li>MarAverageMaxTemp</li>
  <li>NovAverageMinTemp</li>
  <li>MarRain_wt16</li>
 </ul></div>
 <dt>Select Group</dt>
 <dd>The select group of three variables was chosen as follows: 
</dd>
 <div class="pdivsmall"><Ul class="square-bullet more-left-margin">
 <li>February snow depth</li>
 <li>MarThunder</li>
 <li>MarRain_wt16</li>
</ul></div>

```{r cpgroup, results='asis', echo=FALSE}

threevars <- paste0('<dd>The select group of three variables was chosen based on the earlier model comparison in Section-7.2 of over one hundred models constructed with the possible combinations of the six variables in the reduced group using ordinary-least-squares (OLS). This model of three variables was considered to have the best $C_p$ figure in Table-', cptablenum ,' in paragraph 7.2 using a Leave-One-Out-Cross-Validation (LOOCV) procedure. Although February blowing-snow has higher correlation with June rain than average February snow-depth, snow depth was selected in place of blowing-snow because it is a quantitative measurement whereas blowing-snow only indicates the number of days where blowing-snow was reported based on observational judgement.</dd>')

cat(threevars) 
    
```

</dl></div>

<script>
sectionnum = sectionnum + 1;
</script>

# Principal Components Analysis
<p class="sub-heading">Pre-Processing for Machine-learning</p>

<p>Principal Components Analysis or PCA transforms the variables into an alternative set of statistically independent variables which are inherently uncorrelated. Since the method automatically orders the transformed variables by their importance, principal components analysis can help eliminate unimportant dimensions and thereby simplify a model. The coefficients of the transformed variables are eigenvalues or components of the outcome in a transformed system of independent vectors. Here we start with the reduced set of eight variables.</p>

```{r pca1calc, echo=FALSE}
princomphelp <- prcomp(anova_datax, scale = TRUE)
## The trace of the covariance matrix after centering for zero mean but without scaling is equal to the sum of the eigenvalues. Scaling affects the eigenvalues.
#tr(cov_matrix)
#sum(diag(cov_matrix))
#sum(princompCheck$sdev^2)
#summary_princomp <- summary(princomphelp)
#getAnywhere(summary.princomp)
```

```{r summaryprcomp, echo=FALSE} 

pca_importance <- function(sd) {
  var <- sd^2
  ## var is vector of eigenvalues
  proportion <- var/sum(var)
  cumulative <- cumsum(proportion)
  rbind(`Standard deviation` = sd, `Variance` = var, `Proportion of Variance` = proportion, 
      `Cumulative Proportion` = cumsum(var))
  data.frame(sd,var,proportion,cumulative)
}
importance <- pca_importance(princomphelp$sdev)

```

## Scree plot

```{r eigenshow, echo=FALSE,  results='asis'}

heading <- paste("Scree plot\n",fig)
xlims <- c(1, 6)
ylims <- c(0, 3)
tempvec <- princomphelp$sdev^2
ylabs <- seq(0, 10, by=1)
yminor <- ylabs

figurenum <- figurenum + 1
fig <- paste0("Figure-",figurenum)
princompdf <- data.frame(Component=1:6,Eigenvalue=princomphelp$sdev^2)

ggplot(princompdf, aes(Component, Eigenvalue)) +
  coord_cartesian(xlim = c(1, 6), ylim = c(0, 3)) +
  geom_line(size=2,color = "#008800")  +
  xlab("Component") + ylab("Eigenvalue") +
  scale_x_continuous(breaks = seq(0, 50, 1)) +
  scale_y_continuous(minor_breaks = seq(0 , 5, 0.5), breaks = seq(0, 50, 1)) +
  ggtitle(heading) +
  theme(plot.title = element_text(size=14,hjust = 0.5, color="#008800"),
     legend.title = element_text(size=16),
     legend.text = element_text(size=14),
     legend.background = element_rect(fill = "#DDDDEE"),
     legend.key = element_rect(colour = "transparent", fill = "#DDDDEE"),
     text = element_text(size=12),
     axis.text = element_text(size=14,angle=0, hjust=0.5),
     axis.title = element_text(size = 16),
     panel.grid.minor =   element_line(color = "white",size=0.6),
     panel.grid.major =   element_line(color = "light blue",size=0.8),
     panel.background = element_rect(fill = "#EFE5E5", color = "#008800",
            size = 0.5, linetype = "solid"),
     plot.background = element_rect(fill = "#DDDDEE",color = "#DDDDEE"))

xlims <- c(1, 6)
ylims <- c(0, 1)

tempvec <- princomphelp$sdev^2

ylabs <- seq(0, 10, by=0.2)
yminor <- ylabs

```

## Cumulative Proportion

<p class="p1">The cumulative proportion plot indicates the importance of the independent variables with the largest eigenvalues and suggests how many variables would included a simplified model with fewer dimensions where each independent variable corresponds to a dimension.</p>

```{r eigenshow2, echo=FALSE,  results='asis'}
figurenum <- figurenum + 1
fig <- paste0("Figure-",figurenum)
heading <- paste("cumulativeProportion\n",fig)
importancedf <- data.frame(Component=1:6,Cumulative=importance$cumulative)

ggplot(importancedf, aes(Component, Cumulative)) +
  coord_cartesian(xlim = xlims, ylim = ylims) +
  geom_line(size=2,color = "#008800")  +
  xlab("Number of Components") + ylab("Cumulative Proportion") +
  scale_x_continuous(breaks = seq(0, 50, 1)) +
  scale_y_continuous(minor_breaks = seq(0 , 1, 0.1), breaks = seq(0, 1, 0.2)) +
  ggtitle(heading) +
  theme(plot.title = element_text(size=14,hjust = 0.5, color="#008800"),
     legend.title = element_text(size=16),
     legend.text = element_text(size=14),
     legend.background = element_rect(fill = "#DDDDEE"),
     legend.key = element_rect(colour = "transparent", fill = "#DDDDEE"),
     text = element_text(size=12),
     axis.text = element_text(size=14,angle=0, hjust=0.5),
     axis.title = element_text(size = 16),
     panel.grid.minor =   element_line(color = "white",size=0.6),
     panel.grid.major =   element_line(color = "light blue",size=0.8),
     panel.background = element_rect(fill = "#EFE5E5", color = "#008800",
            size = 0.5, linetype = "solid"),
     plot.background = element_rect(fill = "#DDDDEE",color = "#DDDDEE"))

```

<p class="p2">The principal components are combinations of the scaled raw data variables. Without scaling, there is only one significant component of dominated by February snow depth. We need to apply scaling to include the effects of the other variables. Values close to one indicate the component is very similar to an original variable while values close to zero are not very important. </p>

# Simplified Model based on PCA 

<p class="p2">Since the method automatically ordered the transformed variables by their importance, we exploit this to select the first two principal components for a simplified model. As the model with only the first principal component performed just as well, including the first two components is only accomplished for formal reasons.</p>
    
```{r eigenextra, echo=FALSE}

pc1_components <- data.frame(row.names(princomphelp$rotation), princomphelp$rotation[,1])
colnames(pc1_components) <- c("Number of Components","Coefficient")

eign_vectors <- as.data.frame(princomphelp$rotation)
eign_vectors[,1] <-   -eign_vectors[,1]
eign_vectors[,2] <-   -eign_vectors[,2]
     
dfcomp <- eign_vectors
    pc_data <- princomphelp$x
    ## $x is right side of equation of many rows matching record count
    ## columns are named pc1, pc2, pc3 etc.
    pc1 <- -unname(princomphelp$x[,1])
    pc2 <- -unname(princomphelp$x[,2])
    pc3 <- unname(princomphelp$x[,3])
    pc4 <- unname(princomphelp$x[,4])
    pc5 <- -unname(princomphelp$x[,5])
    # pc6 <- unname(princomphelp$x[,6])
    # pc7 <- unname(princomphelp$x[,7])
    
    ## princomphelp$rotation
    ## matrix of eigenvectors also named pc1, pc2, pc3 etc.
    ## number of rows matches the number of data columns 
    helpdf12 <- data.frame(pc1, pc3, y_model)
    colnames(helpdf12) <- c("PC1","PC3","Outcome")
     
    helpdf14 <- data.frame(pc1, pc4, y_model)
    colnames(helpdf14) <- c("PC1","PC4","Outcome")

```

## Interpretation of Principal Components

<div class="p1">

```{r pcatable2, echo=FALSE, results='asis', fig.width=4, fig.height=2}

colcount <- dim(eign_vectors)[2] + 1
alignment <- rep("c", colcount)
colnames(eign_vectors) <- c("PC-1","PC-2","PC-3","PC-4","PC-5","PC-6")
tablenum <- tablenum + 1
tablecaption <- paste0('<p class = "center bluegreen">Eigenvectors</p>',
                    '<p class = "center bluegreen">Table-',tablenum,'</p>')
print(xtable(eign_vectors, caption = tablecaption, 
              align = alignment, digits=3, size="12pt"),
        caption.placement ='top', include.rownames=TRUE, type = "html")

```

</div>
<p class="p1 more-top-margin">Principal component PC-1 consists of a combination of the six components, but loaded the most with March thunder (x3) and March daily high temperature (x4). Principal component PC-2 is mainly loaded with average February snow depth (x1) and total 
February snowfall (x2). PC-3 is loaded with March rain day reports (x6). </p>

```{r cvglmnetcalcs1, echo=FALSE, results='asis', fig.width=4, fig.height=2}
set.seed(3232)
cvfit <- cv.glmnet(x_helpful_matrix, y_model_matrix,alpha=0.000,lambda = seq(2,1000,by = 2), k=10)
lambdamin <- cvfit$lambda.min
lambda <- lambdamin
lambda1se <- cvfit$lambda.1se
lambda1se_txt <- formatC(lambda1se, format = "f", digits = 4)
lambdamin_txt <- formatC(lambdamin, format = "f", digits = 0)
lambda_txt <- formatC(lambda, format = "f", digits = 0)

sparceCoeffmin <- coef(cvfit, s = "lambda.min")
sparceCoeff1se <- coef(cvfit, s = "lambda.1se")
sparceCoeff <- coef(cvfit, s = lambda)

i <- which(cvfit$lambda == cvfit$lambda.min)
## cvm: mean squares error or mean cross-validated error - a vector of length length(lambda)
## Vector of lambda values: cvfit$lambda

## We need force the value of cvfit$lambda,1) to an exact whole number.
## mse <- cvfit$cvm[i]
## cvfit$lambda[31] - 142 equals -1.776357e-15
ridge_mse_min <- cvfit$cvm[round(cvfit$lambda,1)==lambda]
## ridge_mse
## R-squared = 1 - sse/sst
## These are population statistics, not the sample ones
ridge_rsq <- 1 - cvfit$cvm[i]/var(y_model_matrix)
```

<br>

<script>
sectionnum = sectionnum + 1;
</script>

# Training

<p class="p-double-digits">Models were constructed for four classes of models: ordinary-least-squares (OLS), partial least squares (PLS), pre-processing with principal components analysis followed by PLS, and Ridge regression.  </p>

<p class="p-double-digits">Compounding the problem of high variance that is inherent in weather prediction is the small group of sixteen year records that remained for the final test from a complete dataset of sixty weather year records. Due to this small sample size for the final test, cross-validation was used in an attempt to select the best model. After indicating which model has the best cross-validation performance, we still proceed to build final models for each model in the cross-validation comparison and observe how the cross-validation performance is similar to the final test results for the sixteen fresh data records that were held out only for testing. </p>

<p class="p-double-digits">As this is intended as a scientific study, sixteen year records or approximately thirty percent of the records were reserved for final tests and the remaining forty-four records or approximately seventy percent were used for model building and comparing them using cross-validation. </p>

## Model Types

<div class="p1-double-digits">

<Ul class="square-bullet more-left-margin">
 <li>Ordinary-least-squares (OLS)</li>
 <li>Partial least Squares (PLS)</li>
 <li>Pre-processing with Principal Components Analysis (PCA) followed by supervised learning with Partial Least Squares (PLS).</li>
 <li>Ridge Regression</li>
</ul>

</div>

<p class="p1-double-digits">PCA or Principal Components Analysis can also help eliminate dimensions and thereby simplify a model. In this case, the number of dimensions was reduced to the two most important principal components in the cumulative proportion plot of paragraph 8.2.</p>   

```{r fitpls, echo = FALSE, message = FALSE}
set.seed(32323)

## Center and scale the predictors for the training
## Best tuning parameter ncomp that minimize the cross-validation error, RMSE
plsfitcv <- train(y_model ~ .,data = selected, method = "pls",
          preProc = c("center", "scale"),
          trControl = trainControl("cv", number = 10))
          
pls_mse <- (mean(plsfitcv$resample[,"RMSE"]))^2
pls_rsq <- mean(plsfitcv$resample[,"Rsquared"])
plsfitfinal <- train(y_model ~ .,data = selected, method = "pls",
          preProc = c("center", "scale"))
          
```

      
```{r plspredict2, echo=FALSE}

#pls_test_predictions <- predict(plsfit, newdata= validation_datax)
pls_test_predictions <- plsfitfinal %>% predict(validation_datax)

```

```{r olsfitcv, echo=FALSE}

olsfitcv <- train(y_model ~ .,
              data = selected, method = "lm",
              #preProc = c("center", "scale"),
              trControl = trainControl("cv", number = 10)
)

# olsfitcv$resample
# olsfitcv$resample[,"RMSE"]
ols_mse <- (mean(olsfitcv$resample[,"RMSE"]))^2
ols_rsq <-  mean(olsfitcv$resample[,"Rsquared"])


```

```{r olsfitfinal, echo=FALSE}
ols_fit_final <- train(y_model ~ .,
              data = selected, method = "lm"
)
ols_predict  <- predict(ols_fit_final,newdata=validation_datax)

```

```{r pca_pls_cv, echo=FALSE}
    
    pcafitcv <- train(y_model ~ .,data = reduced_data, method = "pls",
                   preProcess=c("center", "scale", "pca"), 
                   trControl = trainControl("cv", number = 10))
    #pcafitcv$resample
    #pcafitcv$resample[,"RMSE"]
    pca_mse <- (mean(pcafitcv$resample[,"RMSE"]))^2
    pca_rsq <- mean(pcafitcv$resample[,"Rsquared"])
```
    
```{r pca_pls_final, echo=FALSE}
    
    preProc <- preProcess(reduced_datax, method="pca",pcaComp=2)  
    #preProc$rotation #: PC1, PC2, ect . . .

##  Train Model : includes predict method call without outcome variable
    trainPC <- predict(preProc, reduced_datax)
    ##  Add outcome variable
    trainPC[,"y_model"] <- y_model
    pc_modelfit <- train(y_model ~ ., method="pls", data=trainPC)
    
##  Final Test: includes extra pre-process prediction step    
    testPC <- predict(preProc, validation_datax)
    pca_predict <- predict(pc_modelfit,testPC)
    ## getAnywhere(predict.preProcess)
    
```

## Model Variables

<p class="p1-double-digits">Below we list the variable set from Section-8 that is used as input for each model type.</p>

<div class="p1-double-digits">

<dl>
 <dt>Ordinary-least-squares (OLS)</dt>
 <dd>An OLS model was constructed using the select group of three variables with the best $C_p$ figures. </dd>
 <dt>Partial Least Squares (PLS)</dt>
 <dd>PLS was applied to the select group of three variables, the same ones as used for OLS.</dd>
 <dt>Principal Components Analysis (PCA)</dt>
 <dd>After pre-processing the reduced group of six variables with Principal Components analysis, a Partial Least Squares (PLS) model was constructed using the two most important principal components.</dd>
 <dt>Ridge Regression</dt>
 <dd>As ridge regression handles large numbers of variables, this model was trained on the entire preliminary group of twenty-one variables. Ridge regression was used after first attempting Lasso regression which filtered the predictions from deviating from the overall average rainfall too much resulting in an underfitting-like effect.</dd>
</dl>

</div>

```{r olstraining, echo=FALSE}

# olsfit$results
biastable <- tablenum + 5
cvtable <- tablenum + 2
msetable <- tablenum + 6

```

## Model Building with Resampling

<p class="p1-double-digits">Predictions are generated in the course of model building before the best performing model is selected. The average result over the folds for cross-validation is replaced by an average over resampling with replacement from the training group for the default procedure named bootstrap. In bootstrap resampling, samples are drawn twenty-five times with replacement from the training set. It follows that bootstrap resampling may not include every record. While some records may not be tested and some can possibly be tested multiple times.</p>

## Ridge Regression

<p class="p1-double-digits">Ridge regression was included as a model instead of LASSO regression because the later did not perform well for this rather unpredictable weather problem with only many weakly correlated variables. LASSO causes an underfitting problem with little predictive power for cases at a distance from the expected mean value. Increasing the hyperparameter $\lambda$ until the model was simplified caused underfitting and loss of sensitivity where the predicted variation from the average response was filtered too much. The underfitting problem is reduced For ridge regression and some predictive power is obtained. </p>

<p class="p1-double-digits">An overview of ridge regression and the related LASSO method is described in the Appendix at paragraph-15.5. Ridge regression reduces variance by shrinking the regression coefficients by means of the $\lambda$ hyperparameter named in Equation-19 in paragraph 15.5.2</p>
   
### Cross-Validation Curve for Optimum $\lambda$

<p class="p2-double-digits">The plot below shows the cross-validation curve as a red dotted line with upper and lower standard deviation error bars. Two values of $\lambda$ are indicated by vertical dotted lines where the one on the left indicates the value of lambda for minimum mean cross-validated error. The one on the right indicates a higher level of penalization that eliminates more variables but introducing more bias error.</p>


```{r cvglmnetplot, echo=FALSE}
plotdf <- data.frame(cvfit$lambda, cvfit$cvm)
colnames(plotdf) <- c("lambda", "mean_squared_error")
#plot(cvfit$lambda, cvfit$cvm)
figurenum <- figurenum + 1
fig <- paste0("Figure-",figurenum)
## unicode lambda \u03BB
heading <- paste("Mean-Squared Error vs. Hyperparameter \u03BB \n",fig)
ggplot(plotdf ,aes(lambda, mean_squared_error)) +
  geom_point(size=3) + 
  coord_cartesian(xlim = c(60, 100), ylim = c(2170, 2190)) +
  xlab("Hyperparameter \u03BB") + ylab("Mean Square Error") +
  scale_x_continuous(minor_breaks = seq(5 , 495, 10), breaks = seq(0, 500, 10)) +
  scale_y_continuous(minor_breaks = seq(2105 , 2205, 1), breaks = seq(2000, 2200, 5)) +
  ggtitle(heading) +
  geom_vline(xintercept = lambda, linetype = "dashed") +
  geom_hline(yintercept = ridge_mse_min, linetype = "dashed") +
  geom_text(x=89, y=2173, label=paste("Minimum at \u03BB = ", lambda_txt), hjust= 0.08)  +
  theme(plot.title = element_text(size=14,hjust = 0.5, color="#008800"),
     text = element_text(size=12),
     axis.text = element_text(size=14,angle=0, hjust=0.5),
     axis.title = element_text(size = 16),
     panel.grid.minor =   element_line(color = "white",size=0.6),
     panel.grid.major =   element_line(color = "light blue",size=0.8),
     panel.background = element_rect(fill = "#EFE5E5", color = "#008800",
            size = 0.5, linetype = "solid"),
     plot.background = element_rect(fill = "#DDDDEE",color = "#DDDDEE"))

```

```{r calc_ridge_mse_min, echo=FALSE}

pred_lambda_min <- predict(cvfit, newx = testglm , s = lambda)
lambdamin_txt <- formatC(lambdamin,format = "f",digits=1)
lambda1se_txt <- formatC(lambda1se,format = "f",digits=4)
ridge_mse_min_txt <- formatC(ridge_mse_min,format = "f",digits=1)

```

```{r hyperparameterselect, echo=FALSE, results= 'asis'}

txt_lambdamin <- paste0('<p class="p1-double-digits more-top-margin">The minimum possible mean-square-error (mse) of the cross-validation error is ',ridge_mse_min_txt,' which was obtained using the cv.glmnet function. The value of the regularization hyperparameter $\\lambda$ for the minimization is called $\\lambda_{min}$ which equals ',lambda_txt,' in this case. The result is affected by the seed value for the random sequence which was set to 3232 before calling the function. Lower predictive power is perhaps indicated by the $\\lambda_{min}$ value of ', lambda_txt,'.</p>')

cat(txt_lambdamin)

```

<br>  
<div class="inline-block align-top">
 <div class="inline-fortypercent more-right-margin ">

```{r resampleresults2, echo=FALSE, results= 'asis'}

txtabbr <- paste0('<p class="p1-double-digits">Variables associated with storms including March blowing-snow (MarBlowingSnow) have the highest regularization coefficients and appear high on the list.</p>
<p class="p1-double-digits">Thunder appears prominently in March (MarThunder), May (MayThunder), and the previous August (AugThunder), all having negative correlation with June rain. </p>
<p class="p1-double-digits">A wet August has some relationship with dryness in June of the following year: August fog (AugFog), August Thunder (AugThunder), and August Rain (AugRain_WT16) appear with negative correlation. There may be some weak oscillation pattern that reverses in late spring.</p>
<p class="p1-double-digits">March thunder was selected for most other models because of its high absolute correlation with June rain.</p>
</div><div class="inline-sixtypercent">')

cat(txtabbr) 
                             
var_names <- dimnames(sparceCoeff)[[1]]
matrix_size <- dim(sparceCoeff)[2]
coeflist <- sparceCoeff[,matrix_size]
coefdf <- data.frame(var_names,coeflist)
coefdf <- coefdf[order(abs(as.numeric(as.character(coefdf$coeflist))),
                       decreasing=T),]
colnames(coefdf) <- c("Variable","Coefficient")

tablenum <- tablenum + 1
fig <- paste0("Figure-",tablenum)
colcount <- dim(coefdf)[2] + 1
alignment <- rep("ccc", colcount)
tablecaption <- paste0('<p class = "center bluegreen">Table-',tablenum,'</p>',
                       '<p class = "center bluegreen">Regularized Coefficients</p>') 
print(xtable(coefdf[2:16,], caption = tablecaption,  digits=c(0,0,2), align = alignment, size="8pt"),
    caption.placement ='top', include.rownames=FALSE, type = "html")

```

</div></div>

<script>
sectionnum = sectionnum + 1;
</script>


```{r meanbias, echo=F}

n <- length(y_validation)
predict_no_regression <- mean(y_model)
outcome_mean <- mean(y_validation)
outcome_mean_txt <- formatC(outcome_mean,format = "f",digits=2)

model_outcome_var <- get_sample_variance(y_model) 
no_regress_mse <- get_mse(predict_no_regression,y_validation) 
#model_mse <- model_outcome_var
mean_pls_bias_error <- mean(pls_test_predictions)  - outcome_mean
mean_ols_bias_error <- mean(ols_predict)  - outcome_mean
mean_pca_bias_error <- mean(pca_predict)  - outcome_mean
mean_cvmin_bias_error <- mean(pred_lambda_min)  - outcome_mean

pls_total_average <- mean(pls_test_predictions) 
ols_total_average <- mean(ols_predict) 
pca_total_average <- mean(pca_predict) 
cvmin_total_average <- mean(pred_lambda_min) 

outcome_vec <- c(paste0(outcome_mean_txt)," ")

## Variance regression

pls_bias_error_vec <- pls_test_predictions - y_validation 
ols_bias_error_vec <- ols_predict - y_validation 
pca_bias_error_vec <- pca_predict - y_validation 
cvmin_bias_error_vec <- pred_lambda_min - y_validation

final_pred <- data.frame(validation_year,y_validation,pls_test_predictions,ols_predict,pca_predict, pred_lambda_min)
final_pred <- final_pred[order(final_pred$y_validation,decreasing = F),]
colnames(final_pred) <- c("Year","June Rain","PLS","OLS","PCA & PLS","Ridge")

pls_error_var     <- get_mse(pls_test_predictions,y_validation)
ols_error_var <- get_mse(ols_predict,y_validation)
pca_error_var <- get_mse(pca_predict,y_validation)
cvmin_error_var <- get_mse(pred_lambda_min,y_validation)

# pls_error_var
# ols_error_var
# pca_error_var
# cvmin_error_var

MSE <- (caret::RMSE(pls_test_predictions, y_validation))^2
Rsquare <- caret::R2(pls_test_predictions, y_validation)

pls_cv_error <- c(formatC(pls_mse,format = "f",digits=0),
                  formatC(pls_rsq,format = "f",digits=3))
ols_cv_error <- c(formatC(ols_mse,format = "f",digits=0),
                  formatC(ols_rsq,format = "f",digits=3))
pca_cv_error <- c(formatC(pca_mse,format = "f",digits=0),
                  formatC(pca_rsq,format = "f",digits=3))
ridge_cv_error <- c(formatC(ridge_mse_min_txt,format = "f",digits=0),
                    formatC(ridge_rsq,format = "f",digits=3))
pls_r2 <- caret::R2(pls_test_predictions, y_validation)
pls_error_var <- c(pls_cv_error[1],
                   formatC((caret::RMSE(pls_test_predictions, y_validation))^2,
                         format = "f",digits=0),
                   formatC(pls_r2,format = "f",digits=3))
ols_error_var <- c(ols_cv_error[1],
                   formatC((caret::RMSE(ols_predict, y_validation))^2,
                         format = "f",digits=0),
                   formatC(caret::R2(ols_predict, y_validation),
                         format = "f",digits=3))
pca_error_var <- c(pca_cv_error[1],
                   formatC((caret::RMSE(pca_predict, y_validation))^2,
                     format = "f",digits=0),
                   formatC(caret::R2(pca_predict, y_validation),
                     format = "f",digits=3))
lasso_error_var <- c(ridge_cv_error[1],
                     formatC((caret::RMSE(pred_lambda_min, y_validation))^2,
                     format = "f",digits=0),
                  formatC(caret::R2(pred_lambda_min, y_validation),
                     format = "f",digits=3))

no_regression <- formatC((caret::RMSE(predict_no_regression, y_validation))^2,
                     format = "f",digits=0)
no_regression_mse <- c(no_regression,no_regression,0)

pls_vec <- c(pls_total_average, mean_pls_bias_error)
ols_vec <- c(ols_total_average, mean_ols_bias_error)
pca_vec <- c(pca_total_average, mean_pca_bias_error)
ridge_vec <- c(cvmin_total_average, mean_cvmin_bias_error)

accuracy_names1 <- c("Average","Bias Error")
cv_error_names <- c("Average MSE","$R^2$")
final_error_names <- c("Cross-Validation","Final Test","Final $R^2$")

simple_averages <- data.frame(predict_no_regression,outcome_mean)
prediction <- data.frame(accuracy_names1, outcome_vec, pls_vec,ols_vec,pca_vec,ridge_vec)
cv_error <- data.frame(cv_error_names,
                               pls_cv_error,ols_cv_error,
                               pca_cv_error,ridge_cv_error)
prediction_error <- data.frame(final_error_names,
                               pls_error_var,ols_error_var,
                               pca_error_var,lasso_error_var,
                               no_regression_mse)
colnames(prediction) <- c("Statistic","Outcome",
     "PLS","OLS","PCA","Ridge $\\lambda_{min}$")
colnames(simple_averages) <- c("Training","Final Test")
colnames(cv_error) <- c("Statistic","PLS","OLS","PCA","Ridge $\\lambda_{min}$")
colnames(prediction_error) <- c("Statistic","PLS","OLS","PCA","Ridge $\\lambda_{min}$","No Regression")

```

# Model Comparisons using Cross-Validation

<p class="p1">Cross-validation was performed to compare the performance of alternative performance models. There were 10-folds. In Leave-One-Out-Cross-Validation, all records are included as each record is assigned to one of the folds. </p>

<p class="p1">Beginning in second column, these methods are Partial Least Squares (PLS), ordinary-least-squares (OLS), pre-processing with Principal Components Analysis (PCA) followed by PLS, and finally ridge regression.</p>

<p class="p1">MSE is an abbreviation for mean-squared-error. </p>

<div class="p1">

```{r cv_error, echo=FALSE, results='asis', fig.width=7, fig.height=7}

colcount1 <- dim(cv_error)[2] + 1
alignment <- rep("c", colcount1)
tablenum <- tablenum + 1
tablecaption <- paste0('<p class = "center bluegreen">Summary of Cross-Validation Results</p><p class = "center bluegreen">Table-',tablenum,'</p>')

print(xtable(cv_error[1,], caption = tablecaption, 
             align = alignment, digits=c(0,0,3,3,3,3)),
       caption.placement ='top', include.rownames=FALSE, type = "html")
       
txtabbr <- paste0('</div><br><p class="p1">The value of the hyperparameter $\\lambda$ that minimizes the cross-validation error is called $\\lambda_{min}$ which is ',lambda_txt,'. The minimum possible mean-square-error (mse) is ',ridge_mse_min_txt,'.</p>')

cat(txtabbr)

```


<p class="p1">From the performance results of the cross-validation comparison, we might select OLS as the model with the lowest average mean-square-error over all of the folds. It is difficult to make any conclusions about the accuracy of the average because the results for different folds are well-correlated, but we have a some idea about the best model. </p>

# Predictions

<p class="p1">If we have enough records beyond what was needed for model building, we can perform final testing with records which were not used for training or cross-validation, If there are enough records, we might even be able to select mostly independent and uncorrelated record sets to determine the statistical accuracy in terms of the estimated variance and bias of the estimates. If there are few samples or prior knowledge, we need recognize the uncertainty that is present if the mean and variance were from a normal distributed population. </p>

<p class="p1">Recognizing the limits inherent in a small of records, predictions based on the final models were nevertheless tested using the test group of sample records which were not used for training the model. We might gain some insight from the cross-validation results above because there is only limited data consisting of fifty-nine weather year records.</p>

```{r predictions, echo=FALSE, results='asis', fig.width=4, fig.height=2}

txtabbr <- paste0('<p class="p1">Table-',cvtable,' shows the cross-validation comparison of mean-square-error for the models. Table-',msetable,' compares the corresponding final models built using the entire training dataset against the holdout test dataset that was not previously used either for cross-validation analysis or training the final model. The overall bias for the final models are shown in Table-',biastable,'.</p>')

cat(txtabbr)

```

## Accuracy


### Final Predictions

<p class="p2-double-digits">The prediction error is the difference between June Rain in the second column and the predictions in Columns 3-6.</p>

<div class="p2">

```{r detailedoutcomestable, echo=FALSE, results='asis'}

colcount1 <- dim(final_pred)[2] + 1
alignment <- rep("c", colcount1)
tablenum <- tablenum + 1
tablecaption <- paste0('<p class = "center bluegreen">Final Prediction Test</p><p class = "center bluegreen">Table-',tablenum,'</p>')

print(xtable(final_pred, caption = tablecaption, 
             align = alignment, digits=c(0,0,1,1,1,1,1)),
       caption.placement ='top', include.rownames=FALSE, type = "html")
```

</div>
<br><p class="p2-double-digits">Wet years with June rain exceeding 150-mm in 2013, 1968, and 2009 had PLS estimates of at least 109-mm of rain or higher as shown in the last three rows of the table. The four driest years at the top with less than 60-mm of June rain in 1991,1984, 1983, 1986 have PLS estimates below 100-mm except for the year 1984 where the model does not predict a dry year. The year 1984 followed some cold winters before years where the level of Lakes Huron-Michigan was high.</p>

### Average June Rain

<p class="p2-double-digits">A prediction can made before applying regression. Before applying regression, we can predict the average amount of rain for the training group. The same prediction would be made for any new record. The purpose of regression is to improve this, to reduce the mean-squared error compared to what it is without regression. </p>

<div class="p2-double-digits">

```{r outcomestable, echo=FALSE, results='asis'}

# The estimates are based on a single dataset alone without any prior knowledge.
colcount1 <- dim(simple_averages)[2] + 1
alignment <- rep("c", colcount1)
tablenum <- tablenum + 1
tablecaption <- paste0('<p class = "center bluegreen">Average June Rain in millimeters</p><p class = "center bluegreen">Table-',tablenum,'</p>')

print(xtable(simple_averages,caption = tablecaption, 
             align = alignment, digits=c(0,0,0)),
       caption.placement ='top', include.rownames=FALSE, type = "html")
```

</div>
<br> 
<p  class="less-top-margin p2-double-digits">The rather small number of years in the test group can easily cause some appreciable difference in the average rain for the test group. Nearly 200-mm of rain in the years 1968 and 2017 helps to increase the average the test group. It will be later in the century before this improves even a little.</p> 

### Prediction Bias

<p class="p2-double-digits">The bias of the average prediction is shown in the table below. Although cross-validation would generally be used to select the best model, We still show final test results for all of the models that were compared using cross-validation. Based on the cross-validation results, we would select Partial Least Squares (PLS) as the final model.</p>

<div class="p2">

```{r prediction_table_before_regression, echo=FALSE, results='asis', fig.width=7, fig.height=7}

colcount1 <- dim(prediction)[2] + 1
alignment <- rep("c", colcount1)
tablenum <- tablenum + 1

tablecaption <- paste0('<p class = "center bluegreen">Bias</p><p class = "center bluegreen">Table-',tablenum,'</p>')

print(xtable(prediction, caption = tablecaption, 
             align = alignment, digits=c(0,2,2,2,2,2,2)),
       caption.placement ='top', include.rownames=FALSE, type = "html")

```

</div>
<br>

<p class="p2">The final predictions are all lower than the actual average June rain which indicates a bias problem, but the overall mean-square-error is lower for PLS and OLS and ridge regression in Table-18. As mean-square-error is a wider measure that includes bias error and variance, it may be possible to the reduce the overall error by choosing the bias level in the optimum way. </p>

### Mean Square Error

<p class="p2-double-digits">Although cross-validation would generally be used to select the best model, we still show final test results for all of the models that were compared using cross-validation. Based on the cross-validation results, we would select Partial Least Squares (PLS) as the final model. The results for the final test in Table-12 below should be compared with the cross-validation results of Table-11 above. To facilitate this, the cross-validation (CV) result is repeated below in the first row. </p>

<div class="p2">
```{r msetable, echo=FALSE, results='asis'}

# In the table above, 'variance' means the sample-variance statistic or the estimated variance based on the data. The tendency of 'regression to the mean' is the origin of the name regression.  
# The sample variance approaches the mean-squared-error for large sample sizes. For small groups of samples, the sample variance is the best estimate of the true variance or squared-deviation from the regression line. 

colcount1 <- dim(prediction_error)[2] + 1
alignment <- rep("c", colcount1)
tablenum <- tablenum + 1
tablecaption <- paste0('<p class = "center bluegreen">Average Mean-Square-Error (MSE)</p><p class = "center bluegreen">Table-',tablenum,'</p>')

print(xtable(prediction_error, caption = tablecaption, 
             align = alignment, digits=c(0,0,3,3,3,3,3)),
       caption.placement ='top', include.rownames=FALSE, type = "html")

no_regress_mse_txt <- formatC(no_regress_mse,format = "f",digits=0)
pls_error_var_txt <- formatC(pls_error_var[1] ,format = "f",digits=0)

pls_r2_percent <- formatC(100*pls_r2,format = "f",digits=1)
pls_r2_unexplained <- formatC(100-100*pls_r2,format = "f",digits=1)

txtabbr <- paste0('</div><br>
<p class="p2">The $R^2$ value of ',pls_error_var[3],' in the bottom row under PLS indicates that partial-least-squares regression can account for ',pls_r2_percent,'% of the observed rain variance which means that ',pls_r2_unexplained,'% of the variation is unexplained.</p>
<p class="p2">For the <strong><i>No-Regression</i></strong> model, the single unique prediction is the average June rain over all years. The purpose of regression is to find a better prediction that depends on the data record. For regression to be helpful, it must provide a more accurate prediction than the <strong><i>No-Regression</i></strong> model with a smaller mean-squared-error than the ',no_regress_mse_txt,' for the <strong><i>No-Regression</i></strong> model in the right-most column. The trivial $R^2$ value of 0 under <strong><i>No-Regression</i></strong> reflects that $R^2$ is a comparison with the <strong><i>No-Regression</i></strong> case.</p>')

cat(txtabbr)


```

<p class="p2"></p>

<p class="p2">With the exception of PCA and the trivial case of the <strong><i>No-Regression</i></strong> model, the mean-squared error is lower for the average final MSE (Average Final MSE) in the second row compared to average cross-validation MSE (Average CV MSE) in the first row. This reflects that the model training data for a particular fold does not include the records for which it provides predictions. For ten folds, the fold training datasets are 10 percent smaller which leads to more variation. On the other hand, the effect of any outlier cases is reduced by averaging over all of the folds.</p>

```{r finalerrorplot, echo=FALSE, results='asis'}

y2 <- y_validation
#x2 <- pls_predictions
x2 <- y_regression_estimate
xbar<- mean (x2)
regress_analysis <- get_regression_analysis(x2,y2)
 
#fit <- lm(formula = y2 ~ x2)
fit <- regress_analysis[[1]]
 
#mslope <- fit$coefficients[2]
mslope <- regress_analysis[[2]][1]
intrcpt <- regress_analysis[[2]][2]

y_est <- regress_analysis[[3]]
Sxx <- regress_analysis[[4]] [1]
Syy <- regress_analysis[[4]] [2]
Sxy <- regress_analysis[[4]] [3]
 
degf <- regress_analysis[[5]]
s_sqr <- regress_analysis[[6]][1]
s <- s_sqr^0.5
 
```

### Prediction Accuracy Plot

```{r residual_results_pls, echo=FALSE, results='asis'}

# txt1 <- paste0("RMSE error for the Partial Least Squares method was ",s_pls)
# cat("<h5>",txt1,"</h5>")

```

```{r residual_plot_calcs_pls, echo=FALSE}

fit_xmu  <- regress_analysis[[11]][[1]]
fit_ymu <- regress_analysis[[11]][[2]]

s_term <- ((fit_xmu - xbar)^2)/Sxx
tscores <- qt(c(.025, .975), df=degf)
low_ymean_limit <- regress_analysis[[9]][[1]]
high_ymean_limit <-  regress_analysis[[9]][[2]]

## population Walpole 11P7
low_pop_limit <- regress_analysis[[10]][[1]]
high_pop_limit <- regress_analysis[[10]][[2]]

fit_x  <- regress_analysis[[12]][[1]]
fit_y <- regress_analysis[[12]][[2]]

```


```{r pred_xy_residualplot, echo=FALSE}
par(mar=c(5.1,6,4.1,2.1), bg=rgb(245,245,250, max=255))

final <- data.frame(x2,y2)
colnames(final) <- c("prediction", "june_rain")

seg_df <- data.frame(x=fit_x[leftIndici],
                     y=fit_y[leftIndici],
                     xend=fit_x[rightIndici],
                     yend=fit_y[rightIndici])
seg_mu_low <- data.frame(x=fit_xmu[leftIndici],
                     y=low_ymean_limit[leftIndici],
                     xend=fit_xmu[rightIndici],
                     yend=low_ymean_limit[rightIndici])
seg_mu_high <- data.frame(x=fit_xmu[leftIndici],
                     y=high_ymean_limit[leftIndici],
                     xend=fit_xmu[rightIndici],
                     yend=high_ymean_limit[rightIndici])
seg_pop_low <- data.frame(x=fit_xmu[leftIndici],
                      y=low_pop_limit[leftIndici],
                      xend=fit_xmu[rightIndici],
                      yend=low_pop_limit[rightIndici])
seg_pop_high <- data.frame(x=fit_xmu[leftIndici],
                      y=high_pop_limit[leftIndici],
                      xend=fit_xmu[rightIndici],
                      yend=high_pop_limit[rightIndici])

figurenum <- figurenum + 1
fig <- paste0("Figure-",figurenum)
heading <- paste("June Rain vs. Prediction\n",fig)

ggplot(final, aes(prediction,june_rain)) +
  coord_cartesian(xlim = c(20, 160), ylim = c(0, 250)) +
  geom_point(size=3.5,color = "#008800")  +
  geom_segment(data=seg_df, aes(x, y, xend=xend, yend=yend),color = "#229922",lwd=1) +
  geom_segment(data=seg_mu_low, aes(x, y, xend=xend, yend=yend),color = "#DDBBAA",lwd=1) +
  geom_segment(data=seg_mu_high, aes(x, y, xend=xend, yend=yend),color = "#DDBBAA",lwd=1) +
  geom_segment(data=seg_pop_low, aes(x, y, xend=xend, yend=yend), color = "#22AABB") +
  geom_segment(data=seg_pop_high, aes(x, y, xend=xend, yend=yend),color = "#22AABB") +
  xlab("Prediction (mm)") + ylab("June Rain (mm)") +
  scale_x_continuous(minor_breaks = seq(0 , 450, 10), breaks = seq(0, 500, 20)) +
  scale_y_continuous(minor_breaks = seq(0 , 450, 10), breaks = seq(0, 500, 20)) +
  ggtitle(heading) +
  theme(plot.title = element_text(size=14,hjust = 0.5, color="#008800"),
     legend.title = element_text(size=16),
     legend.text = element_text(size=14),
     legend.background = element_rect(fill = "#DDDDEE"),
     legend.key = element_rect(colour = "transparent", fill = "#DDDDEE"),
     text = element_text(size=12),
     axis.text = element_text(size=14,angle=0, hjust=0.5),
     axis.title = element_text(size = 16),
     axis.title.x = element_text(margin = margin(t = 7)),
     axis.title.y = element_text(margin = margin(l = 13)),
     panel.grid.minor =   element_line(color = "white",size=0.6),
     panel.grid.major =   element_line(color = "light blue",size=0.8),
     panel.background = element_rect(fill = "#EFE5E5", color = "#008800",
            size = 0.5, linetype = "solid"),
     plot.background = element_rect(fill = "#DDDDEE",color = "#DDDDEE"))


```


<p class="p2">The 95% Confidence limits for the estimated average rain on the green regression line are indicated by the light-brown line and the wider limits for particular years are indicated by the blue-green line. </p>

<p class="p2">Predictions for moderate rain in the range 80-100 mm appear be more reliable than larger ones over 100-mm. The magnitude of the error appears more uncertain when the prediction exceeds 100-mm.</p>

<script>
sectionnum = sectionnum + 1;
</script>

# Appendix

## Prediction Error

<p class="p1-double-digits">Prediction error is the difference between an actual result and a prediction. From the errors, the mean-squared-error of group of prediction errors is a very useful statistic. Mean-squared-error (MSE) can be decomposed into bias and a precision or variance part. Statistical bias has a mathematical definition apart from the common meaning of fearful or pre-conceived ideas as the difference between an expected prediction and the average outcome. Variance has a central role in statistics as a measure of variability. Bias typically increases when we eliminate variables to simply a model or minimize the total error or MSE. Variance always increases when a variable added to a model but the addition is justified when the reduction of the bias error improves total MSE. </p>

## Machine-Learning

### Unsupervised Learning

#### Unlabeled Data 

<p class="p3-double-digits">For unsupervised learning and exploration, data is considered without labels, prior assumptions, or preconceived ideas. At this stage, it is not even information yet. Consider an example: the data are dot patterns, but we have no assumptions or prior knowledge about what they represent. Could they be symbols or characters or are they just random dot patterns? We attempt to explore any relationships between the data variables. </p>

####  Exploration for Clusters

<p class="p3-double-digits">Searching for clusters is a form of unsupervised learning that can help discover patterns and relationships among variables. We should observe if the variables appear to be a list of discrete things or categories or is it a list of decimal numbers. We might explore two columns of numbers that represent the x and y coordinates of dots using K-means cluster analysis to determine if we can find any patterns or clustering among the dots. Assume that our cluster search finds ten types of dot clusters. We have not assigned names or labeled the patterns with respect to anything outside of the data that requires prior knowledge or assumptions yet. Any such labeling would be beyond cluster analysis and unsupervised learning. Predictive models cannot be built from cluster analysis. </p>

### Supervised Learning and Model Building

#### Model Architecture and Prediction Algorithms

<p class="p3-double-digits">After our exploration and cluster analysis and other unsupervised learning where we found ten types of dot patterns, we suspect that the patterns correspond to the ten digits from 0-9 that we have prior knowledge about from having attended school and what we have reason to believe about the patterns. Next we might attempt to develop an algorithm to assign or label the patterns as the digits 0-9. Building or training such an algorithm or model to label the patterns is an example of supervised learning. The patterns must be assigned to one of the ten digits that we are familiar with. Even if some of dots are misplaced or missing or there are other random dots, our algorithm will attempt to assign one of the ten labels. </p>

#### Regression

<p class="p3-double-digits">A regression model is an example of supervised learning that involves using prior knowledge about the relationship or correlation between variables to make predictions where the variables are columns of decimal numbers. Although the numbers appear to be part of an infinite continuous set, we are still ultimately limited to a countable set of discrete set of decimal numbers that our computer can handle, but too many to label. However, decimal numbers are an ordered set that opens-up new possibility to consider a column of numbers as a space-like <i>vector</i> or <i>vectors</i> if there is more than one column.</p>

<p class="p3-double-digits">Let's consider the problem of regression for the amount of June rainfall. There are three columns of decimal data, June rainfall, February snowfall, and average March temperature. How can regression help us predict June rainfall in future years?</p>  

<p class="sub-heading p3-double-digits">Purpose of Regression</p>
<p class="p3-double-digits">The simplest and most basic model would predict that the amount of rain for a given month is the average amount of rain the month over all the years in the training set. This simple prediction is an unbiased estimate with an uncertainty level described by the sample variance for June rain in the training set. The purpose of regression is to find a better prediction than the average for past years, ideally by finding a better unbiased estimate with lower variance where unbiased means that the expected prediction or rain estimate will approach the true average rain level for a large number of years. Building a model involves prior knowledge about the world as well as the training dataset. We apply our model to predict a future data record, we consider the conditional probability of the possible outcome given the data record. Typically the prediction chooses the most probable outcome.</p>

<p class="sub-heading p3-double-digits">Another Regression Example</p>
<p class="p3-double-digits">Consider the problem of predicting the weight of a student in an elementary school. The simplest and unbiased prediction without using regression is the average weight of all students in the school. However, we have good reason to believe we can make a better estimate if we consider the age or the height of the student. Adding a variable is called model adjustment. </p>

<p class="p3-double-digits">We are presented with weight data for fifth graders. The average weight for the entire fifth grade is an unbiased estimate of the student weight. When we apply some prior or other knowledge about student age and weight, this inherently introduces bias for the estimate. The probability distribution for the entire school is replaced by a different distribution based on student age that is a conditional probability for the student weight. Predictions involving conditional probabilities usually depend upon Bayes' theorem.  If there are many students in each age group in the same dataset, this will usually be a good estimate, but it would be more problematic if the age data is prior data or knowledge about other schools which can lead to bias. Other than average rainfall for the training dataset and the model based all the years, the other models with eliminated columns should be evaluated for bias. </p>

<p class="p3-double-digits">Consider a more complicated model. What if we had four student columns, weight, age, the height of the mother, and the last digits of some identification number. Eliminating columns can bias predictions. It's not hard to understand why we would want a data reduction method to remove the column about the last digits of the identification number, that would obviously add only noise, greater variance and make the prediction worse. Our data reduction method might also eliminate the height of the parent because the correlation is well below 1, but leaving it out will probably change the prediction, although it is not obvious yet if the bias of the change is up or down. Finally, the data reduction method might eliminate the student age, leaving only the grade level, which will also change the prediction and bias it either up or down for a somewhat complex reason. </p>

<p class="p3-double-digits"> Consider another example: if a customer has purchased bread, we might compute the probability that butter might be purchased on the next visit by attempting to guess what product a customer will buy based on a prior event by introducing a conditional probability.  This typically changes the probability of purchasing the next product and introduces bias at the same time. The added bias can only be justified if the variance regarding the next purchase is reduced. </p>

<br>

## Matrix Solution for Ordinary-Least-Squares Regression

<p class="p1-double-digits">Similar to the matrix solution called Cramer's Rule, 
  the linear regression equation for multiple covariates can be solved using matrices.</p>  

###### $$\begin{aligned} y = X \beta + \epsilon \end{aligned}$$

###### $$ y = \begin {bmatrix} 
    y_1 \\
    y_2 \\
    \vdots \\
    y_n \\
    \end{bmatrix} \quad X = \begin {bmatrix} 
    1      & x_{11}    & x_{21}    & \cdots & x_{k1} \\
    1      & x_{12}    & x_{22}    & \cdots & y_{k2} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1      &  x_{1n}   & x_{2n}    & \cdots & x_{kn} \\
    \end{bmatrix} \quad  \beta = \begin {bmatrix} 
    \beta_1 \\
    \beta_2 \\
    \vdots \\
    \beta_n \\
    \end{bmatrix} \quad  \epsilon = \begin {bmatrix} 
    \epsilon_1 \\
    \epsilon_2 \\
    \vdots \\
    \epsilon_n \\
    \end{bmatrix}
    $$

<br>
<p class="p1-double-digits">The total error is as follows:  </p>

###### $$SSE = (y - Xb)'(y-Xb)$$

<br>
<p class="p1-double-digits">We minimize the error by first taking the derivative of the error with respect to the vector $b$ and setting it equal to zero. This involves some vector calculus.</p>

###### $$\frac{\partial (SSE)}{d \mathbf{b}} = 0$$

<br>
<p class="p1-double-digits">The $\mathbf {b}$ vector of the estimated coefficients can be solved using more vector and matrix calculus. This is somewhat like a Cramer's rule solution with matrices for this problem.</p>  

###### $$\mathbf {b} = (X'X)^{-1} x' y$$

<br>
<p class="p1-double-digits">The vector $\mathbf {b}$ is an unbiased estimate of the true regression coefficients $\mathbf {\beta}$. The inverse matrix $\mathbf {(X'X)^{-1}}$ is an important object in itself as it represents a variance-covariance matrix for the estimated regression coefficients $\mathbf {b}$.</p>

### Inverse Covariance Matrix

<p class="p2-double-digits" >This important matrix is used many formulas.</p>

<div class="p2-double-digits">

```{r matrix_solution_covariance, echo = FALSE, results='asis'}

xx_inv_df <- as.data.frame(xx_inv)
colcount1 <- dim(xx_inv_df)[2] + 1
alignment <- rep("c", colcount1)
tablenum <- tablenum + 1
tablecaption <- paste0('<p class = "center bluegreen p2-double-digits"">Inverse Covariance Matrix</p>','<p class = "center bluegreen">Table-',tablenum,'</p>')

print(xtable(xx_inv_df, caption = tablecaption,
       align = alignment, digits=4), 
       caption.placement ='top', include.rownames=FALSE, type = "html")
```

</div>

```{r covariance_comments, echo = FALSE, results='asis'}

xx11_txt <- formatC(xx_inv[1,1],format = "f",digits=4)
xx22_txt <- formatC(xx_inv[2,2] ,format = "f",digits=4)
xxout <- paste('<br><p class="p2-double-digits" >The component in the upper left corner of ',xx11_txt,' is the covariance multiplying factor for the outcome-axis intercept if all inputs were zero. The next diagonal element ',xx22_txt,' corresponds to the confidence interval for the coefficient of <strong>x<sub>1</sub></strong>.</p>' )

cat(xxout)

```

### The HAT Matrix

<p class="p2-double-digits">The inverse covariance matrix is used to calculate the HAT matrix.</p> 

###### $$HAT = X(X'X)^{-1}X'$$

<br><p class="p2-double-digits">The diagonal elements of the HAT matrix $h_{ii}$ are used to help calculate the confidence interval for the mean response for a particular data record $x=\{1,x_{i1},x_{i2},x_{i3}, \;.\; .\; .\}$ in the matrix X above at Equations-2 where the first term is always 1.</p> 

###### $$h_{ii} = x'(X'X)^{-1}x$$

<br><p class="p2-double-digits">This formula is from Section 12.10 of the Walpole textbook (Walpole, Myers, Myers, Ye, 2007, chap. 12, pp. 485)</p>

### Variance

<p class="p2-double-digits">Expressing the elements of the above covariance matrix as $c_{ii}$, we calculate the variance of the individual coefficients by multiplying the corresponding diagonal element by the variance $\sigma^2$.</p>

###### $$\begin{aligned} \sigma_{b_i}^{2} = c_{ii}\sigma^2  && {\text{i = 0,1,2, . . .}} \end{aligned}$$

<br>
<p class="p2-double-digits">The total variance can be expressed as the sum of these terms.</p>

###### $$\sigma^{2} = \sigma_{b_0}^{2} +  \sigma_{b_1}^{2} + \sigma_{b_2}^{2} + \quad . \quad . \quad .$$

<br>

## PRESS Statistics

<p class="p1-double-digits">PRESS is an acronym meaning <strong><i>Prediction Sum-Of-Squares</i></strong> which is calculated using a Leave-One-Out-Cross-Validation.  It is similar to the Error Sum-of-Squares (SSE) but the error (e) is corrected for its confidence level. Since the model for a particular cross-validation prediction is always built without the corresponding record, an error can simply be called a prediction error, dropping the residual term used for training errors.</p>

<p class="p1-double-digits">Let June rainfall for the $i_{th}$ record be denoted by $y_i$. The predicted rainfall is denoted by $\widehat y_i$ where the model for the prediction is built without the record that corresponds to $y_i$ according to Leave-One-Out-Cross-Validation (LOOCV). Let $e_i$ be the prediction error for the $i_{th}$ record.</p>

###### $$e_i = y_i - \widehat y_i$$

<br><p class="p1-double-digits">The error $e_i$ is corrected as follows where $h_{ii}$ is a diagonal matrix element of the HAT matrix developed during the ordinary-least-square solution above at Equation-7. The corrected error is denoted by $\delta_i$.</p>

###### $$\delta_i = \frac {e_i} {1-h_{ii}}$$

<br><p class="p1-double-digits">The $h_{ii}$ terms appear in the computation of the confidence interval of the mean response. Finally, the press statistic is calculated as follows: </p>

###### $$PRESS = \sum_{i=0}^n {\delta_i}^2$$

<br><p class="p1-double-digits">The PRESS results appear as a column in the table below.  Another adjusted or modified version of $R^2$ is calculated where the PRESS figure is substituted for the total sum of squared-errors (SSE). </p>

###### $${R^2}_{predict} = 1 - \frac {PRESS} {SST}$$

<br><p class="more-top-margin more-bottom-margin p1-double-digits">The ${R^2}_{predict}$ results appear in the R-Square-Predict column in the table below.  The total sum of squares (SST) is the numerator of the mean-squared-error formula; that is, the mean-squared-error multiplied by the quantity of records. These equations are from Section 12.11 of the Walpole textbook (Walpole, Myers, Myers, Ye, 2007, chap. 12, pp. 490-491)</p>

<br>
<div class="p1-double-digits">

```{r presstable, echo = FALSE, results='asis', fig.width=5, fig.height=2}

colcount1 <- dim(ordered[,c(1,2,3,5:6,8)])[2] + 1
alignment <- rep("c", colcount1)
tablenum <- tablenum + 1
tablecaption <- paste0('<p class = "center bluegreen">PRESS Statistics</p>',
                    '<p class = "center bluegreen">Table-',tablenum,'</p>')
cat(tablecaption)                    
                    
press_report <- ordered[,c(1,2,4,3,6,8)]
datatable(press_report, rownames=F,
          options = list(
            columnDefs = list(list(className = 'dt-center', targets = 0:5))
            ))
```

## LASSO and Ridge Regression

<p class="p1-double-digits">LASSO is an abbreviation for Least Absolute Shrinkage and Selection Operator. This method can reduce overfitting by automatically eliminating uncorrelated and unimportant variables. As illustrated by Equation-13 below, errors are weighted using their absolute values which corresponds to L-1 normalization or the Manhattan taxi-cab distance between points. It depends on a regularization parameter $\lambda_1$, which can be selected to minimize the absolute values of the errors. While LASSO can reduce overfitting, it can result in underfitting if there are few well-correlated variables to be distinguished from many weakly correlated ones. Predictions can loose sensitivity and cluster too close to the mean while failing to predict outlying cases if the regularization becomes too strong as the value of the coefficients are reduced in order to eliminate unimportant variables. </p>

<p class="p1-double-digits">Ridge regression is related to Lasso regression as can seen by comparing Equation-13 and Equation-14 below. Ridge regression also attempts to lower variance by reducing the coefficients but they remain non-zero. It punishes large errors using L-2 normalization to weigh errors at the square Euclidean distance, which is the common understanding of distance, compared to the absolute error values used for LASSO.</p>

<p class="p1-double-digits">Consider a sample consisting of N cases, each of which consists of p covariates and a single outcome. Let $y_i$ be the outcomes and $X$ be the data matrix. The objective is to solve this equation for the unknown column matrix $\beta$ of coefficients. </p>
   
###### $$\min_{\beta_{0},\beta } {\left( \|y-\beta_{0}-X\beta \|_{2}^{2} \right)}$$

###### $${\text{ subject to: }}$$

###### $$\lambda \cdot \left[ \alpha \|\beta \|_{1}+(1-\alpha ) \|\beta \|_{2}^{2} \right]  \leq t.$$

<br>

<p class="p1-double-digits">The subscript for the norm is the ${\displaystyle \ell ^{p}}$ norm like the following expression. </p>

###### $${\displaystyle \|u\|_{p}=\left(\sum _{i=1}^{N}|u_{i}|^{p}\right)^{1/p}}$$

<p class="p1-double-digits">The $\alpha$ parameter is used to select L1 LASSO or Ridge regression or a combination of them. The $t$ parameter is used to reduce or eliminate variable coefficients and is called a regularization parameter. The total error is held below the limit controlled by $t$. The last equation can be put in a Lagrangian form where $t$ is replaced by equivalent $\lambda$ parameter that depends on the data. The elastic net equation encompassing lasso and ridge regression is as follows: </p>

###### $$\displaystyle \min_{\beta \in \mathbb {R} ^{p}}\left\{\| y-X \beta \|_{2}^{2} + \lambda \cdot \left[(\alpha ) \|\beta \|_{1}+(1 - \alpha ) \|\beta \|_{2}^{2}\right] \right\}$$

<br>

<p class="p1-double-digits">The hyperparameter $\lambda$ is either zero or positive-valued. It controls the error penalty beyond the squared errors of the first term inside Equation-12. </p>

<br>
   
### LASSO Regression: L1 Regularization

<p class="p2-double-digits">The LASSO method reduces the coefficients to a degree where some are set to zero and thereby eliminated using the absolute value of the error as a penalty, which is closer to one than the squared value used for regression. The following equation is derived from Equation-12 if $\alpha$ is set equal to one. </p>

###### $$\displaystyle \min_{\beta \in \mathbb {R} ^{p}}\left\{\| y-X \beta \|_{2}^{2} + \lambda \|\beta \|_{1}    \right\}$$

<br>

<p class="p2-double-digits">The value of the regularization hyperparameter $\lambda$ is increased while keeping the total error below a limit which can eliminate variables.</p>

<p class="p2-double-digits">In a Bayesian  interpretation, Lasso is a type of linear regression for which the coefficients have Laplace prior distributions which have an sharp exponential decay to zero and hence sharply peaked near the central location compared to normal distributions for ridge regression.</p>

### Ridge Regression: L2 Regularization

<p class="p2-double-digits">The following equation is derived from Equation-12 if $\alpha$ is set equal to zero. The ridge method reduces the coefficients using the squared Euclidean distance in the penalty term which is the second term in the equation below. </p> 

###### $$\displaystyle \min_{\beta \in \mathbb {R} ^{p}}\left\{\| y-X \beta \|_{2}^{2} + \lambda \|\beta \|_{2}^{2}\right\}$$

<br>

<p class="p2 p1-double-digits">The value of the regularization hyperparameter $\lambda$ is increased while keeping the total error below a limit. This lowers the value of the coefficients but variables are not eliminated.</p>

## Simulated Solar Irradiation Data

<p class="p1-double-digits">The purpose of this study was to evaluate the small variability of solar irradiation impinging upon the earth over a number of Jupiter orbits. A three-body model of the solar system was formed consisting of the sun, Jupiter, and the earth. This simplified model was selected because these three bodies and the related moons account for well over 99% of the total mass the solar system. An improved model requiring more orbit calculations would include Venus and Saturn. The computer code that generated the simulation is in the file <i>SunJupiterEarthSimulation.R</i>.</p>  

<div class="pdiv">

```{r solardiagram, echo=F, fig.width=4.2, fig.height=3.8}

## Class Definitions

## Class 1: PointLocationVector

PointLocationVector <- R6Class("PointLocationVector", 
    list(
       x = NA,
       y = NA,
       z = NA,
       mag = NA,
       theta = NA,
       phi = NA,
       pie = 3.1415926535897932385,
       two_pie = 2 * 3.1415926535897932385,
       ## Option-1: Enter rectangular coordinates x, y, z
       ## Option-2: Enter polar coordinates mag, theta, phi
       initialize = function(...,degrees=TRUE) {
         arg_list <- c(as.list(environment()), list(...))
         name_list <- names(arg_list)
         
         x_found <- sum(grepl("x",name_list))
         if(x_found > 0 ){
           x_param <- grep("x",name_list)
           x <- arg_list[[x_param]]
         }
         y_found <- sum(grepl("y",name_list))
         if(y_found > 0 ){
           y_param <- grep("y",name_list)
           y <- arg_list[[y_param]]
         }
         z_found <- sum(grepl("z",name_list))
         if(z_found > 0 ){
           z_param <- grep("z",name_list)
           z <- arg_list[[z_param]]
         }
         
         mag_found <- sum(grepl("mag",name_list))
         if(mag_found > 0 ){
           mag_param <- grep("mag",name_list)
           mag <- arg_list[[mag_param]]
         }
         theta_found <- sum(grepl("theta",name_list))
         if(theta_found > 0 ){
           theta_param <- grep("theta",name_list)
           theta <- arg_list[[theta_param]]
         }
         phi_found <- sum(grepl("phi",name_list))
         if(phi_found > 0 ){
           phi_param <- grep("phi",name_list)
           phi <- arg_list[[phi_param]]
         }
         if(x_found > 0 && y_found > 0){
            mag <- ((x^2)+(y^2))^0.5
            theta <- self$arctrig360(x,y,degrees)
            self$x <- x
            self$y <- y
            self$z <- z 
            self$mag <- ((x^2)+(y^2)+(z^2))^0.5
            self$theta <- self$arctrig360(x=x,y=y)
            if( (x + y) != 0) {
              self$phi <- 360 * asin(z / (  ((x^2)+(y^2) + z^2  + 0.0001 )^0.5  )) / self$two_pie 
            } else {
              self$phi <- 90 
            }
         } else if (mag_found > 0 && theta_found > 0) {
            x <- mag * cos(self$two_pie*theta/360) * cos(self$two_pie*phi/360)
            y <- mag * sin(self$two_pie*theta/360) * cos(self$two_pie*phi/360)
            z <- mag * sin(self$two_pie*phi/360)
            self$x <- x
            self$y <- y
            self$z <- z
            self$mag <- mag
            self$theta <- theta
            self$phi <- phi
         }
       },
       set_x = function(x) {
           self$x <- x
           if( (x + self$y) != 0) {
             self$phi <- 360 * asin(self$z / (  ((x^2)+(self$y^2)  + (self$z)^2 + 0.0001)^0.5  ) ) /self$two_pie
           } else {
             self$phi <- 90 
           }
           self$mag <- ((self$x^2)+(self$y^2)+(self$z^2))^0.5
           self$theta <- self$arctrig360(x=x,y=self$y)
           },
       set_y = function(y) {
           self$y <- y
           self$mag <- ((self$x^2)+(self$y^2)+(self$z^2))^0.5
           self$theta <- self$arctrig360(x=self$x,y=self$y)
           if( (self$x + y) != 0) {
             self$phi <- 360 * asin(self$z / (  ((self$x^2)+(y^2)  + (self$z)^2 +0.0001)^0.5  ) ) / self$two_pie
           } else {
             self$phi <- 90
           }
       },
       set_z = function(z) {
           self$z <- z
           self$mag <- ((self$x^2)+(self$y^2)+(z^2))^0.5
           self$theta <- self$arctrig360(x=self$x,y=self$y)
           if( (self$x + self$y) != 0) {
             self$phi <- 360 * asin(z / (  ((self$x^2)+(self$y^2) + z^2  + 0.0001)^0.5  ) ) / self$two_pie
           } else {
             self$phi <- 90
           }
       },
       get_x = function() {
           self$x
       },
       get_y = function() {
           self$y
       },
       get_z = function() {
           self$z
       },
       get_mag = function() {
           self$mag
       },
       get_angle = function() {
           self$theta
       },
       get_phi = function() {
           self$phi
       },
       distance = function(pt1, pt2){
         delta_x <- pt1$get_x() - pt2$get_x()
         delta_y <- pt1$get_y() - pt2$get_y()
         delta_z <- pt1$get_z() - pt2$get_z()
         (delta_x^2 + delta_y^2 + delta_z^2)^0.5
       },
       diff = function(pt1, pt2){
         delta_x <- pt1$get_x() - pt2$get_x()
         delta_y <- pt1$get_y() - pt2$get_y()
         delta_z <- pt1$get_z() - pt2$get_z()
         PointLocationVector$new(x=delta_x,y=delta_y,z=delta_z)
       },
       add = function(pt1, pt2){
         delta_x <- pt1$get_x() + pt2$get_x()
         delta_y <- pt1$get_y() + pt2$get_y()
         delta_z <- pt1$get_z() + pt2$get_z()
         PointLocationVector$new(x=delta_x,y=delta_y,z=delta_z)
       },
       sgn = function(x){
         x >= 0
       },
       arctrig360 = function(x,y, degrees = TRUE){
         sx <- self$sgn(x)
         sy <- self$sgn(y)
         if(x != 0 ){
           theta <- atan(y/x) +
             self$pie * ((!sx && sy) || (!sx && !sy)) +
             self$two_pie * (sx && !sy)
           if(degrees == TRUE){
             theta <- 360 * theta / self$two_pie
           }
         } else if (x == 0 && y > 0){
           theta <- self$pie/2
           if(degrees == TRUE){
             theta <- 90
           }
         } else if (x == 0 && y < 0){
           theta <- 3 * self$pie / 2
           if(degrees == TRUE){
             theta <- 270
           }
         } else {
           theta <- 0
         }
         theta
       },
       cos_x = function(x) {
         # Converting degrees to radian 
         #x <- x * (two_pie / 360); 
         x1 <- 1 
         # maps the sum along the series 
         cosx <- x1
         ## holds the actual value cos[n] 
         ## cosval <- cos(x) 
         for (i in 1:5) {
           print(2*i)
           x1 <- -x1 
           denominator <- factorial(2 * i) 
           print(denominator)
           term <- x1 * x^(2*i) / denominator
           print(term)
           cosx <- cosx + term 
           print(cosx)
         }
         cosx
       }
    )
)

## Class 2: VelocityVector

VelocityVector <- R6Class("VelocityVector", 
   list(
     vx = NA,
     vy = NA,
     vz = NA,
     mag = NA,
     theta = NA,
     phi = NA,
     pie = 3.1415926535897932385,
     two_pie = 2 * 3.1415926535897932385,
     ## Option-1: Enter rectangular velocity components vx, vy, vz
     ## Option-2: Enter polar coordinates mag, theta, phi
     initialize = function(...,degrees=TRUE) {
       arg_list <- c(as.list(environment()), list(...))
       name_list <- names(arg_list)
       
       x_found <- sum(grepl("vx",name_list))
       if(x_found > 0 ){
         x_param <- grep("vx",name_list)
         vx <- arg_list[[x_param]]
       }
       y_found <- sum(grepl("vy",name_list))
       if(y_found > 0 ){
         y_param <- grep("vy",name_list)
         vy <- arg_list[[y_param]]
       }
       z_found <- sum(grepl("vz",name_list))
       if(z_found > 0 ){
         z_param <- grep("vz",name_list)
         vz <- arg_list[[z_param]]
       }
       
       mag_found <- sum(grepl("mag",name_list))
       if(mag_found > 0 ){
         mag_param <- grep("mag",name_list)
         mag <- arg_list[[mag_param]]
       }
       theta_found <- sum(grepl("theta",name_list))
       if(theta_found > 0 ){
         theta_param <- grep("theta",name_list)
         theta <- arg_list[[theta_param]]
       }
       phi_found <- sum(grepl("phi",name_list))
       if(phi_found > 0 ){
         phi_param <- grep("phi",name_list)
         phi <- arg_list[[phi_param]]
       }
       if(x_found > 0 && y_found > 0){
          mag <- ((vx^2)+(vy^2))^0.5
          theta <- self$arctrig360(vx,vy,degrees)
          self$vx <- vx
          self$vy <- vy
          self$vz <- vz 
          self$mag <- ((vx^2)+(vy^2)+(self$vz^2))^0.5
          self$theta <- self$arctrig360(x=vx,y=vy)
          if( (vx + vy) != 0) {
             self$phi <- 360 * asin(vz / (  ((vx^2)+(vy^2)  + vz^2 + 0.0001)^0.5  ) ) / self$two_pie 
          }
        } else if (mag_found > 0 && theta_found > 0) {
          vx <- mag * cos(self$two_pie*theta/360) * cos(self$two_pie*phi/360)
          vy <- mag * sin(self$two_pie*theta/360) * cos(self$two_pie*phi/360)
          vz <- mag * sin(self$two_pie*phi/360)
          self$vx <- vx
          self$vy <- vy
          self$vz <- vz
          self$mag <- mag
          self$theta <- theta
          self$phi <- phi
        }
     },
     set_vx = function(vx) {
       self$vx <- vx
       if( (vx + self$vy) != 0) {
         self$phi <- 360 * asin(self$vz / (  ((vx^2)+(self$vy^2) + (self$vz)^2  + 0.0001)^0.5  ) ) / self$two_pie
       } else {
         self$phi <- 90
       }
       self$mag <- ((vx^2)+(self$vy^2)+(self$vz^2))^0.5
       self$theta <- self$arctrig360(x=vx,y=self$vy)
     },
     set_vy = function(vy) {
       self$vy <- vy
       self$mag <- ((self$vx^2)+(vy^2)+(self$vz^2))^0.5
       self$theta <- self$arctrig360(x=self$vx,y=vy)
       if( (self$vx + vy) != 0) {
         self$phi <- self$phi <- 360 * asin(self$vz / (  ((self$vx^2)+(vy^2)  + (self$vz)^2 + 0.0001)^0.5  ) ) /self$two_pie 
       } else {
         self$phi <- 90
       }
     },
     set_vz = function(vz) {
       self$z <- vz
       self$mag <- ((self$vx^2)+(self$vy^2)+(vz^2))^0.5
       self$theta <- self$arctrig360(x=self$vx,y=self$vy)
       if( (self$vx + self$vy) != 0) {
         self$phi <- 360 * asin(vz / (  ((self$vx^2)+(self$vy^2)  + vz^2  + 0.0001)^0.5  ) ) /self$two_pie 
       } else {
         self$phi <- 90
       }
     },
     get_vx = function() {
       self$vx
     },
     get_vy = function() {
       self$vy
     },
     get_vz = function() {
       self$vz
     },
     get_mag = function() {
       self$mag
     },
     get_angle = function() {
       self$theta
     },
     get_phi = function() {
       self$phi
     },
     magnitude = function(pt1, pt2){
       delta_x <- pt1$get_x() - pt2$get_x()
       delta_y <- pt1$get_y() - pt2$get_y()
       delta_z <- pt1$get_z() - pt2$get_z()
       (delta_x^2 + delta_y^2 + delta_z^2)^0.5
     },
     diff = function(pt1, pt2){
       delta_x <- pt1$get_x() - pt2$get_x()
       delta_y <- pt1$get_y() - pt2$get_y()
       delta_z <- pt1$get_z() - pt2$get_z()
       
       PointLocationVector$new(x=delta_x,y=delta_y,z=delta_z)
     },
     sgn = function(x){
       x >= 0
     },
     arctrig360 = function(x,y, degrees = TRUE){
       sx <- self$sgn(x)
       sy <- self$sgn(y)
       if(x != 0 ){
         theta <- atan(y/x) +
           self$pie * ((!sx && sy) || (!sx && !sy)) +
           self$two_pie * (sx && !sy)
         if(degrees == TRUE){
           theta <- 360 * theta / self$two_pie
         }
       } else if (x == 0 && y > 0){
         theta <- self$pie/2
         if(degrees == TRUE){
           theta <- 90
         }
       } else if (x == 0 && y < 0){
         theta <- 3 * self$pie / 2
         if(degrees == TRUE){
           theta <- 270
         }
       } else {
           theta <- 0
       }
       theta
     },
     cos_x = function(x) {
       # Converting degrees to radian 
       #x <- x * (two_pie / 360); 
       x1 <- 1 
       # maps the sum along the series 
       cosx <- x1
       print(x1)
       ## holds the actual value cos[n] 
       ## cosval <- cos(x) 
       for (i in 1:5) {
         print(2*i)
         x1 <- -x1 
         denominator <- factorial(2 * i) 
         print(denominator)
         term <- x1 * x^(2*i) / denominator
         print(term)
         cosx <- cosx + term 
         print(cosx)
       }
       cosx
     }
   )
)


## (Class 3) Force Vector

## The tail point represents the center of mass of all the planets except
## one planet that is deleted. It is represented by the 'PointLocationVector'
## object that specifies its position with respect to the coordinate origen.
 
## The head represents the arrowhead end of the vector at the position of
## the planet that was deleted from the center mass calculation the tail end.
## It is represented by the 'PointLocationVector' object that specifies the 
## center mass of the planet its moons.

## The Force Vector or 'ForceVector' object is obtained by subtracting 
## the tail center of mass vector from the planet head vector PointLocationVector objects.

## Force = magnitude * (head - tail)

## The magnitude is calculated within the ForceVector class. 
## See params tail, head, theta above.
ForceVector <- R6Class("ForceVector",                  
   list(
     mag = NA,
     theta = NA,
     phi = NA,
     fx = NA,
     fy = NA,
     fz = NA,
     pie = 3.1415926535897932385,
     two_pie = 2 * 3.1415926535897932385,
     initialize = function(head, tail,
                           planet_mass , cm_mass, ..., degrees=TRUE) {
       
       yr <- 24*3600 * 365.256363
       arg_list <- c(as.list(environment()), list(...))
       pie <- 3.1415926535897932385
       two_pie <- 2 * pie
       name_list <- names(arg_list)
       
       tail_found <- sum(grepl("tail",name_list))
       if(tail_found > 0 ){
         tail_param <- grep("tail",name_list)
         tail <- arg_list[[tail_param]]
       }
       head_found <- sum(grepl("head",name_list))
       if(head_found > 0 ){
         head_param <- grep("head",name_list)
         head <- arg_list[[head_param]]
       }
       if(tail_found > 0 && head_found > 0){
         diff_vector <- head$diff(head,tail)
         d <- diff_vector$mag
         ## Universal Gravitational Constant 6.6743×10^(-11) m^3 kg second^2
         G <- 6.6743*10^(-11)
         mag <- G * planet_mass * cm_mass / (d^2)
         self$mag <- mag
         theta <- diff_vector$theta
         self$theta <- theta
         phi <- diff_vector$phi
         self$phi <- phi
         self$fx <- mag * cos(self$two_pie*theta/360) * cos(self$two_pie*phi/360)
         self$fy <- mag * sin(self$two_pie*theta/360) * cos(self$two_pie*phi/360)
         self$fz <- mag * sin(self$two_pie*phi/360)
         #cat("Force Vector of magnitude = ", mag," with direction of " ,theta, " degrees.\n")
       } 
       
     },
     set_mag = function(mag) {
       self$mag <- mag
     },
     set_theta = function(theta) {
       self$theta <- theta
     },
     set_phi = function(phi) {
       self$phi <- phi
     },
     get_mag = function() {
       self$mag
     },
     get_theta = function() {
       self$theta
     },
     get_phi = function() {
       self$phi
     },
     distance = function(tail, head){
       delta_x <- head$get_x() - tail$get_x()
       delta_y <- head$get_y() - tail$get_y()
       delta_z <- head$get_z() - tail$get_z()
       (delta_x^2 + delta_y^2  + delta_z^2)^0.5
     },
     sgn = function(x){
       x >= 0
     },
     arctrig360 = function(x,y, degrees = TRUE){
       sx <- self$sgn(x)
       sy <- self$sgn(y)
       if(x != 0 ){
         theta <- atan(y/x) +
           self$pie * ((!sx && sy) || (!sx && !sy)) +
           self$two_pie * (sx && !sy)
         if(degrees == TRUE){
           theta <- 360 * theta / self$two_pie
         }
       } else if (x == 0 && y > 0){
         theta <- self$pie/2
         if(degrees == TRUE){
           theta <- 90
         }
       } else if (x == 0 && y < 0){
         theta <- 3 * self$pie / 2
         if(degrees == TRUE){
           theta <- 270
         }
       } else {
         theta <- 0
       }
       theta
     }
   )
)

## Class 4: Planet

Planet <- R6Class("Planet", 
    list(
      name = NA,
      position = NA,
      x = NA,
      y = NA, 
      z = NA, 
      distance = NA,
      theta = NA, 
      phi = 90, 
      px = NA,
      py = NA,
      pz = NA,
      v0 = NA,
      aphelion = NA,
      heading0 = NA,
      heading_inclination0 = 0,
      year =  365.256363 *60*60*24,
      mass = NA,
      sun_mass = 1.98847 * 10^30,
      # Jupiter system mass including the four largest moons
      jupiter_system_mass = 1.898576 * 10^27,
      radius = NA,
      pie = 3.1415926535897932385,
      # Error in R6Class("Planet", list(position = NA, x = NA, y = NA, distance = NA,  : 
      # All elements of public, private, and active must be named.
      # Error cause by using '<-' assignment for field. 
      two_pie = 2 * 3.1415926535897932385,
      initialize = function(name, year_length=365.256363, aphelion=NA,
                            planet_mass=5.9722 * 10^24, moon_mass=7.342 * 10^22,
                            location=NA, set_v0=FALSE,
                            heading0=192.94719, heading_inclin0=0, radius=6371) {
        self$name <- name
        ## Universal Gravitational Constant
        G <- 6.6743*10^(-11)
        # sun_mass <- 1.99*10^30
        # jupiter_mass <- 1.8982*10^27
        
        # # Jupiter's moons
        # io <- 8.9*10^22
        # europa <- 4.8*10^22
        # callisto <- 8.9*10^22
        # ganymede <- 15*10^22
        # amalthea <- 208*10^16
        # jup_moons <- io + europa + callisto + ganymede
        # jup_mass <- jupiter_mass  + jup_moons
        
        mass <- planet_mass + moon_mass
        jup_distance0 <- 816.6*10^9
        pie <- 3.1415926535897932385
        two_pie <- 2 * pie
        
        if(exists("location", mode="environment")){
           x <- location$x
           y <- location$y
           z <- location$z
           self$distance <- ((x^2)+(y^2)+(z^2))^0.5
           self$theta <- self$arctrig360(x=x,y=y)
           if( (x + y) != 0) {
             self$phi <- 360 * asin(z / (  ((x^2)+(y^2)  + z^2  + 0.0001)^0.5  ) ) / self$two_pie 
           } else {
             self$phi <- 90 
           }
           d <- (x^2 + y^2 + z^2)^0.5
           aphelion <- d
        }  else { 
           location <- PointLocationVector$new(mag=aphelion*10^9, theta=45,phi=1.3)
           x <- location$x
           y <- location$y
           z <- ((x^2 + y^2)^0.5)*cos(two_pie*1.3/360)
           z <- location$z
           d <- aphelion*10^9
        }  
        self$position <- location
        self$x <- x
        self$y <- y
        self$z <- z
        self$distance <- d
        self$aphelion <- d
        self$theta <- location$theta
        self$phi <- location$phi
        self$radius <- radius * 1000
        ##r_sun <- d*mass/m_sun
        sun_distance0 <- -jup_distance0*self$jupiter_system_mass/self$sun_mass
        if(set_v0){
            if (d > (2 * (10^9)) ) {
               v0 <- ((G*self$sun_mass)/d)^0.5
            } else {
               ## Only true for the sun
               ## Initial sun velocity
               v0 <- ((G*self$jupiter_system_mass*sun_distance0)/(d^2))^0.5
            }
            self$v0 <- v0
            self$px <- mass * v0 * cos(self$two_pie*heading0/360) *
                                   cos(self$two_pie*heading_inclin0/360)
            self$py <- mass * v0 * sin(self$two_pie*heading0/360) *
                                   cos(self$two_pie*heading_inclin0/360)
            self$pz <- mass * v0 * sin(self$two_pie*heading_inclin0/360)
            #cat("Initial x-velocity = ",self$px/mass,"m/s, initial y-velocity = ",self$py/mass,"m/s\n")
        } 
        self$heading0 <- heading0
        self$heading_inclination0 <- heading_inclin0 
        self$mass <- mass
        self$year <- year_length * 3600 * 24
      },
      get_mass = function() {
         self$mass
      },
      set_x = function(x) {
         self$x <- x
         d <- ((x^2)+(self$y)^2+(self$z^2))^0.5
         self$distance <- d
         self$position <- PointLocationVector$new(x=x,y=self$y,z=self$z)
         self$theta <- self$position$theta
         self$phi <- self$position$phi
      },
      set_y = function(y) {
         self$y <- y
         d <- ((self$x^2)+(y^2)+(self$z^2))^0.5
         self$distance <- d
         self$position <- PointLocationVector$new(x=self$x,y=y,z=self$z)
         self$theta <- self$position$theta
         self$phi <- self$position$phi
      },
      set_z = function(z) {
        self$z <- z
        d <- ((self$x^2)+(self$y^2)+(z^2))^0.5
        self$distance <- d
        self$position <- PointLocationVector$new(x=self$x,y=self$y,z=z)
        self$theta <- self$position$theta
        self$phi <- self$position$phi
      },
      get_x = function() {
         self$x
      },
      get_y = function() {
         self$y
      },
      get_z = function() {
        self$z
      },
      set_px = function(p) {
         self$px <- p
      },
      set_py = function(p) {
         self$py <- p
      },
      set_pz = function(p) {
        self$pz <- p
      },
      get_px = function() {
         self$px
      },
      get_py = function() {
         self$py
      },
      get_pz = function() {
        self$pz
      },
      get_vx = function() {
        self$px/self$mass
      },
      get_vy = function() {
        self$py/self$mass
      },
      get_vz = function() {
        self$pz/self$mass
      },
      get_v0 = function() {
         self$v0
      },
      set_v0 = function(v,h=self$heading0,incln=self$heading_inclination0) {
        self$v0 <- v
        self$heading0 <- h
        self$heading_inclination0 <- incln
        self$px <- self$mass * v * cos(self$two_pie*h/360) * cos(self$two_pie*incln/360)
        self$py <- self$mass * v * sin(self$two_pie*h/360) * cos(self$two_pie*incln/360)
        self$pz <- self$mass * v * sin(self$two_pie*incln/360)
        #cat("Initial velocity = ",v,"m/s, initial_heading = ",self$heading0,"degrees\n")
      },
      get_heading0 = function() {
         self$heading0
      },
      sgn = function(x){
        x >= 0
      },
      arctrig360 = function(x,y, degrees = TRUE){
        sx <- self$sgn(x)
        sy <- self$sgn(y)
        if(x != 0 ){
          theta <- atan(y/x) +
            self$pie * ((!sx && sy) || (!sx && !sy)) +
            self$two_pie * (sx && !sy)
          if(degrees == TRUE){
            theta <- 360 * theta / self$two_pie
          }
        } else if (x == 0 && y > 0){
          theta <- self$pie/2
          if(degrees == TRUE){
            theta <- 90
          }
        } else if (x == 0 && y < 0){
          theta <- 3 * self$pie / 2
          if(degrees == TRUE){
            theta <- 270
          }
        } else {
          theta <- 0
        }
        theta
      }
   )
)

## Class 5: SolarSystem

SolarSystem <- R6Class("SolarSystem", 
  list(
    other_planets = NULL,
    sun = NULL,
    jupiter= NULL,
    earth = NULL,
    sun_mass = 1.98847 * 10^30,
    jupiter_year = 11.862*365.256363,
    # Jupiter system mass including the four largest moons
    jupiter_system_mass = 1.898576 * 10^27,
    initialize = function(planet="earth",v0_jupiter=12423, v0_earth=29298.59295,
                          calibrate=TRUE) {
      ## Jupiter and the sun are the two dominate features of the solar system.
      ## To simplify the equations, the plane of the solar system is defined
      ## by Jupiter's orbit and the inclination for earth's orbit and all others
      ## are given with respect to zero inclination for Jupiter's orbit plane.
      
      ## The x-axis is defined by the line through the initial positions
      ## of Jupiter and the sun. The x-y plane is defined by the plane formed 
      ## by the x-axis and the initial direction of Jupiter's velocity.
      ## The z-axis is defined by the line through the center of mass for the 
      ## initial positions of Jupiter and the sun and perpendicular
      ## to the x-y plane. 

      ## Initial Conditions
      
      ## The initial direction of the earth's velocity is set at aphelion
      ## above the x-y plane and parallel to it.  The earth's orbit inclined by
      ## about 1.3 degrees from plane of Jupiter's orbit so that aphelion the
      ## z coordinate of the earth is maximum. 
      G <- 6.6743*10^(-11)
      pie <- 3.1415926535897932385
      two_pie <- 2 * pie
      sun <- self$get_sun(calibrate)
      jupiter <- self$get_jupiter(v0=v0_jupiter)
      earth <- self$get_earth(v0=v0_earth)
      self$sun <- sun
      self$jupiter <- jupiter
      self$earth <- earth
      if(planet=="earth"){
        self$other_planets <- list(sun,jupiter) 
      } else {
        self$other_planets <- list(sun,earth) 
      }
      self$other_planets
    },
    get_sun = function(calibrate=TRUE) {
      ## Jupiter and the sun are the two dominate features of the solar system.
      G <- 6.6743*10^(-11)
      pie <- 3.1415926535897932385
      two_pie <- 2 * pie
      
      sun_radius <- 695700 # km
      sun_mass <- self$sun_mass
      jup_mass <- self$jupiter_system_mass
      jup <- self$get_jupiter() 
      
      ## Calculation of initial center of mass 
      jup_distance0 <- jup$distance  #793042000000
      sun_distance0 <- jup_distance0*jup_mass/sun_mass
      inclination <- 1.2986 # degrees

      jup_vx <- jup$px / jup_mass
      jup_vy <- jup$py / jup_mass
      jup_vz <- jup$pz / jup_mass
      
      sun_vx <- -jup_vx*jup_mass/sun_mass
      sun_vy <- -jup_vy*jup_mass/sun_mass
      sun_vz <- -jup_vz*jup_mass/sun_mass
      v_0 <- VelocityVector$new(vx= sun_vx, vy= sun_vy, vz= sun_vz)
      v0 <- v_0$mag
      ## planet1_angle <- 5.246395
      ## heading0 <- 104.75385
      theta <- jup$theta - 180
      heading0 <- v_0$theta
      sun_position0 <- PointLocationVector$new(mag=sun_distance0,
                                                 theta = theta , phi = -inclination)
      sun <- Planet$new(name="sun",year_length=self$jupiter_year,
                        planet_mass=sun_mass, moon_mass= 0,
                        location=sun_position0, set_v0=TRUE, heading0=heading0,
                        radius=sun_radius)
      #v0 <- ((G*jup_mass*sun_distance0)/(jup_distance0^2))^0.5
      # v_sun0 <- ((-G*m2*sun_distance0)/(r2^2))^0.5
      #((-G*m2*r1)/(r2^2))^0.5
      sun$set_v0(v0)
      sun
    },
    get_jupiter = function(v0=12413) {
      G <- 6.6743*10^(-11)
      sun_mass <- 1.98847 * 10^30
      jupiter_mass <- 1.8982*10^27
      
      # Jupiter's moons
      io <- 8.9*10^22
      europa <- 4.8*10^22
      callisto <- 8.9*10^22
      ganymede <- 15*10^22
      amalthea <- 208*10^16
      jup_moons <- io + europa + callisto + ganymede
      
      inclination <- 1.2986 # degrees
      ## Sidereal orbit period (days)	4,332.589	11.862
      # synodic period 398.88; orbit yr = 11.862
      #jup_year <- 11.862*365.256363 #    374342400
 
      jup_year_secs <- 24*3600 * self$jupiter_year

      #jup_distance0 <- 816.62*10^9
      #jup_position0 <- PointLocationVector$new(x=distance0,y=0,z=0)
      
      # Jupiter at Aphelion: "2017-02-17 07:17:00 GMT"
      # https://in-the-sky.org/news.php?id=20170217_12_100
      
      jup_ap_2017_secs <- as.numeric(as.POSIXct("2017-02-17 07:17:00 GMT",
                                                origin="1970-01-01",tz="GMT"))
      jup_ap_2017 <- as.POSIXct(x=jup_ap_2017_secs ,tz="GMT",origin="1970-01-01 00:00:00")
      jup_ap_2017 <- jup_ap_2017 + as.difftime((0)*jup_year_secs, unit="secs")
      jup_ap_1957 <- jup_ap_2017 + as.difftime((-5)*jup_year_secs, unit="secs")
      jup_ap_1957 <- as.POSIXct(x="1957-10-26 23:16:58 GMT",
                                tz="GMT",origin="1970-01-01 00:00:00")
      jup_ap_1957_secs <- as.numeric(jup_ap_1957,origin="1970-01-01",tz="GMT")
      #  -384396182
      
      ap2050secs <- 2540645455
      ap1957secs <- ap2050secs + (1957-2050)*31558118
      
      jup_ap_1957 <- "1957-10-26 23:16:58 GMT"
      earth_ap1957 <- as.POSIXct(x="1957-07-04 19:28:01 GMT",
                                 tz="GMT",origin="1970-01-01 00:00:00")
      earth_ap_1957_secs <- as.numeric(earth_ap1957,origin="1970-01-01",tz="GMT")
      #  [1]  -394259519
      
      diff <- difftime(jup_ap_1957,earth_ap1957)
      lag_angle <- 360 * (as.numeric(diff) / self$jupiter_year ) 
      # [1] -114.159 days
      
      # Lag of 9.559484 degrees or 115.0502 days from the sun's aphelion on
      # July 4, 1957 to Jupiter's aphelion on October 26, 1957. 
      
      # Given that the angle for Jupiter's aphelion is 14.75385 degrees,
      # its angle should lag 9.559484 degrees or 115.0502 days on July 4 
      # from 14.75385 degrees. Jupiter's angle on July 4 should be 
      ## 14.75385 - 9.559484 or 5.194366 degrees. 
      
      # However, given the velocity lower near aphelion resulting in an angle
      # of 5.196012 degrees.
      
      # Starting time is at the aphelion the earth.  
      
      #distance0 <- 816.62*10^9## Calibration: lag is 0 degrees
      distance0 <- 816.616*10^9## Calibration: lag is 0 degrees
      theta <- 194.75385 
      heading0 <- theta + 90
      radius <- 7192
      #v_0 <- VelocityVector$new(vx= 0, vy=  12413.9, vz= 0)
      jup_position0 <- PointLocationVector$new(mag=distance0,
                                               theta=theta, phi = inclination)
      jup <- Planet$new(name="jupiter",year_length=self$jupiter_year, 
                        planet_mass= jupiter_mass, moon_mass=jup_moons,
                        location=jup_position0, set_v0=TRUE, heading0=heading0,
                        radius=radius)  
      v_jup0 <- ((G*sun_mass)/distance0)^0.5 # 12753.08
      jup$distance <- distance0
      jup$set_v0(v0)
      jup$theta <- theta
      jup$heading0 <- heading0
      v_0 <- VelocityVector$new(mag = v0, theta = heading0,
                                phi = inclination)
      mass <- jupiter_mass + jup_moons
      jup$px <- v_0$vx  * mass
      jup$py <- v_0$vy  * mass
      jup$pz <- v_0$vz  * mass
      jup
    },
    get_earth = function(v0=29298.59295) {
      # Aphelion distance to CM for solar system
      ap2050secs <- 2540645455
      ap2050 <- as.POSIXct(x=ap2050secs,tz="GMT",origin="1970-01-01 00:00:00")
      ap1957secs <- ap2050secs + (1957-2050)*31558118
      ap1958secs <- ap2050secs + (1958-2050)*31558118
      ap2020secs <- ap2050secs + (2020-2050)*31558118
      as.POSIXct(x=ap1958secs,tz="GMT",origin="1970-01-01 00:00:00")
      # "1957-07-04 19:28:01 GMT"
      # "1958-07-05 01:36:39 GMT"
      #distance0 <- 152.1
      distance0 <- 152.147
      inclination <- 0
      #earth_distance_xy_plane <- 152.1*cos(6.28*earth_inclination/360)
      earth_mass <- 5.9722 * 10^24
      radius <- 6378 #km
      moon_mass <- 7.342 * 10^22  # [1] 31558118 seconds
      m3 <- earth_mass + moon_mass
      year <- 365.256363
      heading0 <- 12.94719
      # 282.94719 + 90
      earth_position0 <- PointLocationVector$new(mag=distance0*10^9,
                                                  theta=282.94719,
                                                  phi = inclination)
      earth <- Planet$new(name="earth",year_length=year,
                          planet_mass=earth_mass,moon_mass=moon_mass,
                          location=earth_position0,radius=radius,
                          set_v0=TRUE, heading0=heading0)
      earth$set_v0(v0)
      earth
    },
    get_cm = function(planet1,planet2){
      cm <- CenterOfMass$new(planet1,planet2)
      cm$position
    }
  )
)

solar_system <- SolarSystem$new()
sun <- solar_system$sun 
jupiter <- solar_system$jupiter 
earth <- solar_system$earth 

pie <- 3.1415926535897932385
two_pie <-  2 * 3.1415926535897932385
x1 <- 10^12*c(0,2)
y1 <- c(0,2)

xlims <- c(-10^12,10^12) / 10^9
ylims <- c(-0.8*10^12,0.8*10^12) / 10^9

xlabs <- seq(-1000, 1000, by=500)
ylabs <- seq(-1000, 1000, by=500)

samples <- seq(2,length(x1),by=1)
xs1 <- x1[samples]/10^9
ys1 <- y1[samples]/10^9

par(mar=c(1,1,3,1), bg = rgb(0.87,0.87,0.94, alpha = 0.03)) 
pltraw <- plot(xs1,ys1,cex=0.4,
               xlim=xlims, ylim=ylims, xlab=" ", ylab=" ",
               main="Solar System with Jupiter and Earth",
               col.main="#008800",
               xaxt="n", yaxt="n",cex.main=0.9,cex.sub=0.3)

abline(h=0,v=0, col = "#44AA44",lwd=0.8)

text(-920, -140, "Jupiter",
     adj = c(0,00), cex = 1.1, col = "brown")
text(-380, 295, "Earth",
     adj = c(0,00), cex = 0.95, col = "blue")

text(20, 580, "Sun orbits center-of-mass",
     adj = c(0,0), cex = 0.7, col = "#888800")
text(20, 500, "of solar system",
     adj = c(0,0), cex = 0.7, col = "#888800")
## Segment line to arrow
segments(290,460,
         -80, 70,lty=1, col="#888811",lwd=0.8)

text(-480, -420, "Center-of-Mass of Sun and Jupiter",
     adj = c(0,0), cex = 0.7, col = "#008800")
text(-480, -500, "is at origin of coordinate system.",
     adj = c(0,0), cex = 0.7, col = "#008800")
text(-480, -580, "Earth orbits this center of mass.",
     adj = c(0,0), cex = 0.7, col = "#008800")
## Arrow for comment at bottom
arrows(-300, -350,-20, -20, col= '#22AA22',length = 0.11,lwd=0.9)

xs2 <- x2[samples]/10^9
ys2 <- y2[samples]/10^9

#sun$x [1] 753.797500 ## sun$radius [1] 695.700000
x_sun <- 150*sun$x / 10^9
orb_sun <- 150*sun$radius / 10^9
## sun orbit
draw.circle(0,0, radius=orb_sun, lty=4, border='#dd8800')
## sun
draw.circle(x_sun,0, radius=orb_sun,col="yellow")

## earth orbit
## draw.circle(0,0, radius=350, lty=4)
ell <- Ellipse$new(c(0,0), 350, 337, 135)
draw(ell, border = "blue", lty=4, lwd = 1)

## earth
e_pos <- PointLocationVector$new(mag=350,theta=135,phi=0,degrees=TRUE)
draw.circle(e_pos$x,e_pos$y, radius=3000*earth$radius/10^9,col="blue")

## jupiter
ell <- Ellipse$new(c(-30,0), 781, 741, 0)
draw(ell, border = "#44AA44", lty=4, lwd = 1)
draw.circle(jupiter$x/10^9-25,0, radius=5000*jupiter$radius/10^9,col="brown")

arrow_sun <- PointLocationVector$new(mag=orb_sun,theta=135,phi=0,degrees=TRUE)
## solar orbit arrow
arrows(arrow_sun$x, arrow_sun$y,arrow_sun$x-20, arrow_sun$y-20, col= '#dd8800',length = 0.12,lwd=1.5)


```


```{r solar_irrad_data, echo=FALSE,  results='asis'}
filename <- "SolarSystemBodies.csv"
#conn <- file(filename,open="r")
dfraw <- read.csv(filename,skip=2)
#close(conn)

colnames(dfraw) <- c("group","name","equatorial_radius","radius","mass","density",
  "rotation_period","orbit_Period","gravity","escape_velocity")
dfcleaned <- dfraw
cleaned2 <- gsub("±\\d+\\.\\d*","",x=dfraw[,"density"])
dfcleaned[,"density"] <- round(as.numeric(cleaned2),digits=3)
cleaned3 <- gsub("±\\d+\\.\\d*","",x=dfraw[,"radius"])
dfcleaned[,"radius"] <- round(as.numeric(cleaned3),digits=3)

G <- 6.67259*10^(-20)
dfcleaned[,"mass"] <- round(as.numeric(dfcleaned[,"mass"]),digits=7)
dfcleaned[,"mass"] <- signif(dfcleaned[,"mass"], 6)

dfcleaned <- dfcleaned[,c(1,2,4,3,5:10)]
total_mass <- sum(dfcleaned[,"mass"], na.rm = TRUE)
total_planet_mass <- sum(dfcleaned[dfcleaned$name != "sun" ,"mass"],na.rm = TRUE)
jup_mass <- dfcleaned[dfcleaned$name == "Jupiter","mass"]

dfgroup <- aggregate(mass ~ group, data = dfcleaned, sum)
jup_group_mass <- dfgroup[dfgroup$group == "Jupiter","mass"]
sat_group_mass <- dfgroup[dfgroup$group == "Saturn","mass"]
urn_group_mass <- dfgroup[dfgroup$group == "Uranus","mass"]
nep_group_mass <- dfgroup[dfgroup$group == "Neptune","mass"]
earth_group_mass <- dfgroup[dfgroup$group == "Earth","mass"]
ven_group_mass <- dfgroup[dfgroup$group == "Venus","mass"]

jup_frac <- jup_group_mass/total_planet_mass
jup_cum <- jup_frac
sat_frac <- sat_group_mass/total_planet_mass
sat_cum <- jup_frac + sat_frac
urn_frac <- urn_group_mass/total_planet_mass
urn_cum <- jup_frac + sat_frac + urn_frac
nep_frac <- nep_group_mass/total_planet_mass
nep_cum <- jup_frac + sat_frac + urn_frac + nep_frac
earth_frac <- earth_group_mass/total_planet_mass
earth_cum <- jup_frac + sat_frac + urn_frac + nep_frac + earth_frac
ven_frac <- ven_group_mass/total_planet_mass
ven_cum <- jup_frac + sat_frac + urn_frac + nep_frac + earth_frac + ven_frac

fraction <- c(jup_frac,sat_frac,urn_frac,nep_frac,earth_frac,ven_frac)
cumulative <- c(jup_cum,sat_cum,urn_cum,nep_cum,earth_cum,ven_cum)
planet <- c("Jupiter","Saturn","Uranus","Neptune","Earth","Venus")
distance <- c(778.3,1427,2871,4497.1,149.6,108.2)
fraction <- round(fraction,digits=3)
cumulative <- round(cumulative,digits=3)
total_force <- sum(fraction / distance^2)
fractional_force <- (fraction / distance^2) / total_force
fractional_force <- round(fractional_force ,digits=4)

dffrac <-  data.frame(planet,fraction, cumulative,distance,fractional_force)
jupfraction1 <- dffrac[dffrac$planet=="Jupiter","fraction"]

fraction2 <- c(jup_frac,sat_frac,urn_frac,nep_frac)
cumulative2 <- c(jup_cum,sat_cum,urn_cum,nep_cum)
planet2 <- c("Jupiter","Saturn","Uranus","Neptune")
distance2 <- c(778.3,1427,2871,4497.1)
fraction2 <- round(fraction2,digits=3)
cumulative2 <- round(cumulative2,digits=3)

total_force2 <- sum(fraction2 / distance2^2)
fractional_force2 <- (fraction2 / distance2^2) / total_force2
fractional_force2 <- round(fractional_force2 ,digits=4)

ff2 <- data.frame(planet2,fraction2, cumulative2,distance2,fractional_force2)
jupfraction2 <- ff2[ff2$planet2=="Jupiter","fractional_force2"]

fraction2 <- c(jup_frac,sat_frac,urn_frac,nep_frac)
cumulative2 <- c(jup_cum,sat_cum,urn_cum,nep_cum)
planet2 <- c("Jupiter","Saturn","Uranus","Neptune")
distance2 <- c(778.3,1427,2871,4497.1)
fraction2 <- round(fraction2,digits=3)
cumulative2 <- round(cumulative2,digits=3)

total_force2 <- sum(fraction2 / distance2^2)
fractional_force2 <- (fraction2 / distance2^2) / total_force2

```

</div><p class="p1-double-digits">Although the solar output power is fairly constant, the instantaneous irradiation impinging upon the earth varies as the distance between the earth and the sun changes in its rather circular but elliptical orbit about the center of the solar system. . The distance between the earth and the center of the solar system is known to vary from about 147.1-million to 152.1-million kilometers every year. The investigation considers the larger variance of the distance between the earth and the sun caused by the small orbit of the sun around the center of the solar system of very roughly 0.8-million kilometers with a period roughly the same as Jupiter's orbit of 11.862 years to a first approximation for this model which neglects Saturn and the other planets excluded from this three-body model. </p>

### Dominance of Jupiter among the Planets

```{r jupiterdominance, echo=FALSE,  results='asis'}

cat('<p class="p2-double-digits">If we add up the mass in a <a  href="https://ssd.jpl.nasa.gov/?planet_phys_par#D">NASA-JPL table</a>, Jupiter accounts for approximately ',round(100*jupfraction1,1),'% of the solar system mass outside the sun, but it is responsible for most of the force applied to the inner solar system consisting mainly of the sun, Mercury, Venus, and earth lumped together as a system because the Saturn, Uranus, and Neptune gas giants are further away. As far as Jupiter and the other gas giants are concerned, the inner rocky planets including Venus, earth and the sun are a combined system with a center-of-mass within 1000-km of the center of the sun. After this approximation, Jupiter accounts for at least ',round(100*jupfraction2,1),'% of the force applied to the sun-venus-earth system; Saturn accounts for about 8% because it is much further away, Uranus 0.3%. Although these error terms are large, the calculations can still shead some light about the variation of solar irradiation upon the earth.</p>',sep = "")

```

### Sampling Frequency for Simulation

<p class="p2-double-digits">More sample points would be helpful. The sampling rate was increased to 80,000 samples per year for the first year of earth’s orbit to obtain a more accurate preliminary year after which the final sampling rate was reset at 8000 samples per year on July 5th, 1959. The longitudinal position of earth’s orbit advances about 1 degree at the shift to the final lower sampling rate. Since the computer could recalculate the forces, velocities, and positions about 2000 times per minute, 72 years of data was generated in about five hours. The sample rate of 8000 is a compromise between accuracy and what was feasible. </p>    

### Initial conditions

<p class="p2-double-digits">Solving the orbit equations would permit one to start the planets simultaneously with the required velocity and heading necessary to reach aphelion at the proper times. Indeed, solving the equation would eliminate the need for this entire project of generating simulated orbit data. The solution of the orbits of a three-body planetary system has not been found as an equation or in what is called closed form.</p>

<p class="p2-double-digits">Simulated data collection began on July 5th, 1959 after setting the initial conditions as follows. The planets were started at aphelion where the direction of the planet velocity is approximately perpendicular to the position vector to the center of mass of the solar system. Starting orbits at aphelion leads to a new problem because the planets do not reach the aphelion of their orbit simultaneously. For example, while the earth is at aphelion every year in early July, the calendar date of aphelion for Jupiter varies because the period of Jupiter’s orbit is 11.862 years. Since, this makes it impossible to set the initial conditions for all of the planets simultaneously, a process was used to set the planets at aphelion in a sequential manner. After starting the three bodies at aphelion at the time of Jupiter’s aphelion in 1957, the earth is arbitrarily moved and restarted at its own aphelion after a delay of 251 days on July 5, 1958.</p>

<p class="more-top-margin p2app">A. &nbsp; &nbsp; The Sun and Jupiter</p>

<p class="p2-double-digits">The biggest error in this three-body model of the sun, Jupiter, and earth is likely the initial position of the sun opposite jupiter to start the simulation. Consider that two stars of a binary star system pair orbit a common center of mass with the same period. Similar to a binary star pair, we might be tempted to guess that the sun would orbit the common center of mass of the Jupiter-sun combination somewhat roughly every 11.86 years like Jupiter because Jupiter contains 71% of the total planet masses and that the center of the solar system is bounded within some region near the center of mass of the Jupiter-sun combination subject to the effects of the other planets as known error terms for this model. The position of the sun is probably not well known beyond this approximation at this time in the year 2020. Adding Saturn to the model would help reduce the error concerning the center-of-mass as the combined mass of Jupiter and Saturn contains 90% of the total planet masses.</p>

<p class="p2-double-digits">Lacking better information, the sun is placed at the aphelion its orbit simultaneously with Jupiter on October 26, 1957. The position of the Sun is set opposite Jupiter so that the center-of-mass of the Jupiter-Sun system is positioned at origin of the coordinate system. </p>

<p class="p2-double-digits">Time of aphelion for Sun-Jupiter System: October 26, 1957 at 1957-10-26 23:16:58 GMT, corresponding to -384,396,182 seconds before 1970-01-01 00:00:00.</p>

<p class="more-top-margin p2app">B. &nbsp; &nbsp; Earth</p>

<p class="p2-double-digits">After starting Jupiter on October 26, 1957, the position of earth is reset at aphelion on July 5, 1958 after a delay of approximately 251.097 days at -362,701,401 seconds.</p>

<p class="p2-double-digits">Aphelion of the earth-moon system in the year 2050 was estimated as the 99-year average of the earth’s aphelions from 2001 to 2099 using the data from Fred Espenak at www.Astropixels.com based on JPL DE405. Using this data, the 99-year mean is in the year 2050 at 2050-07-05 14:50:55 GMT, which corresponds to 2,540,645,455 seconds after 1970-01-01 00:00:00.</p>

<div class="p2-double-digits"><div class="pdivsmall">
<Ul class="square-bullet more-left-margin">

<li>Aphelion2050 = 2,540,645,455 seconds; Going back in time using a sideral year of approximately 365.256 days or 31,558,118 seconds, the corresponding time of Earth’s aphelion on July 5, 1958 at 01:36:39 GMT is as follows:</li>
<li>Aphelion1958 = Aphelion2050 + (1958-2050)*31558118</li>
<li>Aphelion1958 =  -362,701,401 seconds</li>

</ul></div></div>

<p class="p2-double-digits">Adding the earth as a third body transforms the stationary center-of-mass of the sun-Jupiter system at the origin of the coordinate system to a new center-of-mass which orbits the origin with radius of 0.00045 million kilometers. As the distance from the earth to the sun varies about 5-million kilometers, this issue was small enough to proceed with the other analysis. This problem could be eliminated by a solution of the equation for the three-body model.</p>

<br>

### Simulated Data for Sun-Jupiter-Earth System

<p class="p2-double-digits">At the start of the simulation, the initial position of the sun is set opposite Jupiter such that the center of mass for the Sun-Jupiter pair is at the origin the solar system as described in more detail above. In a better model, the sun would be initially placed opposite the center of mass of Jupiter and Saturn. The following plot only illustrates why such a four-body model should be computed with a more powerful computer. The plot below indicates that the climate on earth could be modulated by the solar system. </p>

<br>

```{r irradiationfig, echo=FALSE}

irrad_df <- read.csv('yearlyIrradiationData.csv',stringsAsFactors=FALSE, row.names = NULL)
  colnames(irrad_df) <- c("Year","Year Average","July","Aug","Sep","Oct","Nov","Dec",
                               "Jan","Feb","Mar","Apr","May","June")
melt_data <- irrad_df[,c(1,2,3,9)]                             
colnames(melt_data) <- c("Year","Year Average","July","January")
melt_data <- melt(melt_data, id = c("Year")) 
colnames(melt_data) <- c("Year","month","Irradiation")
melt_data$month <- factor(melt_data$month, levels = c("January", "Year Average", "July"))

figurenum <- figurenum + 1
fig <- paste0("Figure-",figurenum)
heading <- paste("Solar Irradiation for Sun-Jupiter-Earth System\n",fig)
ggplot(melt_data, aes(Year,Irradiation)) +
  geom_point(aes(color = month),size=2)  +
  coord_cartesian(xlim = c(1958, 2020), ylim = c(1300, 1440)) +
  xlab("Year") + ylab("Watts / Square Meter") +
  #scale_x_discrete(breaks = seq(1940, 2100, 10)) +
  scale_x_continuous(minor_breaks = seq(1950, 2030, 10), breaks = seq(1940, 2040, 20)) +
  scale_y_continuous(minor_breaks = seq(1310, 1450, 10), breaks = seq(1300, 1460, 20)) +
  ggtitle(heading) +
  theme(plot.title = element_text(size=14,hjust = 0.5, color="#008800"),
     #legend.title = element_text(size=16),
     legend.title = element_blank(),
     legend.text = element_text(size=14),
     legend.background = element_rect(fill = "#DDDDEE"),
     legend.key = element_rect(colour = "transparent", fill = "#DDDDEE"),
     text = element_text(size=12),
     axis.text = element_text(size=14,angle=0, hjust=0.5),
     axis.title = element_text(size = 16),
     axis.title.x = element_text(margin = margin(t = 7)),
     axis.title.y = element_text(margin = margin(l = 13)),
     panel.grid.minor =   element_line(color = "white",size=0.6),
     panel.grid.major =   element_line(color = "light blue",size=0.8),
     panel.background = element_rect(fill = "#EFE5E5", color = "#008800",
            size = 0.5, linetype = "solid"),
     plot.background = element_rect(fill = "#DDDDEE",color = "#DDDDEE"))

```

<br>

<p>While our model for predicting June rain is only based the weather data, January irradiation still appears in Table-1 with a correlation of -0.25. If we were to add Saturn to the model, perhaps we might expect this would reduce the effect of Jupiter when Saturn is opposite Jupiter and increase the effect when Saturn is in conjucture with Jupiter. As is well-reported, Saturn and Jupiter appeared together in the western sky briefly after sundown in December of 2020, which indicates an approximate conjuncture with earth on the opposite side of the sun. But this is only approximate because the earth is offset somewhat from the line passing through the sun and Jupiter by the angle which is apparent after sundown at that time.</p>

<p>The table below lists the data plotted immediately above.</p>

<br>

```{r irradiationdata, echo=FALSE, results='asis'}

tablenum <- tablenum + 1
tablecaption <- paste0('<p class = "center bluegreen">Simulated Solar Irradiation for Earth (Watts / square-meter)</p><p class = "center bluegreen">Table-',tablenum,'</p>') 
cat(tablecaption)

```

```{r irradiationdata2, echo=FALSE}


irrad_df  %>% mutate_at(2:14,  
   formatC,format = "f", digits = 1)  %>%  datatable(irrad_df, rownames = FALSE)

```

<br>

## Bibliography

<p class="p1-double-digits">R.E. Walpole, R. H. Myers, S. L. Myers, K. Ye (Eds.). (2007).<i> Probability and Statistics for Scientists and Engineers</i> (8th ed.). Upper Saddle River, NJ: Pearson Prentice Hall.</p> 


